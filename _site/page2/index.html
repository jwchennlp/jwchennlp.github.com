<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Latest Posts &#8211; My blog</title>
<meta name="description" content="Describe this nonsense.">
<meta name="keywords" content="Jekyll, theme, themes, responsive, blog, modern">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Latest Posts">
<meta property="og:description" content="Describe this nonsense.">
<meta property="og:url" content="http://jwchennlp.github.com/page2/index.html">
<meta property="og:site_name" content="My blog">





<link rel="canonical" href="http://jwchennlp.github.com/page2/">
<link href="http://jwchennlp.github.com/feed.xml" type="application/atom+xml" rel="alternate" title="My blog Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://jwchennlp.github.com/assets/css/main.min.css">
<!-- Webfonts -->
<link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://jwchennlp.github.com/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://jwchennlp.github.com/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://jwchennlp.github.com/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://jwchennlp.github.com/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://jwchennlp.github.com/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://jwchennlp.github.com/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://jwchennlp.github.com/images/apple-touch-icon-144x144-precomposed.png">





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
                  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                          });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
<!--
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
-->

</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://jwchennlp.github.com">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://jwchennlp.github.com/images/avatar.jpg" alt="jwchen photo" class="author-photo">
					<h4>jwchen</h4>
					<p>不积跬步，无以至千里;不积小流，无以成江海</p>
				</li>
				<li><a href="http://jwchennlp.github.com/about/">Learn More</a></li>
				<li>
					<a href="mailto:hit1093710417@email.com"><i class="icon-envelope"></i> Email</a>
				</li>
				
				
				
				
				<li>
					<a href="http://github.com/jwchennlp"><i class="icon-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://jwchennlp.github.com/posts/">All Posts</a></li>
				<li><a href="http://jwchennlp.github.com/tags/">All Tags</a></li>
			</ul>
		</li>
		<li><a href="http://jwchennlp.github.com/theme-setup">Theme Setup</a></li><li><a href="http://mademistakes.com">External Link</a></li>
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  <div class="image-credit">Image source: <a href="http://www.dargadgetz.com/ios-7-abstract-wallpaper-pack-for-iphone-5-and-ipod-touch-retina/">dargadgetz</a></div><!-- /.image-credit -->
  
    <div class="entry-image">
      <img src="http://jwchennlp.github.com/images/abstract-1.jpg" alt="Latest Posts">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>My blog</h1>
      <h2>Latest Posts</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-05-07T00:00:00-04:00"><a href="http://jwchennlp.github.com/generative-model-and-discriminative-model/">May 07, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/generative-model-and-discriminative-model/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/generative-model-and-discriminative-model/" rel="bookmark" title="生成模型和判别模型" itemprop="url">生成模型和判别模型</a></h1>
    
  </header>
  <div class="entry-content">
    <h2 id="section">定义</h2>
<hr />
<blockquote>
  <p>生成方法由数据学习联合概率分布P(x,y)，然后求出条件概率分布P(y｜ｘ)作为预测的模
型，即成生模型:  <br />
<img src="../images/140507/1.png" alt="image" /> <br />
这样的方法成为生成方法，是因为模型表示了给定输入ｘ产生输出ｙ的生成关系．典型的生成模型有，朴素贝叶斯和隐马尔可夫模型．  <br />
判别方法是由数据直接学习决策函数ｆ(x)或者条件概率分布P(y｜x)作为预测的模型，即判别模型，判别方法关心的是对给定的输入x,应该预测什么样的输出ｙ，典型的方法包括感知机，决策树，逻辑回归．   </p>
</blockquote>

<h2 id="section-1">理解</h2>
<hr />
<p>在面对猫狗分类问题时，我们该如何实现呢？  <br />
方法一：当我们利用逻辑回归或是感知机模型时，我们需要数据集所投射的空间中，找到一个决策边界，在决策边界一边的属于一类动物，在决策边界另一边的属于另一种动物．当来一个我们不知道的动物时，我们将它放入空间中，通过判断它在决策边界的那一侧来判断是猫还是狗． <br />
方法二：将数据集中的猫都拿出来，建立一个关于猫的特征的模型．按同样的方法建立一个关于狗的模型．这样，当判断一个动物时，我们分别查看它在猫模型中属于猫的概率和在狗模型中属于狗的概率，哪个值大，便说明属于哪个模型．   </p>

<p>方法一通过对数据集训练出一个模型，并通过判断P(y|x)下的条件概率来判断ｙ的类别．这种方法成为判别方法，对应建立的模型属于判别模型．   <br />
方法二对数据集的每一个类别建立一个模型，并通过联合概率P(x,y)来判断ｘ特征所应对应的类别．这种方法成为生成方法．  </p>

<p>其实通过联合概率来判断类别进行了一个变形，一般我们是要判断P(y|x)下的概率，可以进行如下转换： <br />
<img src="../images/140507/1.png" alt="image" />  <br />
对于某个参数ｘ，其概率值P(x)值在所有类别下都是相同的，所以问题便等同于如下问题：            <br />
<img src="../images/140507/2.png" alt="image" />        </p>

<p>不妨通过一个朴素贝叶斯生成模型来了解生成模型的判定过程．
如图，训练集包含４篇文档，我们需要验证测试集中的文档类别： <br />
<img src="../images/140507/3.png" alt="iamge" />    </p>

<p>我们需要计算每一个类别下P(x｜y)P(y)的概率，并且概率最大的那一类便是文档所属类别．即计算P((Chinese,chinese,Chinese,Tokyo,Japan)｜y=c)P(y=c)和P((Chinese,chinese,Chinese,Tokyo,Japan)｜y=$\bar{c}$)P(y=$\bar{c}$)．</p>

<p>之后利用朴素贝叶斯的的条件独立定义进行求解便能获知测试及属于哪个类别．</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-05-06T00:00:00-04:00"><a href="http://jwchennlp.github.com/index-compress/">May 06, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/index-compress/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/index-compress/" rel="bookmark" title="索引压缩" itemprop="url">索引压缩</a></h1>
    
  </header>
  <div class="entry-content">
    <h3 id="section">为什么要进行索引压缩？</h3>
<hr />
<p>进行索引压缩有以下优点：  </p>

<ul>
  <li>节省磁盘空间．  </li>
  <li>增加高速缓存(cache)的利用率.<br />
倒排索引词典是放在内存中的，倒排记录表放在磁盘上．对与到拍记录上的某些词项ｔ，我们是需要经常访问的，如果将这次词项ｔ所对应的到拍记录表压缩后放在高速缓存中，只要采用得当的解压缩算法，那么当查询词项ｔ的倒排记录表时，只需要访问cache，而不用从磁盘读取数据，能充分减少IR系统的响应时间． </li>
  <li>压缩能够加快从磁盘读取数据的速度．</li>
</ul>

<p>压缩技术分为有损压缩和无损压缩，有损压缩指的是压缩后，原始数据的所有信息都保存下来了．词干还原，大小写转换都属于有损压缩．   </p>

<h3 id="heaps">Heaps定律：词项数目的估计</h3>

<p>heaps定律认为，文档集大小和词汇量之间存在对数上的线性关系.它将词项的数目估计为文档集大小的函数:<script type="math/tex">M=kT^b</script>,其中Ｔ代表文档集合中的词条的个数． </p>

<p>不同文档集下ｋ取值差异较大，因为词汇量大小取决于文档本身以及对他进行处理的方式．当进行词干还原，大小写转换时将降低词汇量增长的速度，允许加入数字和容忍拼写错误则会增加增长率．无论参数取值如何，heaps定律满足一下两条性质：    </p>

<ul>
  <li>词汇量会随着文档集的增加而增加，不会趋于一个定值．     </li>
  <li>大规模文档集的词汇量也会很大．       </li>
</ul>

<h3 id="zipf">Zipf定律：词项在文档中的分布</h3>
<p>Zipf定律用于估计词项在文档中分布，假设$t_1$用于表示文档集中出现最多的词，$t_2$用于表示文档集中出现第二多的词，文档集合中出现第i多的词的文档频率$cf_i$与$\frac{1}{i}$成正比:   <br />
    <script type="math/tex">cf_i=k\frac{1}{i}</script></p>

<h2 id="section-1">词典压缩</h2>
<hr />

<h3 id="section-2">为什么要进行词典压缩</h3>
<p>理想情况下在建立好索引后，我们希望将词典存放在内存中，但是这往往很难实现，尤其是对于实用的搜索引擎和嵌入式系统．限制IR系统的响应之间的一个因素包多对磁盘的访问次数．所以，如果通过压缩来讲所有的或大部分的词典存入内存，将大大加快IR系统的响应速度．    </p>

<h3 id="section-3">将词典看作单一字符串的压缩方法</h3>
<p>采用如下的数据结构进行存储：一个定长的数组用于存储词项（２０Ｂ），４Ｂ的空间用于存储文档频率，４Ｂ的空间用于存储指向倒排记录表的指针．对于一个包含Ｍ个词项的文档空间来说，词典的总空间为M*(20+4+4),当Ｍ＝400,000时，占用空间为11.2MB.
<img src="../images/1/dic_compress_1.png" alt="image" /></p>

<p>这种方法存在很大的不足，首先大部分的英文词平均长度为8B,这显然造成了大部分的空间浪费，其次也存在有些词的长度超过20B,导致的结果便是不能存储这些词．</p>

<p>我们可以采用如下的改进措施，我们建立一个字符串在存储字典中的所有词项,4B的空间存储文档频率,4B的空间存储倒排记录表的指针，这个指针指向前面所有词典构成的长字符串，在长字符串中我们需要每一个词加入一个定位指针，用于指定下一个词的开始位置和当前词的结束位置，由于有400,000个词，每个词为８B,所以寻址空间为400,000<em>8=3.2</em>$10^6$,所以可以用一个长为$\log{3.2<em>10^6}$$\approx$22b，即３Ｂ的指针来表示．词典的总空间为M</em>(4+4+3+8)=7.6MB．  <br />
<img src="../images/1/1.png" alt="image" /></p>

<h3 id="section-4">按块存储</h3>
<p>对上面的压缩方法进行一个变形，这里不再对每个词项都维护一个指向字符串(所有词的组合)的指针．我们首先将我们的词典按块进行划分，例如每５个词为一块，这样对没一个块只需要维护一个这个块指向字符串的指针，同时在长字符串中，我们需要加入一个空间用于指定当前词的长度．在这种机制下，假设一个块内有ｋ个词，我们减少了(k-1)个指针的空间，但是我们需要在字符串中对没个词增加空间以记录其词的长度．假设每个块内有４个词，减少的指针空间为9B,同时对４个词需要增加４Ｂ的空间用于记录词的长度，所以没４个词产生了5B的压缩，所以压缩的空间为400,000*$\frac{1}{4}$*5=0.5MB.    </p>

<p><strong>注意：</strong>我们在这里维护了两个指针，一个用于指向倒排记录表，一个指向字符串用词项的位置，我们压缩的部分是词项指针．   </p>

<p><img src="../images/1/2.png" alt="image" /></p>

<p>我们发现，每个块内的词越多时，则可以压缩的空间越大．但是并非块内词越多越好，在进行词项查找时，对于块间的词我们可以通过二分查找快速定位，但是在快内查找时则是简单的线性遍历，所以我们必须在查找速度和空间压缩见进行权衡．    </p>

<h2 id="section-5">倒排记录表的压缩</h2>
<hr />

<p>倒排记录表的压缩基于下面一个前提，当用文档ID来表示倒排记录表，对与高频词来说，倒排记录表中的记录多并且相邻的记录之间差距会很小，当某高频词出现在某篇文档中时，将其相近的文档中出现高频词的概率会很大．这就给我们提供了对倒排记录表进行压缩的灵感，正常情况下我们对到拍记录表中的每个文档id,都是用定长的空间来存储的，那么对那些高频词的话，我可以通过存储他们倒排记录表相邻的距离（明显小于存储文档id的长度）来达到压缩的目的．     </p>

<h3 id="section-6">可变字节码</h3>
<p>VB(Variable byte,可变字节)码的思想为，我们采用整数个字节来存储文档id,每个字节的后７位为有效编码，第一位为延续位，表示本次编码的结束与否,’1’表示结束．     </p>

<p>可变字节码的解码过程如下，根据延续位（一直获取字节直到字节的首位为１）来获取编码结果，对编码结果进行以下处理，去除所有的延续位，剩余有效编码表示间隔位，将此编码值与前一个编码的结果进行累加即表示文档的ID. 
<img src="../images/1/3.png" alt="image" /></p>

<h3 id="section-7">γ编码</h3>

<p>一元编码：将数值为ｎ的数用ｎ个１并在之后加上一个０来表示的编码方式．  </p>

<p>γ编码主要由两部分组成，偏移量(offset)和长度(length)．长度是数组的二进制编码，但是去除了首位１，长度则是偏移量的长度，但是是通过一元编码的方式实现．对于数值5,二进制编码是101,去掉首位的１，其偏移量是01,偏移量长度为２，则由一元编码表示为110,所以数值５的γ编码为11001.
<img src="../images/1/4.png" alt="image" /></p>

<p>我们发现对数值为Ｋ的数进行二进制编码，其偏移量的长度为$\lfloor\log{k}\rfloor$,其长度的长度为$\lfloor\log{k}\rfloor$+1,所以，数的γ编码长度为２$\lfloor\log{k}\rfloor$＋１．      </p>

<p>有一点不太明白的是采用γ编码是如何实现数据压缩的呢？书的原文是这么说的：      <br />
<img src="../images/1/5.png" alt="image" />
按上面理解，采用ｎ为进行进行表示，那么间距在1~$2^\left(n-1\right)－１$之间都将产生浪费，并在在间距为$2^n$时不能表示．</p>

<p>我的理解是这样的，我们有对倒排记录表的实现一般也是通过链表或是定长数组来实现的，当采用定长的编码格式来存储每个文档ＩＤ时，必然会产生很大的浪费．那么如何通过变长的编码格式并且不需要额外的数组或指针来表明文档的长度，这边是γ编码所做的事了，让我们看一下γ编码的的解码过程：11001,我们首先遍历该编码，知道遇到０时停止，发现长度为２，剩下的偏移量为01,我们知道实际的二进制数为101,也就是说我们通过编码本身可以确定文档id,不需要进行额外的存储．</p>

<h4 id="section-8">参考资料</h4>
<p><a href="https://www.google.com.hk/search?q=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;oq=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;aqs=chrome..69i57j69i65j69i61l3j0.3218j0j1&amp;sourceid=chrome&amp;ie=UTF-8">索引压缩</a>     <br />
<strong>说明：</strong>文章主要内容和图片来自信息检索导论一书。</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-04-26T00:00:00-04:00"><a href="http://jwchennlp.github.com/odps-sql/">April 26, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/odps-sql/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/odps-sql/" rel="bookmark" title="odps_sql" itemprop="url">odps_sql</a></h1>
    
  </header>
  <div class="entry-content">
    <p>odps(open data processing service，开源数据处理服务)是阿里巴巴的分布式计算平台。</p>

<p>数据以sql表格的形式存放在odps中，我们可以是使用类似与sql命令的方式对数据进行操作。当让sql中嵌入了odps平台自己的函数和命令。</p>

<p>文档学习ing…</p>

<h2 id="section">表格建立</h2>

<p>创建表格语句如下：   </p>

<div class="highlight"><pre><code class="sql"><span class="lineno">1</span> <span class="k">create</span> <span class="k">table</span> <span class="n">if</span> <span class="k">not</span> <span class="n">exist</span> <span class="n">sales</span><span class="p">(</span>
<span class="lineno">2</span> <span class="n">shop_name</span>  <span class="n">string</span><span class="p">,</span>      
<span class="lineno">3</span> <span class="p">...</span>     
<span class="lineno">4</span> <span class="n">partitioned</span> <span class="k">by</span> <span class="p">(</span><span class="n">sale_date</span> <span class="n">string</span><span class="p">,</span><span class="n">region</span> <span class="n">string</span><span class="p">);</span>     
<span class="lineno">5</span>     <span class="c1">--创建一张分区表sales</span>
</code></pre></div>

<p>partitioned by指定了分区字段，采用分区字段主要是在跟新，新增和读取分区数据时不需要做全表扫描，可以提高效率。       </p>

<p>可以用命令create table…as select…来新建表格，如：        </p>

<div class="highlight"><pre><code class="sql"><span class="lineno">1</span> <span class="k">create</span> <span class="k">table</span> <span class="n">sales1</span> <span class="k">as</span>      
<span class="lineno">2</span>     <span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">sales</span><span class="p">;</span>    
</code></pre></div>

<p>这样在建立表格的同时，将sales的数据复制到新表中，但是原表格的分区字段没有复制到新表中。如果希望新表格和原表格有相同的数据和表结构（分区属性）.可以用create table… like …命令：    </p>

<div class="highlight"><pre><code class="sql"><span class="lineno">1</span> <span class="k">create</span> <span class="k">table</span> <span class="n">sales1</span> <span class="k">like</span> <span class="n">sales</span>  
</code></pre></div>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-04-24T00:00:00-04:00"><a href="http://jwchennlp.github.com/index-construction/">April 24, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/index-construction/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/index-construction/" rel="bookmark" title="索引构建" itemprop="url">索引构建</a></h1>
    
  </header>
  <div class="entry-content">
    <p>索引构建主要是对建立好的词典中的每个词项，构建词项关于文档集合的索引记录表。一般索引构建算法会受硬件设施的制约。    </p>

<h2 id="section">硬件基础</h2>
<hr />
<p>构建信息检索系统时，很多决策都依赖于系统所运行的硬件环境。与信息检索系统相关的硬件基本性能参数如下：      </p>

<ul>
  <li>系统访问内存中数据的速度比访问硬盘中数据要快的多，访问内存中的一个字节只需要几个时钟周期（大约5×10-9s），从磁盘传输一个字节的时间则长得多（大概2×10-8s）。因此，为了更快的响应速度，我们应该尽可能将数据放在内存中，特别是那种频繁访问的数据。这种将频繁使用的数据放入内存中的机制称为caching（缓存）。    </li>
  <li>进行磁盘读写时，磁头移动到数据所在的磁道需要一定的时间，该时间称为寻道时间，对典型的磁盘来说平均在5ms左右。寻道过程中并不进行数据的传输。于是，为了时数据传输率最大，连续读取的数据块也应该在磁盘上连续存放。      </li>
  <li>操作系统往往以数据块为单位进行读写。因此，从磁盘读取一个字节和一个数据块所耗费的时间可能一样多。我们将内存中保存读写块的那块区域称之为缓冲区（buffer）。       </li>
  <li>数据从磁盘传输到内存是由系统总线而不是处理器来实现的，这以为着在磁盘I/O时处理器仍然可以处理数据。我们可以利用这一点来加速数据的传输过程，比如将数据压缩后存储在磁盘上。假定采用一种高效的解压缩算法的话，那么从磁盘读取压缩数据再解压缩所花时间往往比直接读取未压缩数据所花时间少。       </li>
</ul>

<h2 id="section-1">基于块的排序索引方法</h2>
<hr />
<p>第一章建立倒排索引时，所有的处理过程都是在内存中完成的。我们将文档一次性读入内存，而后建立文档的词典，并建立词典中的词项的倒排记录表。如果当文档集过大，大到难以一次性读入内存时，上述方法便失效。   </p>

<p>由于内存的不足，我们可以采用磁盘的外部排序的方法（external sorting algorithm）。我们知道读取数据过程中的寻道时间与数据传输相比是十分耗时的，所以我们应该尽量将数据按块的方式存储以减少寻道的次数。BSBI（blocked sort-based indexing algorithm，基于块的排序索引算法）是一种解决的方法。算法实现如下：     <br />
第1步：将文档切分成均匀的若干个部分。   <br />
第2步：对每个部分的词项ID-文档ID对排序。  <br />
第3步：将中间产生的临时排序结果存储在磁盘上。   <br />
第4步：将所有的中间文件合并形成最终结果。   </p>

<p>在第2步中我们将词项用词项id代替，词项id是能代表词项的唯一标识，这样做能提高索引构建效率。</p>

<div class="highlight"><pre><code class="python">      
           
<span class="n">BSBIndex</span><span class="o">-</span><span class="n">construction</span><span class="p">()</span>     
<span class="n">n</span> <span class="o">&lt;-</span> <span class="mi">0</span>      
<span class="k">while</span><span class="p">(</span><span class="nb">all</span> <span class="n">documents</span> <span class="n">have</span> <span class="ow">not</span> <span class="n">been</span> <span class="n">processed</span><span class="p">)</span>        
<span class="n">do</span> <span class="n">n</span> <span class="o">&lt;-</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span>                 
    <span class="n">block</span> <span class="o">&lt;-</span> <span class="n">ParseNextBlock</span><span class="p">()</span>       
    <span class="n">BSBI</span><span class="o">-</span><span class="n">INVERT</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>            
    <span class="n">WriteBlockToDisk</span><span class="p">(</span><span class="n">block</span><span class="p">,</span><span class="n">fn</span><span class="p">)</span>      
    <span class="n">MergeBlocks</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span><span class="o">....</span><span class="p">,</span><span class="n">fn</span><span class="p">;</span><span class="n">fmerge</span><span class="p">)</span>      
             
</code></pre></div>

<p>合并时，同时打开所有块对应的文件，内存中维护10个块的读缓冲区和一个为最终合并索引准备的写缓冲区。每次迭代中，利用优先级队列（即堆结构）或者类似的数据结构选择最小的未处理词项id进行处理。读入该词项的倒排记录表进行合并，合并结果返回磁盘中。需要时，再次从文件中读入数据到每个读缓冲区。  </p>

<p>由于该算法主要的时间耗费在排序上，因此其时间复杂度为O（T*logT),其中T是要排序的项目数的上界（即词项ID-文档—ID对的个数），然而，实际的索引构建的时间往往取决与文档温习（ParseNextBlock）和最后合并（MergeBlocks）。</p>

<p>由于我们知道，为了提高索引构建效率，我们将词项映射成词项ID，初始的倒排记录表形式为：     </p>

<div class="highlight"><pre><code class="python">      
    <span class="n">wordi</span> <span class="o">-&gt;</span> <span class="n">doc1</span><span class="o">|</span><span class="n">doc2</span><span class="o">.....|</span><span class="n">docn</span>                    
</code></pre></div>

<p>在进行词项id的映射之后，每个词项ID-文档ID对就是简单的（wid,did）的形式了。这样做为什么能提高效率呢？虽然映射过程需要话费一定的时间，可是映射之后，每个块得到的都是这样的二值对，这样可以以词项ID为主键，以文档ID为次键按照快速排序一类方法进行排序，这样使得倒排记录的构建变得简单。     </p>

<h2 id="section-2">内存式单遍扫描索引构建方法</h2>
<hr />
<p>BSBI方法需要将词项映射成词项ID，所以必须在内存中维护一个（词项，词项ID）表的数据结构。当对大规模文档来说，这种数据结构的大小将超过内存大小。
SPIMI（single-pass in-memory indexing,内存式单遍扫描索引构建算法）使用词项而不是词项ID作为词典，它将为个块的词典读入磁盘，对于下一个块则采用新的词典。只要硬盘空间足够大，SPIMI就能够索引任何大小的文档集。     </p>

<p>SPIMI算法流程如下所示：      </p>

<div class="highlight"><pre><code class="c"><span class="lineno"> 1</span> <span class="n">SPIMI</span><span class="o">-</span><span class="n">Invert</span><span class="p">(</span><span class="n">token</span><span class="o">-</span><span class="n">stream</span><span class="p">)</span>      
<span class="lineno"> 2</span> <span class="n">output_file</span> <span class="o">=</span> <span class="n">NewFile</span><span class="p">()</span>     
<span class="lineno"> 3</span> <span class="n">dictionary</span> <span class="o">=</span> <span class="n">NewHash</span><span class="p">()</span>      
<span class="lineno"> 4</span> <span class="k">while</span><span class="p">(</span><span class="n">free</span> <span class="n">memory</span> <span class="n">available</span><span class="p">)</span>        
<span class="lineno"> 5</span> <span class="k">do</span> <span class="n">token</span> <span class="o">&lt;-</span> <span class="n">next</span><span class="p">(</span><span class="n">token</span><span class="o">-</span><span class="n">stream</span><span class="p">)</span>      
<span class="lineno"> 6</span>     <span class="k">if</span> <span class="n">term</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="n">not</span> <span class="n">in</span> <span class="n">dictionary</span>        
<span class="lineno"> 7</span>         <span class="n">then</span> <span class="n">posting_list</span> <span class="o">=</span> <span class="n">AddToDictionary</span><span class="p">(</span><span class="n">dictionary</span><span class="p">,</span><span class="n">term</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>     
<span class="lineno"> 8</span>         <span class="k">else</span> <span class="n">posting_list</span> <span class="o">=</span> <span class="n">GetPostingList</span><span class="p">(</span><span class="n">dictionary</span><span class="p">,</span><span class="n">term</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>      
<span class="lineno"> 9</span>         <span class="k">if</span> <span class="n">full</span><span class="p">(</span><span class="n">posting_list</span><span class="p">)</span>       
<span class="lineno">10</span>         <span class="n">then</span> <span class="n">posting_list</span> <span class="o">=</span> <span class="n">DoublePostingList</span><span class="p">(</span><span class="n">dictionary</span><span class="p">,</span><span class="n">term</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>       
<span class="lineno">11</span>         <span class="n">AddToPostingList</span><span class="p">(</span><span class="n">posting_list</span><span class="p">,</span><span class="n">dicID</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>     
<span class="lineno">12</span> <span class="n">sorted_term</span> <span class="o">&lt;-</span> <span class="n">SortTerms</span><span class="p">(</span><span class="n">dictionary</span><span class="p">)</span>        
<span class="lineno">13</span> <span class="n">WriteBlockToDisk</span><span class="p">(</span><span class="n">sorted_term</span><span class="p">,</span><span class="n">dictionary</span><span class="p">,</span><span class="n">output_file</span><span class="p">)</span>        
<span class="lineno">14</span> <span class="k">return</span> <span class="n">output_file</span>      
</code></pre></div>

<h4 id="bsbispimi">BSBI和SPIMI的区别</h4>
<p>BSBI在读入一块内存中的文档内容时，会构建这块文档的词项ID—文档ID对序列，在对序列进行排序后，构建这块文档的倒排索引表，也就是说倒排索引的构建是对读入的整个文件块这个整体。SPIMI当然也是将初始大规模文档划分成等大小的块，并按块读入内存，新建一个初始为空的字典，首先他直接以词项作为词典单位，也就是说在遍历内存中的文档时，对文档进行词条话和词干化后，查看每个词，如果这个词不再字典中，则将词加如词典中，并新建一个关于此词项的倒排记录表，如果词项在字典中存在，则需要在此词项的到拍记录表的基础上进行添加操作。由于实现并不清楚每个词项的倒排记录表的长度，所以初始设定倒排记录表的长度为某个较小的值，当倒排记录表已满时，可以按倍数进行扩展。</p>

<p>SPIMI的倒排记录表是动态增长的，同时立刻就可以实现全体倒排记录表的收集。这样做有两个好处： </p>

<ul>
  <li>由于不需要排序操作，所以处理的速度更快。  </li>
  <li>由于保留了倒排记录表对词项的归属关系，因此能够节省内存，词项的ID也不需要保存。</li>
</ul>

<p>SPIMI算法的时间负责度是O（T），因为它不需要对词项ID-文档ID排序，所以操作最多和文档集大小成线性关系。</p>

<h2 id="section-3">分布式索引构建方法</h2>
<hr />
<p>实际中，文档集合一般相当大，一台计算机很难实现高效的实现索引构建。尤其是对于万维网来说。因此Web搜索引擎通常使用分布式索引构建（distribuction index）算法来构建索引，其索引结果也是分布式的，它往往按照词项或是文档分割后分布在多台计算机上。   </p>

<p>这里介绍的分布式索引构建方法是MapReduce的一个应用。MapReduce是一个分布式的计算框架，它面向大规模计算机集群而设计。集群中有一个主控节点（master node）,主要负责任务在工作节点的分配和重分配。重分配是实现分布式框架的鲁棒性，因为集群在工作当中，可能工作节点会出现故障，这个时候主节点应当能识别这些故障并将故障机器的任务重新分配给其它可工作的工作节点。    </p>

<p>一般来说MapReduce会通过键-值对（Key-Value pair）的转换处理，将一个大型的计算问题转换成较小的子问题。在索引构建中，键-值对就是（词项ID,文档ID）。在分布式索引构建中，词项到词项ID的映射同样要分布式进行，因此分布式的索引构建方法要比单机上的索引构建方法复杂的多。一种简单方法就是维护一张高频词到其ID的映射表，并将它复制到所有节点的计算机上，而对低频词则直接使用词项本身。</p>

<p>MapReduce的Map过程将输入的数据片映射成键值对，这个映射过程对英语BSBI和SPIMI算法中的分析任务，执行Map过程的机器也称之为分析器（parser）。每个分析器将输出结果保存在本地的中间文件。</p>

<p>Reduce主要是对中间结果进行合并，形成最终的索引。对每个词项（键值），获取此词项的所有文档集合并构建词项的倒排记录表主要通过倒排器来实现。   </p>

<h2 id="section-4">动态索引构建方法</h2>
<hr />
<p>上述建立索引的方法都是基于静态文档的，在很多情况下，文档都会随着时间动态变化的。那么，当文档更新速度很慢时，我们可以采用定期更新索引的策略。如果文档更新速度很快时，则实时更新索引的方法将十分耗时。</p>

<p>可以采用如下方法实现动态索引的构建，这里我们主要维护两个索引，第一个主索引是对初始的文档集构建的索引，第二个辅助索引是在主索引建立之后随着时间推移，而更新的索引，辅助索引存放在内存中，这样实时检索时通过查询主索引和辅助索引实现。如果是对主索引在未来时间的更新，可以通过一个无效位向量实现，用无效位向量来标致文档的删除，同时在辅助索引中加入此文档的更新，便实现了主索引内容的更新。同时随着时间的推移，辅助索引的容量是不断增大的。当辅助索引长度大一某一值时，我们可以将辅助索引并入到主索引中。</p>

<p>将辅助索引并入主索引的开销主要取决于索引为文件中的存储方式。如果将每个词项对应的倒排记录表存储为一个文件，则此词项的辅助索引和主索引的合并通过简单的将辅助索引扩展到主索引的倒排记录表即可。显示情况是因为文件管理的各种限制，将所有词项的倒排记录表分别存储为文件是不可行的。替代方案是将所有词项的倒排记录表存储为一个大的文件。</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-04-22T00:00:00-04:00"><a href="http://jwchennlp.github.com/dic-and-fault-tolerance-retrieve/">April 22, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/dic-and-fault-tolerance-retrieve/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/dic-and-fault-tolerance-retrieve/" rel="bookmark" title="词典及容错式检索" itemprop="url">词典及容错式检索</a></h1>
    
  </header>
  <div class="entry-content">
    <h2 id="section">词典搜索的数据结构</h2>

<p>在给定倒排索引和查询，首要任务是确定查询中的各个查询词是否在词汇表中，如果在则返回该词所对应的倒排记录表的指针。词汇表的查找操作通常采用一种称之为词典的数据结构，主要有两种解决方案：<strong>哈希表方式</strong>和<strong>搜索树方式</strong>。通常在选择选用何种解决方式时，我们需要考虑如下问题：                       </p>

<ul>
  <li>关键字的数目    </li>
  <li>关键字的数目是经常变化还是相对固定，在变化的情况下，是只插入新关键字还是同时要删除某些旧关键字。  </li>
  <li>不同关键字的相对访问频率如何。   </li>
</ul>

<p>对于哈希表，词汇表中的每个词通过哈希函数映射成一个数，可以认为这个数代表这个词的存储地址。所以对于query里面的查询词来说，同样通过哈希函数应查看查询词映射到的地址，如果此地址存在数，则表示该查询词存在词典中。采用哈希表方式时，存在以下问题：          </p>

<ul>
  <li>哈希函数的空间要足够大，并且不易扩展。哈希函数必须要有足够大的空间来               存储字典，同时它的空间很难实时扩展，如需扩展，需要更改哈希映射函数，使得整个数据结构都发生变化。 </li>
  <li>冲突问题的解决，因为哈希函数可能使得两个不同的词映射到统一地址，如何减少映射冲突也是一个要考虑的问题。   </li>
  <li>哈希表方式很难解决前缀式查询，因为在不知道整体词的情况下，哈希映射函数是失效的。  </li>
</ul>

<p>搜索树可以很好的解决上述问题，它支持前缀式查询。最出名的搜索树是二叉树，每个内部节点都有两个字节点。在二叉树中搜索词要从根节点开始，每个内部节点代表一个二值测试，测试的结果用于确定下一步应该搜索的子树。二叉树的平衡性是实现高效搜索的关键，，平衡二叉树指的是任何节点的两个子树的高度相差小于等于1.下图为一个二叉树表示的词典的例子。为了实现搜索树的平衡性，我们必须在加入增加或删除节点时对树进行处理以保持树的平衡性，这里用B-树实现。</p>

<p><img src="../images/binary-tree.png" alt="image" />   	</p>

<h2 id="section-1">通配符查询</h2>

<p>通配符通常用于以下情形：  <br />
1. 用户不确定查询查询词的拼写。 <br />
2. 用户知道某个查询词项可能有不同的拼写版本，并且要把包含这些版本的文档都查找出来。       <br />
3. 用户查找某个查询词项的所有变形，这些词项还做了词干还原，但是用户并不知道搜索引擎是否做了词干还原。  <br />
4. 用户不确定一个外来词或者短语的正确拼写形式。   </p>

<p>当通配符出现在一个查询词的尾部时，如ca<em>，则是需要查找词典中所有词前两个字母是ca的所有词的文档。我们可以通过搜索树来实现这一查找，在搜索树的根（root）节点,首先我们确定首字母为c所指定的分支，而后在以分支作为搜索树查询a所对应的分支，这样这个分支下的所有单词都为符号ca</em>查询的单词。   <br />
然后，当通配符出现在词的首部时，如<em>ay,需要查找词典中后两个字母是ay的所有词项，显然用之前的搜索树不能实现这一查询。这里我们可以引入词典的反向B-树结构。前面的词典的B-树的构建是从词项的首字母开始，接着词的第二个字母知道最后一个字母构建B-树。反向B-树恰恰相反，它是从词典的尾字母开始，依次到倒数第二个字母直到第一个字母构建B-树。这样的反向B-树便能匹配通配符出现在词首部的查询。
那么对于通配符出现在查询词中间的查询，如t</em>o,我们可以采用如下策略，首先用构建的B-树查找t<em>的所有词，而后采用构建的反向B-树查找符合</em>o的所有词，最后两个查询的词求交集便是所查找的词。</p>

<h3 id="section-2">轮排索引</h3>
<p>轮排索引是一种用于一般通配符查询的索引，它是倒排索引的一种特殊方式。它的工作原理为，首先引入一终结符$，对词项集合中的每个词在其末尾增加$符号。如词项hello扩展成hello$,随后我们需要按如下方式建立词的轮排索引，对词hello$连续进行首位翻转，将出现的所有形式记录的集合称之为轮排词汇表。hello$的轮排索引如图所示: <br />
<img src="../images/permutern-index.png" alt="image" />   <br />
那么如何用轮排索引实现通配符查询呢，由上图我们知道轮排索引中的任何一个状态都指向词项hello，也就是说ello$h或者llo$he的查询过程都会通过轮排索引指向词项hello的查询过程。所以例如查询通配符h<em>llo,处理的关键是将通配符</em>移动到词的尾部，将h<em>llo转换成h</em>llo$,接着进行翻转得到llo$h<em>,接着在轮排索引中查找该字符串，我们发现llo$h</em>与hello词的轮排索引中的llo$he一致，所以hello是满足条件的查询结果。     <br />
对于查询中存在多个通配符的情况，如查询（fi<em>mo</em>er）,我们可以按如下方式进行处理，首先查找er$fi*的所有结果，接着可以通过穷举法过滤出包含mo的词，这些词便是符合通配符查询的结果。 </p>

<h3 id="k-gram">支持通配符查询的k-gram索引</h3>
<p>上面介绍的轮排索引结构简单，但是在构建轮排索引的过程中，我们需要对词进行旋转并记录所有旋转的结果，这会引起存储空间的急剧增加。     </p>

<p>k-gram索引是如下的倒排索引机制，它将原始词典中的所有词项进行拆分，每个词项都拆分成若干个长度为k的新的词项，并根据这些新的词项构建倒排索引，如happy按照3-gram拆分成的新词词项有$ha,hap,app,ppy,py$,这里用$来对词的开始和结束进行标记。这里倒排索引的构建方式与第一章提到的略微不同，之前的倒排索引是词典是文档中经过词条化和语言话处理的所有词，而倒排记录表是这些词所出现的文档。而这里词典则是文档中的所有词根据k-gram拆分的所有新词，倒排记录表这是包含这些长度为k的新词的原始词。如3-gram的新词etr对应的倒排记录表为,词项为etr，倒排记录表为所有包含etr的词：    </p>

<p><img src="../images/k-gram-index.png" alt="image" />    </p>

<p>那么k-gram是如何实现通配符查询的呢，如查询he<em>lo,是要查询首字符为he，尾字符为lo的所有词，根据3-gram索引，我们可以够找如下的布尔查询$heANDlo$,则3-gram的查询词便是所期望的
结果。k-gram索引有时也会导致非预期的结果，如查询red</em>,根据3-gram索引构建的布尔查询为$reANDred,其返回结果可能包含retired，但显然这个词并不符合初始期望。为了解决这一问题，我们可以引入一个后过滤的步骤，实现方式很简单，用初始的查询词与返回的词进行匹配，那些成功匹配的词便是符合要求的词。<br />
通配符查找往往是非常耗时的，对于单个通配符查询，我们可能要构建轮排索引或者k-gram索引来返回中间结果，并且对这中间结果要求交集来返回确切的要查找的词，最后才依据这些词通过倒排索引来查找这些词所对应的文档。    </p>

<h2 id="section-3">拼写校正</h2>
<p>拼写校正是在用户输入某个查询词或查询短语时，用户能识别其中词的拼写错误并返回正确词的查询结果。</p>

<h3 id="section-4">拼写校正的实现</h3>

<p>对于大多数拼写校正算法而言，有以下两条基本规则：        </p>

<ul>
  <li>对于一个错误拼写的查询中，则需要在其所有正确的拼写中，返回最近的正确拼写的查询。</li>
  <li>当两个正确拼写查询临近度相等时，则需要返回更常见的那个正确查询。更常见可以通过以下两个方式衡量，可以统计文档集合中两个查询出现的次数，出现次数高的标记为“更常见”。也可以统计用户查询日志中两个查询的出现次数，出现次数更高的标记为”更常见”。  </li>
</ul>

<h3 id="section-5">拼写校正的方法</h3>
<p>词独立校正:不管查询是单个词还是多个词构成的短语，对查询的词的拼写校正是独立进行的，也就是说是上下文独立的，即某个词是否校正与上下文语境没有关联。校正方法主要有编辑距离方法和k-gram重合度方法。 <br />
上下文敏感校正:则是在校正过程中，会根据上下文信息来决定词的校正。</p>

<h4 id="section-6">编辑距离</h4>

<p>给定两个字符串S1和S2，两者的编辑距离定义为由S1转换成S2的最小编辑操作数。通常这些编辑操作包括：     </p>

<ul>
  <li>将一个字符插入字符串   </li>
  <li>将一个字符从字符串中删除  </li>
  <li>将字符串中的一个字符替换成另一个字符    </li>
</ul>

<p>可以在O(S1*S2）的时间复杂度下计算S1和S2之间的编辑距离，主要方法是采用动态规划的思想（类似于动态规划中的求最长公共子串问题），其中S1和S2以字符数组方式进行存放。整数矩阵m的行数和列书分表代表两个字符串的长度，算法在运行过程中不断填写矩阵元素。例如，在算法结束时，m[i,j]表示S1的前i个字符和S2的前j个字符的编辑距离。其代码实现如下：</p>

<div class="highlight"><pre><code class="python"> 
       
<span class="n">EditDistance</span><span class="p">(</span><span class="n">S1</span><span class="p">,</span><span class="n">S2</span><span class="p">)</span> 
<span class="nb">int</span> <span class="n">m</span><span class="p">[</span><span class="o">|</span><span class="n">S1</span><span class="o">|</span><span class="p">,</span><span class="o">|</span><span class="n">S2</span><span class="o">|</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>    
<span class="k">for</span> <span class="n">i</span> <span class="o">&lt;</span><span class="err">—</span> <span class="mi">1</span> <span class="n">to</span> <span class="o">|</span><span class="n">S1</span><span class="o">|</span>      
<span class="n">do</span> <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">o</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>       
<span class="k">for</span> <span class="n">j</span> <span class="o">&lt;</span><span class="err">—</span> <span class="mi">1</span> <span class="n">to</span> <span class="o">|</span><span class="n">S2</span><span class="o">|</span>      
<span class="n">do</span> <span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">j</span>
<span class="k">for</span> <span class="n">i</span> <span class="o">&lt;</span><span class="err">—</span> <span class="mi">1</span> <span class="n">to</span> <span class="o">|</span><span class="n">S1</span><span class="o">|</span>      
<span class="n">do</span> <span class="k">for</span> <span class="n">j</span> <span class="o">&lt;</span><span class="err">—</span> <span class="mi">1</span> <span class="n">to</span> <span class="o">|</span><span class="n">S2</span><span class="o">|</span>       
    <span class="k">if</span> <span class="n">S1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">S2</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>       
        <span class="n">k</span> <span class="o">=</span> <span class="mi">1</span>       
    <span class="k">else</span>        
        <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>       
    <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="nb">min</span><span class="p">{</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">k</span><span class="p">,</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span>        
<span class="k">return</span> <span class="n">m</span>    
</code></pre></div>

<p>加入对于某个拼写错误查询q，我们需要从词典W中找出与q相邻最近的正确词项，最简单的方法是便利W中的所有词项wi，计算wi和q之间的编辑距离，最后返回和q最近编辑距离的词项wi，并返回wi所指向的文档。很显然，这种遍历的方法是十分低效的，我们可以采用如下的启发式优化策略，我们将搜索限定在首字母相同的词典词项上，对于查询q，我们认为错误不出现在首字符上，这样对于词典W，我们只计算与q有相同首字符的词项与q之间的编辑距离。当然在此基础上更复杂的方法是加入轮排索引，对于词错误拼写查询helo，忽略词的终结符$,构建词的轮排索引{helo，ohel，lohe，eloh}，对轮排索引中的每个词，按照上述的启发式规则与词典W中查找最近编辑距离的正确拼写。（个人理解）</p>

<h4 id="k-gram-1">拼写校正中的k-gram索引</h4>

<p>对与某一个错误拼写查询，我们可以根据之前的构建k-gram索引来实现拼写校正，过程如下：
对于错误的拼写单词，我们可以将此单词拆分成长度为k的多个字符串，并查找这些字符串所对应的倒排索引表，这些倒排索引表分别表示包含这些字符串的拼写正确的单词，这里我们认为，只要一个单词在在写倒排索引表中出现次数超过某一阀值m，则认为这个词是原错误拼写的正确拼写结果。例如错误拼写bord，其2-gram索引拆分成的新词有{$b,bo,or,rd,d$},这里去除词$b和d$,从文档集合中查找bo，or，rd对应的倒排记录表，如下所示：    </p>

<p><img src="../images/sc-k-gram-index.png" alt="image" /> </p>

<p>随后，我们只要遍历这些倒排记录表，找到那些在倒排记录表中出现次数高的词，便是正确的拼写词。k-gram索引的缺点像boardroom这种不可能是bord的正确拼写形式的词也会被检索出来。所以我们需要计算词汇表中词项与查询q之间的更精确的重合度计算方法。可以采用雅可比系数对先前的线性扫描合并方法进行修正。雅可比系数的计算公式是length(AandB)/length(AorB)，其中A和B分别表示查询q和词汇表词项中的k-gram集合。当扫描到词t时，计算出q和t的雅可比系数，如果系数大于某一阀值，则将词t返回。
采用雅可比系数进行验证的时候，我们需要知道q和t的k-gram索引，首先q的k-gram索引是已知的，那么在验证的过程中我们需要遍历所有q的k-gram索引中出现的词t的k-gram索引，如果穷举词t的k-gram索引是个缓慢的过程。我们可以通过一下方式来进行简化处理，当我们知道词t的长度时，可以认为他的k-gram长度为length(t)-k+1,这样能快速计算出雅可比系数。</p>

<h4 id="section-7">上下文敏感的拼写校正</h4>

<p>当查询的短语中每个单词都是正确的单词，但是返回的查询结果很少时，我们可以认为单词中存在拼写错误，并对其中的单词查找其正确的拼写结果，并返回修正后的短语的查询结果。当采用这种穷举法对词语中的词进行拼写校正时，工作量大，效率低。这时可以采用启发式的方法通过用户的查询日志来统计最有查询短语拼写校正后最有可能出现的短语。</p>

<h4 id="section-8">参考资料</h4>
<p><a href="https://www.google.com.hk/search?q=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;oq=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;aqs=chrome..69i57j69i65j69i61l3j0.3218j0j1&amp;sourceid=chrome&amp;ie=UTF-8">信息检索导论-词典及容错式检索</a><br />
<strong>说明：</strong>文章主要内容和图片来自信息检索导论一书。</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->


<div class="pagination">
  
    
      <a href="http://jwchennlp.github.com" class="btn">Previous</a>
    
  
  <ul class="inline-list">
    <li>
      
        <a href="http://jwchennlp.github.com">1</a>
      
    </li>
    
      <li>
        
          <span class="current-page">2</span>
        
      </li>
    
      <li>
        
          <a href="http://jwchennlp.github.com/page3">3</a>
        
      </li>
    
  </ul>
  
    <a href="http://jwchennlp.github.com/page3" class="btn">Next</a>
  
</div><!-- /.pagination -->
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2014 jwchen. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://jwchennlp.github.com/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://jwchennlp.github.com/assets/js/scripts.min.js"></script>

          

</body>
</html>