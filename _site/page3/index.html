<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Latest Posts &#8211; My blog</title>
<meta name="description" content="Describe this nonsense.">
<meta name="keywords" content="Jekyll, theme, themes, responsive, blog, modern">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Latest Posts">
<meta property="og:description" content="Describe this nonsense.">
<meta property="og:url" content="http://jwchennlp.github.com/page3/index.html">
<meta property="og:site_name" content="My blog">





<link rel="canonical" href="http://jwchennlp.github.com/page3/">
<link href="http://jwchennlp.github.com/feed.xml" type="application/atom+xml" rel="alternate" title="My blog Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://jwchennlp.github.com/assets/css/main.min.css">
<!-- Webfonts -->
<link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://jwchennlp.github.com/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://jwchennlp.github.com/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://jwchennlp.github.com/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://jwchennlp.github.com/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://jwchennlp.github.com/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://jwchennlp.github.com/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://jwchennlp.github.com/images/apple-touch-icon-144x144-precomposed.png">





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
                  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                          });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
<!--
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
-->

</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://jwchennlp.github.com">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://jwchennlp.github.com/images/avatar.jpg" alt="jwchen photo" class="author-photo">
					<h4>jwchen</h4>
					<p>不积跬步，无以至千里;不积小流，无以成江海</p>
				</li>
				<li><a href="http://jwchennlp.github.com/about/">Learn More</a></li>
				<li>
					<a href="mailto:hit1093710417@email.com"><i class="icon-envelope"></i> Email</a>
				</li>
				
				
				
				
				<li>
					<a href="http://github.com/jwchennlp"><i class="icon-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://jwchennlp.github.com/posts/">All Posts</a></li>
				<li><a href="http://jwchennlp.github.com/tags/">All Tags</a></li>
			</ul>
		</li>
		<li><a href="http://jwchennlp.github.com/theme-setup">Theme Setup</a></li><li><a href="http://mademistakes.com">External Link</a></li>
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  <div class="image-credit">Image source: <a href="http://www.dargadgetz.com/ios-7-abstract-wallpaper-pack-for-iphone-5-and-ipod-touch-retina/">dargadgetz</a></div><!-- /.image-credit -->
  
    <div class="entry-image">
      <img src="http://jwchennlp.github.com/images/abstract-1.jpg" alt="Latest Posts">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>My blog</h1>
      <h2>Latest Posts</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-05-08T00:00:00-04:00"><a href="http://jwchennlp.github.com/support-vector-machine/">May 08, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/support-vector-machine/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/support-vector-machine/" rel="bookmark" title="支持向量机" itemprop="url">支持向量机</a></h1>
    
  </header>
  <div class="entry-content">
    <h2 id="section">问题引出</h2>
<hr />
<p>在面对一个最简单的二分类问题，并且假设数据集可分的情况下．具体如下图所示．当我们采用逻辑回归实现分类时，我们用一个分类超平面（决策边界）对数据进行数据进行划分，并在划分后，不同类别的数据分布在分类超平面的两边，这表示分类成功．其实，在数据可分的情况下，我们发现可以有很多条这样的分类超平面，并且都能达到正确分类的效果,这个时候我们可能要问，这些分类超平面的效果一样吗？是否存在一个最优的分类超平面．
<img src="../images/140508/1.png" alt="image" /></p>

<h2 id="section-1">函数间隔和几何间隔</h2>
<hr />

<h3 id="section-2">函数间隔</h3>

<p>对于上面数据集，我们计算出了一个超平面$w^Tx+b=０$.对于某一个数据点x我们需要判断其内别,如果$w^Tx+b＞0$，则其类ｙ=1，并且如果$w^Tx+b＞0$并且$w^Tx+b$值越大，则这个点的类别为正例的置信度就越高．当$w^Tx+b＜0$时,点所属类别为－１，并且$w^Tx+b$值越小，这这个点类别为负例的置信度就越高．并且当点ｘ被正确分类时，$y(w^T+b)$为正数．从上面图中可以看出，我们设定分类超平面的上部为正例，A,B,C三个的点都被标记为正例，但是C离决策边界最近，可能稍微变化决策边界就可能导致分类错误，所以C分类正确的置信读低．A离决策边界最远，所以Ｃ被分为正例的置信读高.对于点$\left(x^\left(i\right),y^\left(i\right)\right)$为了获得更好的分类效果，我们希望$y\left(i\right)(w^Tx+b)$尽量大，则分类的置信度就越高．</p>

<p>所以，对数据$\left(x^\left(i\right),y^\left(i\right)\right)$，我们就定义$y\left(i\right)(w^Tx+b)$为此数据点的函数间隔，并且如果使得每个点的函数间隔都倾向于一个大值，则分类置信度越高，分类效果越好．   </p>

<p>给定训练集合S={($x^\left(i\right),y^\left(i\right)）$,i=1,…m},我们需要计算每一个样本点到分类超平面的函数间隔.在这里，我们需要求出最小的函数间隔，并且通过修改分类超平面使得最小函数间隔尽可能大，则分类效果更好．     </p>

<script type="math/tex; mode=display">\widehat{\gamma}=\min_{i=1,...,m}\widehat\gamma^\left(i\right)</script>

<p>利用函数间隔来衡量分类效果的置信度有一个缺陷，当我们确定某一个分类超平面$w^Tx+b=0$，我们对ｗ,b同时增加ｋ倍,函数间隔由原来的$y\left(w^Tx+b\right)$变成$ky\left(w^Tx+b\right)$．也就是说某一数据点的函数间隔可以可以任意的缩放或增加．</p>

<h3 id="section-3">几何间隔</h3>

<p>在确定分类超平面之后，任一数据点到分类超平面的距离应该是不变的．如果我们用这个距离来衡量分类置信度的话，效果会很好．    <br />
<img src="../images/140508/2.png" alt="image" />  <br />
点A到平面的距离设为$\gamma$,知道分类超平面的法向量为ｗ，那么将A投影到分类超平面上的点B的坐标为$x-\frac{w}{\left|w\right|}\gamma$,且点在决策边界$w^Tx+b=0$上，所以有:        </p>

<script type="math/tex; mode=display">w^T\left(x-\frac{w}{｜w｜}\gamma\right)+b=0</script>

<p>求解得$\gamma=\frac{w}{｜w｜}x+\frac{b}{｜w｜}$,所以对所有的样本点点$\left(x^\left(i\right),y^\left(i\right)\right)$,我们求得每个样本点的几何间隔为:      </p>

<script type="math/tex; mode=display">\gamma^\left(i\right)=y^\left(i\right)\left(\frac{w}{｜w｜}x^\left(i\right)+\frac{b}{｜w｜}\right)</script>

<p>给定训练集合S={($x^\left(i\right),y^\left(i\right)）$,i=1,…m},我们需要计算每一个样本点到分类超平面的几何距离.在这里，我们需要求出最小的几何距离，并且通过修改分类超平面使得最小几何距离尽可能大，则分类效果更好．        </p>

<script type="math/tex; mode=display">\gamma=\min_{i=1,...,m}\gamma^\left(i\right)</script>

<h2 id="section-4">最优间隔分类器</h2>

<p>当给定训练集之后，按照前面分析直观上最好的分类效果是找到决策边界使得(几何)间隔最大化.因为我们的决策是使得最小几何间隔最大化，则显然对所有点的分类的置信度很高．所以当对于一个线性可分的数据集，我们要通过一个分类超平面来分割所有的正例和负例．那么我们的问题可以转化成下面的优化问题:   </p>

<script type="math/tex; mode=display">\max_{\gamma,w,b}   \gamma \\
 s.t.    y^\left(i\right)\left(\frac{w}{|w|}x^\left(i\right)+\frac{b}{|w|}\right)\geq\gamma,i=1,...,m
</script>

<p>由于我们知道在确定了(w,b)之后，我们可以通过同比例的缩放或增加(w,b),所以我们可以通过相应的扩张比例使得｜w｜的值为１．所以优化问题转化成如下形式：     </p>

<script type="math/tex; mode=display">\max_{\gamma,w,b}   \gamma \\
 s.t.    y^\left(i\right)\left(wx^\left(i\right)+b\right)\geq\gamma,i=1,...,m　\\
 |w|=1
</script>

<table>
  <tbody>
    <tr>
      <td>通过约束</td>
      <td>w</td>
      <td>=1,使得函数间隔等于几何间隔．但是因为如上的优化问题是非凸问题，我们很难通过软件来进行求解．所以我们将第一个问题转化成如下问题:</td>
    </tr>
  </tbody>
</table>

<script type="math/tex; mode=display">\max_{\gamma,w,b}:  \frac{\widehat\gamma}{|w|} \\
 s.t.    y^\left(i\right)\left(wx^\left(i\right)+b\right)\geq\widehat\gamma,i=1,...,m
</script>

<p>其中$\widehat\gamma$代表的是最小函数间隔，我们知道函数间隔是可以通过(w,b)的同比例变化而变化，这里为了为了简化计算，我们将$\widehat\gamma$设为１．那么如上的优化问题变为:     </p>

<script type="math/tex; mode=display">\max_{\gamma,w,b}:  \frac{1}{|w|} \\
 s.t.    y^\left(i\right)\left(wx^\left(i\right)+b\right)\geq 1,i=1,...,m
</script>

<p>进一步转变，优化问题变成如下格式：       </p>

<script type="math/tex; mode=display">\min_{\gamma,w,b}: \frac{1}{2}w^2 \\
 s.t.    y^\left(i\right)\left(wx^\left(i\right)+b\right)\geq 1,i=1,...,m
</script>

<h2 id="section-5">拉格朗日算子</h2>

<p>对于如下的原始优化问题：    </p>

<script type="math/tex; mode=display"> \min_w:f(w) \\
s.t.:g_i(w)\leq0,i=1,...k  \\
h_i(w)=0,i=1,...l   
</script>

<p>其拉格朗日算子为<script type="math/tex">L(w,\alpha,\beta)=f(w)+\sum_\left(i=1\right)^k\alpha_ig_i(w)+\sum_\left(i=1\right)^l\beta_ih_i(w)</script>,其中$\alpha_i,\beta_i$为拉格朗日乘数．      <br />
考虑如下等式：     </p>

<script type="math/tex; mode=display">\theta_p(w)=\max_\left(\alpha,\beta:\alpha_i\geq0\right)L(w,\alpha,\beta)</script>

<p>其中ｐ表示原始的,我们发现当给定ｗ，并且ｗ满足我们原始问题的约束（$g_i(w)\leq0，h_i(w)=0$），如果ｗ违背这些约束，则显然$\theta_p(w)=\infty$,当ｗ满足原始问题约束时，$\theta_p(w)=０$．那么可以的出如下结论：  </p>

<script type="math/tex; mode=display">\min_w\theta_p(w)=\min_w\max_\left(\alpha,\beta:\alpha_i\geq0\right)L(w,\alpha,\beta)</script>

<p>同时我们定义$p^*=\min_w\theta_p(w)$为原始问题的解．   <br />
现在对应它的对偶问题：     </p>

<script type="math/tex; mode=display">\max_\left(\alpha,\beta:\alpha_i\geq0\right)\theta_d(p)=\max_\left(\alpha,\beta:\alpha_i\geq0\right)\min_wL(w,\alpha,\beta)</script>

<p>我们知道，最大最小问题的解小于最小最大问题的解：     </p>

<script type="math/tex; mode=display">d^*=\max_\left(\alpha,\beta:\alpha_i\geq0\right)\min_wL(w,\alpha,\beta)\leq\min_w\max_\left(\alpha,\beta:\alpha_i\geq0\right)L(w,\alpha,\beta)=p^*</script>

<p>当ｆ和$g_i$为凸函数时，$h_i$为仿射函数(仿射变换的定义是在几何空间中，一个向量空间进行一次线性变换并接上一个平移，变换成另一个向量空间)，有$d^＊=p^＊$，在这些约束下，一定存在一个$w^＊$是原始问题的解，$\alpha^＊,\beta^＊$是对偶问题的解，并且有$d^＊=p^＊=L(w^＊,\alpha^＊,\beta^＊)$,同时这３个参数满足KKT条件，KKT条件描述如下：    <br />
<img src="../images/140508/3.png" alt="image" />		</p>

<p>其中第三个约束称为对偶互补条件，并且当$a_i＞０$时，$g_i(w^*)=0$,满足这些条件的点所对应的几何间隔便是最小几何间隔．这些点称为支持向量.</p>

<p>现在回到优化边界分类器部分,我们的原始问题定义为：       </p>

<script type="math/tex; mode=display">\min_{\gamma,w,b}: \frac{1}{2}w^2 \\
 s.t.    y^\left(i\right)\left(wx^\left(i\right)+b\right)\geq 1,i=1,...,m
</script>

<p>约束条件可以表示为：        </p>

<script type="math/tex; mode=display">g_i(w)=-y^\left(i\right)(w^Tx\left(i\right)+b)+1\leq0</script>

<p><img src="../images/140508/4.png" alt="image" />    </p>

<p>我们可以看到，有最小几何间隔的点离决策边界最近．我们知道这些点$\left(x^\left(i\right),y\left(i\right)\right)$满足$g_i(w)=0$.我们将这些点称之为支持向量．从上图知道，数据集中有３个支持向量，一般来说支持向量的个数会明显小于数据集的大小，这在后面会相当有用．      </p>

<p>原始问题的拉格朗日算子可以表示为：      </p>

<script type="math/tex; mode=display">L(w,b,\alpha)=\frac{1}{2}w^2-\sum_{i=1}^m\alpha_i[y^\left(i\right)(w^Tx\left(i\right)+b)+1]</script>

<p>这个时候我们通过求对偶问题$\theta_p(w)$来求原始问题的解．具体方法是对Ｌ函数关于参数ｗ和ｂ求偏导数：    <br />
<img src="../images/140508/5.png" alt="image" />
<img src="../images/140508/6.png" alt="image" />
<img src="../images/140508/7.png" alt="image" />      <br />
将得到的约束代回到上面的拉格朗日算子中．得到：          <br />
<img src="../images/140508/8.png" alt="image" />
<img src="../images/140508/9.png" alt="image" />      <br />
最后原始问题的对偶优化问题可以定义为：            <br />
<img src="../images/140508/10.png" alt="image" /></p>

<p>现在假定我们已经求得对偶问题最优解的$\alpha_i$，那么我们是如何做预测的呢？对于一个新的数据点ｘ，我们知道预测是通过判断$w^Tx+b$的值来判定的，如果大于０，类别为正例，如果小于０，类别为负例,我们将上面求解的ｗ值代入：         <br />
<img src="../images/140508/11.png" alt="image" />		</p>

<p>我们知道$\alpha_i\geq0$,且根据KKT约束条件中的对偶互补条件$\alpha_ig_i(w)\geq0$,并且$\alpha&gt;0$时，$g_i(w)=0$,表示这些点有最小的几何间隔，也就是说这些点表示支持向量．我们知道，在判断ｘ类别的时候，我们只需要考虑$\alpha_i&gt;0$的情况，也对应的我们只需要考虑数据集中的支持向量．同时，支持向量想对于数据集来说是小很多的．这样很显然可以进行高效求解.</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-05-07T00:00:00-04:00"><a href="http://jwchennlp.github.com/generative-model-and-discriminative-model/">May 07, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/generative-model-and-discriminative-model/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/generative-model-and-discriminative-model/" rel="bookmark" title="生成模型和判别模型" itemprop="url">生成模型和判别模型</a></h1>
    
  </header>
  <div class="entry-content">
    <h2 id="section">定义</h2>
<hr />
<blockquote>
  <p>生成方法由数据学习联合概率分布P(x,y)，然后求出条件概率分布P(y｜ｘ)作为预测的模
型，即成生模型:  <br />
<img src="../images/140507/1.png" alt="image" /> <br />
这样的方法成为生成方法，是因为模型表示了给定输入ｘ产生输出ｙ的生成关系．典型的生成模型有，朴素贝叶斯和隐马尔可夫模型．  <br />
判别方法是由数据直接学习决策函数ｆ(x)或者条件概率分布P(y｜x)作为预测的模型，即判别模型，判别方法关心的是对给定的输入x,应该预测什么样的输出ｙ，典型的方法包括感知机，决策树，逻辑回归．   </p>
</blockquote>

<h2 id="section-1">理解</h2>
<hr />
<p>在面对猫狗分类问题时，我们该如何实现呢？  <br />
方法一：当我们利用逻辑回归或是感知机模型时，我们需要数据集所投射的空间中，找到一个决策边界，在决策边界一边的属于一类动物，在决策边界另一边的属于另一种动物．当来一个我们不知道的动物时，我们将它放入空间中，通过判断它在决策边界的那一侧来判断是猫还是狗． <br />
方法二：将数据集中的猫都拿出来，建立一个关于猫的特征的模型．按同样的方法建立一个关于狗的模型．这样，当判断一个动物时，我们分别查看它在猫模型中属于猫的概率和在狗模型中属于狗的概率，哪个值大，便说明属于哪个模型．   </p>

<p>方法一通过对数据集训练出一个模型，并通过判断P(y|x)下的条件概率来判断ｙ的类别．这种方法成为判别方法，对应建立的模型属于判别模型．   <br />
方法二对数据集的每一个类别建立一个模型，并通过联合概率P(x,y)来判断ｘ特征所应对应的类别．这种方法成为生成方法．  </p>

<p>其实通过联合概率来判断类别进行了一个变形，一般我们是要判断P(y|x)下的概率，可以进行如下转换： <br />
<img src="../images/140507/1.png" alt="image" />  <br />
对于某个参数ｘ，其概率值P(x)值在所有类别下都是相同的，所以问题便等同于如下问题：            <br />
<img src="../images/140507/2.png" alt="image" />        </p>

<p>不妨通过一个朴素贝叶斯生成模型来了解生成模型的判定过程．
如图，训练集包含４篇文档，我们需要验证测试集中的文档类别： <br />
<img src="../images/140507/3.png" alt="iamge" />    </p>

<p>我们需要计算每一个类别下P(x｜y)P(y)的概率，并且概率最大的那一类便是文档所属类别．即计算P((Chinese,chinese,Chinese,Tokyo,Japan)｜y=c)P(y=c)和P((Chinese,chinese,Chinese,Tokyo,Japan)｜y=$\bar{c}$)P(y=$\bar{c}$)．</p>

<p>之后利用朴素贝叶斯的的条件独立定义进行求解便能获知测试及属于哪个类别．</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-05-06T00:00:00-04:00"><a href="http://jwchennlp.github.com/index-compress/">May 06, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/index-compress/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/index-compress/" rel="bookmark" title="索引压缩" itemprop="url">索引压缩</a></h1>
    
  </header>
  <div class="entry-content">
    <h3 id="section">为什么要进行索引压缩？</h3>
<hr />
<p>进行索引压缩有以下优点：  </p>

<ul>
  <li>节省磁盘空间．  </li>
  <li>增加高速缓存(cache)的利用率.<br />
倒排索引词典是放在内存中的，倒排记录表放在磁盘上．对与到拍记录上的某些词项ｔ，我们是需要经常访问的，如果将这次词项ｔ所对应的到拍记录表压缩后放在高速缓存中，只要采用得当的解压缩算法，那么当查询词项ｔ的倒排记录表时，只需要访问cache，而不用从磁盘读取数据，能充分减少IR系统的响应时间． </li>
  <li>压缩能够加快从磁盘读取数据的速度．</li>
</ul>

<p>压缩技术分为有损压缩和无损压缩，有损压缩指的是压缩后，原始数据的所有信息都保存下来了．词干还原，大小写转换都属于有损压缩．   </p>

<h3 id="heaps">Heaps定律：词项数目的估计</h3>

<p>heaps定律认为，文档集大小和词汇量之间存在对数上的线性关系.它将词项的数目估计为文档集大小的函数:<script type="math/tex">M=kT^b</script>,其中Ｔ代表文档集合中的词条的个数． </p>

<p>不同文档集下ｋ取值差异较大，因为词汇量大小取决于文档本身以及对他进行处理的方式．当进行词干还原，大小写转换时将降低词汇量增长的速度，允许加入数字和容忍拼写错误则会增加增长率．无论参数取值如何，heaps定律满足一下两条性质：    </p>

<ul>
  <li>词汇量会随着文档集的增加而增加，不会趋于一个定值．     </li>
  <li>大规模文档集的词汇量也会很大．       </li>
</ul>

<h3 id="zipf">Zipf定律：词项在文档中的分布</h3>
<p>Zipf定律用于估计词项在文档中分布，假设$t_1$用于表示文档集中出现最多的词，$t_2$用于表示文档集中出现第二多的词，文档集合中出现第i多的词的文档频率$cf_i$与$\frac{1}{i}$成正比:   <br />
    <script type="math/tex">cf_i=k\frac{1}{i}</script></p>

<h2 id="section-1">词典压缩</h2>
<hr />

<h3 id="section-2">为什么要进行词典压缩</h3>
<p>理想情况下在建立好索引后，我们希望将词典存放在内存中，但是这往往很难实现，尤其是对于实用的搜索引擎和嵌入式系统．限制IR系统的响应之间的一个因素包多对磁盘的访问次数．所以，如果通过压缩来讲所有的或大部分的词典存入内存，将大大加快IR系统的响应速度．    </p>

<h3 id="section-3">将词典看作单一字符串的压缩方法</h3>
<p>采用如下的数据结构进行存储：一个定长的数组用于存储词项（２０Ｂ），４Ｂ的空间用于存储文档频率，４Ｂ的空间用于存储指向倒排记录表的指针．对于一个包含Ｍ个词项的文档空间来说，词典的总空间为M*(20+4+4),当Ｍ＝400,000时，占用空间为11.2MB.
<img src="../images/1/dic_compress_1.png" alt="image" /></p>

<p>这种方法存在很大的不足，首先大部分的英文词平均长度为8B,这显然造成了大部分的空间浪费，其次也存在有些词的长度超过20B,导致的结果便是不能存储这些词．</p>

<p>我们可以采用如下的改进措施，我们建立一个字符串在存储字典中的所有词项,4B的空间存储文档频率,4B的空间存储倒排记录表的指针，这个指针指向前面所有词典构成的长字符串，在长字符串中我们需要每一个词加入一个定位指针，用于指定下一个词的开始位置和当前词的结束位置，由于有400,000个词，每个词为８B,所以寻址空间为400,000<em>8=3.2</em>$10^6$,所以可以用一个长为$\log{3.2<em>10^6}$$\approx$22b，即３Ｂ的指针来表示．词典的总空间为M</em>(4+4+3+8)=7.6MB．  <br />
<img src="../images/1/1.png" alt="image" /></p>

<h3 id="section-4">按块存储</h3>
<p>对上面的压缩方法进行一个变形，这里不再对每个词项都维护一个指向字符串(所有词的组合)的指针．我们首先将我们的词典按块进行划分，例如每５个词为一块，这样对没一个块只需要维护一个这个块指向字符串的指针，同时在长字符串中，我们需要加入一个空间用于指定当前词的长度．在这种机制下，假设一个块内有ｋ个词，我们减少了(k-1)个指针的空间，但是我们需要在字符串中对没个词增加空间以记录其词的长度．假设每个块内有４个词，减少的指针空间为9B,同时对４个词需要增加４Ｂ的空间用于记录词的长度，所以没４个词产生了5B的压缩，所以压缩的空间为400,000*$\frac{1}{4}$*5=0.5MB.    </p>

<p><strong>注意：</strong>我们在这里维护了两个指针，一个用于指向倒排记录表，一个指向字符串用词项的位置，我们压缩的部分是词项指针．   </p>

<p><img src="../images/1/2.png" alt="image" /></p>

<p>我们发现，每个块内的词越多时，则可以压缩的空间越大．但是并非块内词越多越好，在进行词项查找时，对于块间的词我们可以通过二分查找快速定位，但是在快内查找时则是简单的线性遍历，所以我们必须在查找速度和空间压缩见进行权衡．    </p>

<h2 id="section-5">倒排记录表的压缩</h2>
<hr />

<p>倒排记录表的压缩基于下面一个前提，当用文档ID来表示倒排记录表，对与高频词来说，倒排记录表中的记录多并且相邻的记录之间差距会很小，当某高频词出现在某篇文档中时，将其相近的文档中出现高频词的概率会很大．这就给我们提供了对倒排记录表进行压缩的灵感，正常情况下我们对到拍记录表中的每个文档id,都是用定长的空间来存储的，那么对那些高频词的话，我可以通过存储他们倒排记录表相邻的距离（明显小于存储文档id的长度）来达到压缩的目的．     </p>

<h3 id="section-6">可变字节码</h3>
<p>VB(Variable byte,可变字节)码的思想为，我们采用整数个字节来存储文档id,每个字节的后７位为有效编码，第一位为延续位，表示本次编码的结束与否,’1’表示结束．     </p>

<p>可变字节码的解码过程如下，根据延续位（一直获取字节直到字节的首位为１）来获取编码结果，对编码结果进行以下处理，去除所有的延续位，剩余有效编码表示间隔位，将此编码值与前一个编码的结果进行累加即表示文档的ID. 
<img src="../images/1/3.png" alt="image" /></p>

<h3 id="section-7">γ编码</h3>

<p>一元编码：将数值为ｎ的数用ｎ个１并在之后加上一个０来表示的编码方式．  </p>

<p>γ编码主要由两部分组成，偏移量(offset)和长度(length)．长度是数组的二进制编码，但是去除了首位１，长度则是偏移量的长度，但是是通过一元编码的方式实现．对于数值5,二进制编码是101,去掉首位的１，其偏移量是01,偏移量长度为２，则由一元编码表示为110,所以数值５的γ编码为11001.
<img src="../images/1/4.png" alt="image" /></p>

<p>我们发现对数值为Ｋ的数进行二进制编码，其偏移量的长度为$\lfloor\log{k}\rfloor$,其长度的长度为$\lfloor\log{k}\rfloor$+1,所以，数的γ编码长度为２$\lfloor\log{k}\rfloor$＋１．      </p>

<p>有一点不太明白的是采用γ编码是如何实现数据压缩的呢？书的原文是这么说的：      <br />
<img src="../images/1/5.png" alt="image" />
按上面理解，采用ｎ为进行进行表示，那么间距在1~$2^\left(n-1\right)－１$之间都将产生浪费，并在在间距为$2^n$时不能表示．</p>

<p>我的理解是这样的，我们有对倒排记录表的实现一般也是通过链表或是定长数组来实现的，当采用定长的编码格式来存储每个文档ＩＤ时，必然会产生很大的浪费．那么如何通过变长的编码格式并且不需要额外的数组或指针来表明文档的长度，这边是γ编码所做的事了，让我们看一下γ编码的的解码过程：11001,我们首先遍历该编码，知道遇到０时停止，发现长度为２，剩下的偏移量为01,我们知道实际的二进制数为101,也就是说我们通过编码本身可以确定文档id,不需要进行额外的存储．</p>

<h4 id="section-8">参考资料</h4>
<p><a href="https://www.google.com.hk/search?q=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;oq=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;aqs=chrome..69i57j69i65j69i61l3j0.3218j0j1&amp;sourceid=chrome&amp;ie=UTF-8">索引压缩</a>     <br />
<strong>说明：</strong>文章主要内容和图片来自信息检索导论一书。</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-04-26T00:00:00-04:00"><a href="http://jwchennlp.github.com/odps-sql/">April 26, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/odps-sql/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/odps-sql/" rel="bookmark" title="odps_sql" itemprop="url">odps_sql</a></h1>
    
  </header>
  <div class="entry-content">
    <p>odps(open data processing service，开源数据处理服务)是阿里巴巴的分布式计算平台。</p>

<p>数据以sql表格的形式存放在odps中，我们可以是使用类似与sql命令的方式对数据进行操作。当让sql中嵌入了odps平台自己的函数和命令。</p>

<p>文档学习ing…</p>

<h2 id="section">表格建立</h2>

<p>创建表格语句如下：   </p>

<div class="highlight"><pre><code class="sql"><span class="lineno">1</span> <span class="k">create</span> <span class="k">table</span> <span class="n">if</span> <span class="k">not</span> <span class="n">exist</span> <span class="n">sales</span><span class="p">(</span>
<span class="lineno">2</span> <span class="n">shop_name</span>  <span class="n">string</span><span class="p">,</span>      
<span class="lineno">3</span> <span class="p">...</span>     
<span class="lineno">4</span> <span class="n">partitioned</span> <span class="k">by</span> <span class="p">(</span><span class="n">sale_date</span> <span class="n">string</span><span class="p">,</span><span class="n">region</span> <span class="n">string</span><span class="p">);</span>     
<span class="lineno">5</span>     <span class="c1">--创建一张分区表sales</span>
</code></pre></div>

<p>partitioned by指定了分区字段，采用分区字段主要是在跟新，新增和读取分区数据时不需要做全表扫描，可以提高效率。       </p>

<p>可以用命令create table…as select…来新建表格，如：        </p>

<div class="highlight"><pre><code class="sql"><span class="lineno">1</span> <span class="k">create</span> <span class="k">table</span> <span class="n">sales1</span> <span class="k">as</span>      
<span class="lineno">2</span>     <span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">sales</span><span class="p">;</span>    
</code></pre></div>

<p>这样在建立表格的同时，将sales的数据复制到新表中，但是原表格的分区字段没有复制到新表中。如果希望新表格和原表格有相同的数据和表结构（分区属性）.可以用create table… like …命令：    </p>

<div class="highlight"><pre><code class="sql"><span class="lineno">1</span> <span class="k">create</span> <span class="k">table</span> <span class="n">sales1</span> <span class="k">like</span> <span class="n">sales</span>  
</code></pre></div>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-04-24T00:00:00-04:00"><a href="http://jwchennlp.github.com/index-construction/">April 24, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/index-construction/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/index-construction/" rel="bookmark" title="索引构建" itemprop="url">索引构建</a></h1>
    
  </header>
  <div class="entry-content">
    <p>索引构建主要是对建立好的词典中的每个词项，构建词项关于文档集合的索引记录表。一般索引构建算法会受硬件设施的制约。    </p>

<h2 id="section">硬件基础</h2>
<hr />
<p>构建信息检索系统时，很多决策都依赖于系统所运行的硬件环境。与信息检索系统相关的硬件基本性能参数如下：      </p>

<ul>
  <li>系统访问内存中数据的速度比访问硬盘中数据要快的多，访问内存中的一个字节只需要几个时钟周期（大约5×10-9s），从磁盘传输一个字节的时间则长得多（大概2×10-8s）。因此，为了更快的响应速度，我们应该尽可能将数据放在内存中，特别是那种频繁访问的数据。这种将频繁使用的数据放入内存中的机制称为caching（缓存）。    </li>
  <li>进行磁盘读写时，磁头移动到数据所在的磁道需要一定的时间，该时间称为寻道时间，对典型的磁盘来说平均在5ms左右。寻道过程中并不进行数据的传输。于是，为了时数据传输率最大，连续读取的数据块也应该在磁盘上连续存放。      </li>
  <li>操作系统往往以数据块为单位进行读写。因此，从磁盘读取一个字节和一个数据块所耗费的时间可能一样多。我们将内存中保存读写块的那块区域称之为缓冲区（buffer）。       </li>
  <li>数据从磁盘传输到内存是由系统总线而不是处理器来实现的，这以为着在磁盘I/O时处理器仍然可以处理数据。我们可以利用这一点来加速数据的传输过程，比如将数据压缩后存储在磁盘上。假定采用一种高效的解压缩算法的话，那么从磁盘读取压缩数据再解压缩所花时间往往比直接读取未压缩数据所花时间少。       </li>
</ul>

<h2 id="section-1">基于块的排序索引方法</h2>
<hr />
<p>第一章建立倒排索引时，所有的处理过程都是在内存中完成的。我们将文档一次性读入内存，而后建立文档的词典，并建立词典中的词项的倒排记录表。如果当文档集过大，大到难以一次性读入内存时，上述方法便失效。   </p>

<p>由于内存的不足，我们可以采用磁盘的外部排序的方法（external sorting algorithm）。我们知道读取数据过程中的寻道时间与数据传输相比是十分耗时的，所以我们应该尽量将数据按块的方式存储以减少寻道的次数。BSBI（blocked sort-based indexing algorithm，基于块的排序索引算法）是一种解决的方法。算法实现如下：     <br />
第1步：将文档切分成均匀的若干个部分。   <br />
第2步：对每个部分的词项ID-文档ID对排序。  <br />
第3步：将中间产生的临时排序结果存储在磁盘上。   <br />
第4步：将所有的中间文件合并形成最终结果。   </p>

<p>在第2步中我们将词项用词项id代替，词项id是能代表词项的唯一标识，这样做能提高索引构建效率。</p>

<div class="highlight"><pre><code class="python">      
           
<span class="n">BSBIndex</span><span class="o">-</span><span class="n">construction</span><span class="p">()</span>     
<span class="n">n</span> <span class="o">&lt;-</span> <span class="mi">0</span>      
<span class="k">while</span><span class="p">(</span><span class="nb">all</span> <span class="n">documents</span> <span class="n">have</span> <span class="ow">not</span> <span class="n">been</span> <span class="n">processed</span><span class="p">)</span>        
<span class="n">do</span> <span class="n">n</span> <span class="o">&lt;-</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span>                 
    <span class="n">block</span> <span class="o">&lt;-</span> <span class="n">ParseNextBlock</span><span class="p">()</span>       
    <span class="n">BSBI</span><span class="o">-</span><span class="n">INVERT</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>            
    <span class="n">WriteBlockToDisk</span><span class="p">(</span><span class="n">block</span><span class="p">,</span><span class="n">fn</span><span class="p">)</span>      
    <span class="n">MergeBlocks</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span><span class="o">....</span><span class="p">,</span><span class="n">fn</span><span class="p">;</span><span class="n">fmerge</span><span class="p">)</span>      
             
</code></pre></div>

<p>合并时，同时打开所有块对应的文件，内存中维护10个块的读缓冲区和一个为最终合并索引准备的写缓冲区。每次迭代中，利用优先级队列（即堆结构）或者类似的数据结构选择最小的未处理词项id进行处理。读入该词项的倒排记录表进行合并，合并结果返回磁盘中。需要时，再次从文件中读入数据到每个读缓冲区。  </p>

<p>由于该算法主要的时间耗费在排序上，因此其时间复杂度为O（T*logT),其中T是要排序的项目数的上界（即词项ID-文档—ID对的个数），然而，实际的索引构建的时间往往取决与文档温习（ParseNextBlock）和最后合并（MergeBlocks）。</p>

<p>由于我们知道，为了提高索引构建效率，我们将词项映射成词项ID，初始的倒排记录表形式为：     </p>

<div class="highlight"><pre><code class="python">      
    <span class="n">wordi</span> <span class="o">-&gt;</span> <span class="n">doc1</span><span class="o">|</span><span class="n">doc2</span><span class="o">.....|</span><span class="n">docn</span>                    
</code></pre></div>

<p>在进行词项id的映射之后，每个词项ID-文档ID对就是简单的（wid,did）的形式了。这样做为什么能提高效率呢？虽然映射过程需要话费一定的时间，可是映射之后，每个块得到的都是这样的二值对，这样可以以词项ID为主键，以文档ID为次键按照快速排序一类方法进行排序，这样使得倒排记录的构建变得简单。     </p>

<h2 id="section-2">内存式单遍扫描索引构建方法</h2>
<hr />
<p>BSBI方法需要将词项映射成词项ID，所以必须在内存中维护一个（词项，词项ID）表的数据结构。当对大规模文档来说，这种数据结构的大小将超过内存大小。
SPIMI（single-pass in-memory indexing,内存式单遍扫描索引构建算法）使用词项而不是词项ID作为词典，它将为个块的词典读入磁盘，对于下一个块则采用新的词典。只要硬盘空间足够大，SPIMI就能够索引任何大小的文档集。     </p>

<p>SPIMI算法流程如下所示：      </p>

<div class="highlight"><pre><code class="c"><span class="lineno"> 1</span> <span class="n">SPIMI</span><span class="o">-</span><span class="n">Invert</span><span class="p">(</span><span class="n">token</span><span class="o">-</span><span class="n">stream</span><span class="p">)</span>      
<span class="lineno"> 2</span> <span class="n">output_file</span> <span class="o">=</span> <span class="n">NewFile</span><span class="p">()</span>     
<span class="lineno"> 3</span> <span class="n">dictionary</span> <span class="o">=</span> <span class="n">NewHash</span><span class="p">()</span>      
<span class="lineno"> 4</span> <span class="k">while</span><span class="p">(</span><span class="n">free</span> <span class="n">memory</span> <span class="n">available</span><span class="p">)</span>        
<span class="lineno"> 5</span> <span class="k">do</span> <span class="n">token</span> <span class="o">&lt;-</span> <span class="n">next</span><span class="p">(</span><span class="n">token</span><span class="o">-</span><span class="n">stream</span><span class="p">)</span>      
<span class="lineno"> 6</span>     <span class="k">if</span> <span class="n">term</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="n">not</span> <span class="n">in</span> <span class="n">dictionary</span>        
<span class="lineno"> 7</span>         <span class="n">then</span> <span class="n">posting_list</span> <span class="o">=</span> <span class="n">AddToDictionary</span><span class="p">(</span><span class="n">dictionary</span><span class="p">,</span><span class="n">term</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>     
<span class="lineno"> 8</span>         <span class="k">else</span> <span class="n">posting_list</span> <span class="o">=</span> <span class="n">GetPostingList</span><span class="p">(</span><span class="n">dictionary</span><span class="p">,</span><span class="n">term</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>      
<span class="lineno"> 9</span>         <span class="k">if</span> <span class="n">full</span><span class="p">(</span><span class="n">posting_list</span><span class="p">)</span>       
<span class="lineno">10</span>         <span class="n">then</span> <span class="n">posting_list</span> <span class="o">=</span> <span class="n">DoublePostingList</span><span class="p">(</span><span class="n">dictionary</span><span class="p">,</span><span class="n">term</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>       
<span class="lineno">11</span>         <span class="n">AddToPostingList</span><span class="p">(</span><span class="n">posting_list</span><span class="p">,</span><span class="n">dicID</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>     
<span class="lineno">12</span> <span class="n">sorted_term</span> <span class="o">&lt;-</span> <span class="n">SortTerms</span><span class="p">(</span><span class="n">dictionary</span><span class="p">)</span>        
<span class="lineno">13</span> <span class="n">WriteBlockToDisk</span><span class="p">(</span><span class="n">sorted_term</span><span class="p">,</span><span class="n">dictionary</span><span class="p">,</span><span class="n">output_file</span><span class="p">)</span>        
<span class="lineno">14</span> <span class="k">return</span> <span class="n">output_file</span>      
</code></pre></div>

<h4 id="bsbispimi">BSBI和SPIMI的区别</h4>
<p>BSBI在读入一块内存中的文档内容时，会构建这块文档的词项ID—文档ID对序列，在对序列进行排序后，构建这块文档的倒排索引表，也就是说倒排索引的构建是对读入的整个文件块这个整体。SPIMI当然也是将初始大规模文档划分成等大小的块，并按块读入内存，新建一个初始为空的字典，首先他直接以词项作为词典单位，也就是说在遍历内存中的文档时，对文档进行词条话和词干化后，查看每个词，如果这个词不再字典中，则将词加如词典中，并新建一个关于此词项的倒排记录表，如果词项在字典中存在，则需要在此词项的到拍记录表的基础上进行添加操作。由于实现并不清楚每个词项的倒排记录表的长度，所以初始设定倒排记录表的长度为某个较小的值，当倒排记录表已满时，可以按倍数进行扩展。</p>

<p>SPIMI的倒排记录表是动态增长的，同时立刻就可以实现全体倒排记录表的收集。这样做有两个好处： </p>

<ul>
  <li>由于不需要排序操作，所以处理的速度更快。  </li>
  <li>由于保留了倒排记录表对词项的归属关系，因此能够节省内存，词项的ID也不需要保存。</li>
</ul>

<p>SPIMI算法的时间负责度是O（T），因为它不需要对词项ID-文档ID排序，所以操作最多和文档集大小成线性关系。</p>

<h2 id="section-3">分布式索引构建方法</h2>
<hr />
<p>实际中，文档集合一般相当大，一台计算机很难实现高效的实现索引构建。尤其是对于万维网来说。因此Web搜索引擎通常使用分布式索引构建（distribuction index）算法来构建索引，其索引结果也是分布式的，它往往按照词项或是文档分割后分布在多台计算机上。   </p>

<p>这里介绍的分布式索引构建方法是MapReduce的一个应用。MapReduce是一个分布式的计算框架，它面向大规模计算机集群而设计。集群中有一个主控节点（master node）,主要负责任务在工作节点的分配和重分配。重分配是实现分布式框架的鲁棒性，因为集群在工作当中，可能工作节点会出现故障，这个时候主节点应当能识别这些故障并将故障机器的任务重新分配给其它可工作的工作节点。    </p>

<p>一般来说MapReduce会通过键-值对（Key-Value pair）的转换处理，将一个大型的计算问题转换成较小的子问题。在索引构建中，键-值对就是（词项ID,文档ID）。在分布式索引构建中，词项到词项ID的映射同样要分布式进行，因此分布式的索引构建方法要比单机上的索引构建方法复杂的多。一种简单方法就是维护一张高频词到其ID的映射表，并将它复制到所有节点的计算机上，而对低频词则直接使用词项本身。</p>

<p>MapReduce的Map过程将输入的数据片映射成键值对，这个映射过程对英语BSBI和SPIMI算法中的分析任务，执行Map过程的机器也称之为分析器（parser）。每个分析器将输出结果保存在本地的中间文件。</p>

<p>Reduce主要是对中间结果进行合并，形成最终的索引。对每个词项（键值），获取此词项的所有文档集合并构建词项的倒排记录表主要通过倒排器来实现。   </p>

<h2 id="section-4">动态索引构建方法</h2>
<hr />
<p>上述建立索引的方法都是基于静态文档的，在很多情况下，文档都会随着时间动态变化的。那么，当文档更新速度很慢时，我们可以采用定期更新索引的策略。如果文档更新速度很快时，则实时更新索引的方法将十分耗时。</p>

<p>可以采用如下方法实现动态索引的构建，这里我们主要维护两个索引，第一个主索引是对初始的文档集构建的索引，第二个辅助索引是在主索引建立之后随着时间推移，而更新的索引，辅助索引存放在内存中，这样实时检索时通过查询主索引和辅助索引实现。如果是对主索引在未来时间的更新，可以通过一个无效位向量实现，用无效位向量来标致文档的删除，同时在辅助索引中加入此文档的更新，便实现了主索引内容的更新。同时随着时间的推移，辅助索引的容量是不断增大的。当辅助索引长度大一某一值时，我们可以将辅助索引并入到主索引中。</p>

<p>将辅助索引并入主索引的开销主要取决于索引为文件中的存储方式。如果将每个词项对应的倒排记录表存储为一个文件，则此词项的辅助索引和主索引的合并通过简单的将辅助索引扩展到主索引的倒排记录表即可。显示情况是因为文件管理的各种限制，将所有词项的倒排记录表分别存储为文件是不可行的。替代方案是将所有词项的倒排记录表存储为一个大的文件。</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->


<div class="pagination">
  
    
      <a href="http://jwchennlp.github.com/page2" class="btn">Previous</a>
    
  
  <ul class="inline-list">
    <li>
      
        <a href="http://jwchennlp.github.com">1</a>
      
    </li>
    
      <li>
        
          <a href="http://jwchennlp.github.com/page2">2</a>
        
      </li>
    
      <li>
        
          <span class="current-page">3</span>
        
      </li>
    
      <li>
        
          <a href="http://jwchennlp.github.com/page4">4</a>
        
      </li>
    
  </ul>
  
    <a href="http://jwchennlp.github.com/page4" class="btn">Next</a>
  
</div><!-- /.pagination -->
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2014 jwchen. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://jwchennlp.github.com/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://jwchennlp.github.com/assets/js/scripts.min.js"></script>

          

</body>
</html>