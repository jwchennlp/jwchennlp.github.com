<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>参数评估方法 &#8211; My blog</title>
<meta name="description" content="good good study,day day up">
<meta name="keywords" content="参数评估, 主题模型, 极大似然估计, 最大后验, 贝叶斯估计">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="参数评估方法">
<meta property="og:description" content="good good study,day day up">
<meta property="og:url" content="http://jwchennlp.github.com/parameter-estimation-approaches/">
<meta property="og:site_name" content="My blog">





<link rel="canonical" href="http://jwchennlp.github.com/parameter-estimation-approaches/">
<link href="http://jwchennlp.github.com/feed.xml" type="application/atom+xml" rel="alternate" title="My blog Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://jwchennlp.github.com/assets/css/main.min.css">
<!-- Webfonts -->
<link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://jwchennlp.github.com/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://jwchennlp.github.com/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://jwchennlp.github.com/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://jwchennlp.github.com/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://jwchennlp.github.com/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://jwchennlp.github.com/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://jwchennlp.github.com/images/apple-touch-icon-144x144-precomposed.png">





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
                  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                          });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
<!--
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
-->

</head>

<body id="post" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://jwchennlp.github.com">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://jwchennlp.github.com/images/avatar.jpg" alt="jwchen photo" class="author-photo">
					<h4>jwchen</h4>
					<p>不积跬步，无以至千里;不积小流，无以成江海</p>
				</li>
				<li><a href="http://jwchennlp.github.com/about/">Learn More</a></li>
				<li>
					<a href="mailto:hit1093710417@email.com"><i class="icon-envelope"></i> Email</a>
				</li>
				
				
				
				
				<li>
					<a href="http://github.com/jwchennlp"><i class="icon-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://jwchennlp.github.com/posts/">All Posts</a></li>
				<li><a href="http://jwchennlp.github.com/tags/">All Tags</a></li>
			</ul>
		</li>
		<li><a href="http://jwchennlp.github.com/theme-setup">Theme Setup</a></li><li><a href="http://mademistakes.com">External Link</a></li>
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->



<div class="entry-header">
  <div class="image-credit">Image source: <a href="http://www.dargadgetz.com/ios-7-abstract-wallpaper-pack-for-iphone-5-and-ipod-touch-retina/">dargadgetz</a></div><!-- /.image-credit -->
  <div class="entry-image">
    <img src="http://jwchennlp.github.com/images/abstract-3.jpg" alt="参数评估方法">
  </div><!-- /.entry-image -->
</div><!-- /.entry-header -->


<div id="main" role="main">
  <article class="hentry">
    <header class="header-title">
      <div class="header-title-wrap">
        
          <h1 class="entry-title"><a href="http://jwchennlp.github.com/parameter-estimation-approaches/" rel="bookmark" title="参数评估方法">参数评估方法</a></h1>
        
        <h2>June 02, 2014</h2>
      </div><!-- /.header-title-wrap -->
    </header>
    <div class="entry-content">
      <p>pLSA和LDA主题模型是当前统计自然语言处理领域非常热门的问题,这些主题模型一般都是对文本的生成过程提出自己的概率图模型,然后利用已有的文本数据做参数评估.本文主要介绍其中会用到的三种参数评估方法,包括极大似然估计(MLE),最大后验(MAL)和贝叶斯估计.	</p>

<p>我们主要考虑两个推理问题:		</p>

<p>(1). 评估参数$\theta$的值以最好的拟合观察到的数据集X.		
(2). 根据已观测到的数据集X计算新的观察值$\widetilde x$的概率		</p>

<p>第一个问题可以看成是估计问题,第二个问题可以看成是预测或是回归问题.	在贝叶斯统计中,参数估计问题可以表述为:		</p>

<script type="math/tex; mode=display">p(\theta｜X) = \frac{p(\theta)p(X｜\theta)}{p(X)}</script>

<p>并且我们可以用如下术语定义:</p>

<script type="math/tex; mode=display">posterior=\frac{likelihood*prior}{evidence}</script>

<h2 id="mle">1.极大似然估计(MLE)</h2>
<hr />

<p>极大似然估计主要是求使似然函数值最大的参数值($\theta$),似然函数为:		</p>

<script type="math/tex; mode=display">L(\theta｜X) = \prod_{x\in X}p(x｜\theta)</script>

<p>对似然函数取对数并另偏导数为0,则可求得参数的解:		</p>

<script type="math/tex; mode=display">\theta_{ML} = argmax_{\theta}L(\theta｜X) = argmax_{\theta}\sum_{x\in X}\log{p(x｜\theta)}</script>

<p>对上述函数关于参数$\theta_k$求偏导并是的偏导值为0:		</p>

<script type="math/tex; mode=display">\frac{\partial L(\theta｜X)}{\partial \theta_k} = 0, \forall \theta_k\in theta</script>

<p>根据已观测到的数据集X计算新的观察值$\widetilde x$的概率为:		</p>

<p><img src="../images/140530/mle.png" alt="image" /></p>

<p>以抛硬币的贝努利实验为例,每次抛硬币时正面出现的概率为p(未知),假设进行抛硬币N次得到结果集合C.用极大似然估计来求解参数p:		</p>

<script type="math/tex; mode=display">l(p)=\sum_{i=1}^N\log{p(C=c_i｜p)} \\
	  =n^{(1)}log{p(C=1｜p)}	+n^{(0)}log{p(C=0｜p)} \\
	  =n^{(1)}log{p}+n^{(0)}log{(1-p)}</script>

<p>其中$n^{(1)}表示N次实验结果中正面出现的次数,n^{(0)}$表示反面出现的次数.对似然函数求偏导得:		</p>

<script type="math/tex; mode=display">\frac{\partial l}{\partial p} = \frac{n^{(1)}}{p}-\frac{n^{(0)}}{1-p}=0</script>

<script type="math/tex; mode=display">\hat{p}_{ML} = \frac{n^{(1)}}{n^{(1)}+n^{(0)}}=\frac{n^{(1)}}{N}</script>

<p>假设抛硬币20次,其中正面出现的次数为12次,则由极大似然估计得出正面出现的概率p=0.6,并且可以预测下一次抛硬币正面向上的概率为0.6．</p>

<h2 id="map">2.最大后验估计(MAP)</h2>
<hr />

<p>最大后验估计(Maximum a posteriori estimation,MAP)和极大似然估计十分相似,但是最大后验估计中加入了对参数的先验信念(priori belief),它的权重设定为先验分布$p(\theta)$,最大后验估计不是要求似然函数最大化,而是要求由贝叶斯公式算出的整个后验概率最大.		</p>

<p><img src="../images/140530/map.png" align="center" /></p>

<p>和极大似然估计相比,在最大后验估计中我们需要加入先验分布的对数.先验分布可以理解为我们对事物约定俗成的看法或普遍接受的规律.例如在抛硬币的过程中,如果硬币是一枚正常的硬币,我们认为每次抛硬币正面发生的概率应该服从一个概率分布,这个概率在0.5出取得最大值,这个分布就是先验分布.先验分布的参数我们成为超参数(hyperparameter),即:	</p>

<script type="math/tex; mode=display">p(\theta) = p(\theta｜\alpha)</script>

<p>通过最大化<script type="math/tex">L(\theta｜X)+\log{p(\theta)},可以得到MAP的参数估计值\hat{\theta}_{MAP}</script>.当根据已有的数据预测新数据x的概率时:		</p>

<script type="math/tex; mode=display">p(\hat{x}｜X) \approx \int_{\theta\in \Theta}p(\hat{x}｜\hat{\theta}_{MAP})p(\theta｜X)d\theta=p(\hat{x}｜\hat{\theta}_{MAP})</script>

<p>这里用beta分布来描述硬币的先验分布:		</p>

<script type="math/tex; mode=display">p(p｜\alpha,\beta) = \frac{1}{B(\alpha,\beta)}p^{\alpha-1}(1-p)^{\beta-1}</script>

<p>其中beta函数<script type="math/tex">B(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}</script>,函数<script type="math/tex">\Gamma</script>是Gamma函数,可以理解为实数域的阶乘函数:<script type="math/tex">x!=\Gamma(x+1)</script>,beta分布的变量取值范围为[0,1].下面为beta函数在不同参数($\alpha,\beta$)下的概率密度函数:		</p>

<p><img src="../images/140530/map2.png" alt="image" /></p>

<p>从图中可以看出,当参数取不同值时,beta函数的概率密度函数差异很大,基本上beta函数可以通过调节参数来拟合很多的概率分布.beta算是”万能”的概率分布函数.		
在前面的例子中,相信正常硬币正面发生的概率在0.5处取得最大值,所以设定$\alpha=\beta=5(其实,只要\beta=\alpha)$就能保证在0.5处的概率最大,他们的取值只是限定了他们的收敛速度(值越大概率值越密集).		</p>

<p><img src="../images/140530/map3.png" alt="image" /></p>

<p>我们可以看出与极大似然估计相比,分子项多了$\alpha-1,分母项多了\alpha+\beta-2$,这就是我们的先验信念(priori belief)在起作用,当我们对先验分布有一个很确信的认知时,如我们假设银币实验中,正面出现的概率在0.5处取最大值,并且左右波动的可能性很小,这样为了表示我们的强先验信念,我们可以将$\alpha=\beta$设定为一个大的值.这表现在公式中就可以表现为,当实验次数不多时,参数估计结果会更多的偏向于先验分布,只有当实验次数足够多时,先验估计的影响才会减弱.</p>

<p>仍采用上面的样例,20次实验中,正面出现的次数为12次,则<script type="math/tex">\hat{\theta}_{MAP} = \frac{12+4}{20+8}=0.571</script>,这表明”硬币是均匀的”这一先验对参数估计有影响.</p>

<p>上面的实验是$\alpha=\beta=5$的情况,我们假设先验分布在最大值左右波动概率很大,如设定$\alpha=\beta=2$,则<script type="math/tex">\hat{\theta}_{MAP} = \frac{12+1}{20+2}=0.591</script>,先验对参数估计的影响减弱.		</p>

<h2 id="section">3.贝叶斯估计</h2>
<hr />

<p>贝叶斯估计对MAP做了如下扩展,参数$\theta$由一个具体的值变成参数上个的一个概率分布.这里就不再是单纯的考虑后验概率最大时的参数值,而是将参数的期望和方差信息一同考虑在内.首先通过贝叶斯准则计算后验分布:		</p>

<script type="math/tex; mode=display">p(\theta｜X) = \frac{p(\theta)p(X｜\theta)}{p(X)}</script>

<p>由于我们并不是要找后验分布的最大值,所以我们需要计算P(X),由全概率公式展开的:		</p>

<script type="math/tex; mode=display">p(X) = \int_{\theta \in \Theta}p(X｜\theta)p(\theta)d\theta</script>

<p>当观测到新数据时,参数的后验概率会自动调整,并且通过统计分析可以最终得出后验概率分布.但是,P(X)的积分的求解十分复杂.			</p>

<p>根据已观察到的数据来预测观测一个新数据的概率为:			</p>

<p><img src="../images/140530/map4.png" alt="image" />		</p>

<p>对于抛硬币的贝努利实验,假设N次试验得到的试验结果集合C,这里我们加入beta(5,5)的先验信念,在MAP中我们要求后验概率的最大值,在贝叶斯估计中我们要求	满足beta分布的参数的期望.		</p>

<p><img src="../images/140530/map5.png" alt="image" /></p>

<p>$Beta(\alpha,\beta)分布的均值,&lt;p｜\alpha,\beta&gt;=\alpha(\alpha+\beta)^{-1},方差V(p｜\alpha,\beta)=\alpha\beta(\alpha+\beta+1)^{-1}(\alpha+\beta)^{-2}$,根据之前的统计,评估结果为:		</p>

<script type="math/tex; mode=display">% <![CDATA[
<P｜C> = \frac{n^{1}+\alpha}{n^{1}+n^{0}+\alpha+\beta}=\frac{n^{1}+5}{N+10} %]]></script>

<script type="math/tex; mode=display">V(p｜\alpha,\beta)=\frac{(n^{1}+\alpha)(n^{0}+\beta)}{(N+\alpha+\beta+1)(N+\alpha+\beta)^2}=\frac{(n^{1}+5)(n^{0}+5)}{(N+11)(N+10)^2}</script>

<p>当20次试验中有12次出现正面时,均值为17/30=0.567,方差为17<em>13/(30$</em>31^2$)=0.0079.		</p>

<p>对于上述三种方法的参数估计结果如下图所示:		</p>

<p><img src="../images/140530/map6.png" alt="image" />		</p>

<h3 id="section-1">参考内容:</h3>

<p>Gregor Heinrich:Parameter estimation for text analysis		
<a href="http://blog.csdn.net/yangliuy/article/details/8296481">文本语言模型的参数估计 http://blog.csdn.net/yangliuy/article/details/8296481</a>		</p>

<p><a href="http://blog.jqian.net/post/lda.html">主题模型之lda http://blog.jqian.net/post/lda.html</a></p>

      <footer class="entry-meta">
        <span class="entry-tags"><a href="http://jwchennlp.github.com/tags/#参数评估" title="Pages tagged 参数评估" class="tag">参数评估</a><a href="http://jwchennlp.github.com/tags/#主题模型" title="Pages tagged 主题模型" class="tag">主题模型</a><a href="http://jwchennlp.github.com/tags/#极大似然估计" title="Pages tagged 极大似然估计" class="tag">极大似然估计</a><a href="http://jwchennlp.github.com/tags/#最大后验" title="Pages tagged 最大后验" class="tag">最大后验</a><a href="http://jwchennlp.github.com/tags/#贝叶斯估计" title="Pages tagged 贝叶斯估计" class="tag">贝叶斯估计</a></span>
        <span><a href="http://jwchennlp.github.com/parameter-estimation-approaches/" rel="bookmark" title="参数评估方法">参数评估方法</a> was published on <span class="entry-date date published updated"><time datetime="2014-06-02T00:00:00-04:00">June 02, 2014</time></span></span>
        (revised: <span class="entry-date date modified"><time datetime="2014-06-02 13:27:24 UTC">06/02/2014</time></span>)
        <span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>
        <div class="social-share">
          <ul class="socialcount socialcount-small inline-list">
            <li class="facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=http://jwchennlp.github.com/parameter-estimation-approaches/" title="Share on Facebook"><span class="count"><i class="icon-facebook-sign"></i> Like</span></a></li>
            <li class="twitter"><a href="https://twitter.com/intent/tweet?text=http://jwchennlp.github.com/parameter-estimation-approaches/" title="Share on Twitter"><span class="count"><i class="icon-twitter-sign"></i> Tweet</span></a></li>
            <li class="googleplus"><a href="https://plus.google.com/share?url=http://jwchennlp.github.com/parameter-estimation-approaches/" title="Share on Google Plus"><span class="count"><i class="icon-google-plus-sign"></i> +1</span></a></li>
          </ul>
        </div><!-- /.social-share -->
      </footer>
    </div><!-- /.entry-content -->
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
    
    <div class="read-more">
      
        <div class="read-more-header">
          <a href="http://jwchennlp.github.com/lsa-topic-model/" class="read-more-btn">Read More</a>
        </div><!-- /.read-more-header -->
        <div class="read-more-content">
          <h3><a href="http://jwchennlp.github.com/plsa/" title="pLSA">pLSA</a></h3>
          <p>在对LSA的介绍中,我们知道LSA的核心思想是将建立的文档-词项矩阵运用SVD将高维空间映射到到隐语义空间,这样可以较好的解决同义词的问题.但语义的权重不好解释.##1.层面模型(aspect model)-----------pLSA是以层面模型进行建模,层面模型是一个统...&hellip; <a href="http://jwchennlp.github.com/plsa/">Continue reading</a></p>
        </div><!-- /.read-more-content -->
      
      <div class="read-more-list">
        
          <div class="list-item">
            <h4><a href="http://jwchennlp.github.com/lsa-topic-model/" title="Latent Semantic Analysis(LSA的目标是在找到一个数据映射之后能很好的词汇层面信息的同时能够表示不同实体间的语义关系)">Latent Semantic Analysis(LSA的目标是在找到一个数据映射之后能很好的词汇层面信息的同时能够表示不同实体间的语义关系)</a></h4>
            <span>Published on May 30, 2014</span>
          </div><!-- /.list-item -->
        
          <div class="list-item">
            <h4><a href="http://jwchennlp.github.com/em-algorithm/" title="EM算法">EM算法</a></h4>
            <span>Published on May 20, 2014</span>
          </div><!-- /.list-item -->
        
      </div><!-- /.read-more-list -->
      
    </div><!-- /.read-more -->
  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2014 jwchen. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://jwchennlp.github.com/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://jwchennlp.github.com/assets/js/scripts.min.js"></script>

<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'jwchennlp'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>	        

</body>
</html>
