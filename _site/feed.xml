<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">My blog</title>
<generator uri="https://github.com/mojombo/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="http://jwchennlp.github.com/feed.xml" />
<link rel="alternate" type="text/html" href="http://jwchennlp.github.com" />
<updated>2014-05-14T09:51:49-04:00</updated>
<id>http://jwchennlp.github.com/</id>
<author>
  <name>jwchen</name>
  <uri>http://jwchennlp.github.com/</uri>
  <email>hit1093710417@email.com</email>
</author>


<entry>
  <title type="html"><![CDATA[文本分类]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/test-classification/" />
  <id>http://jwchennlp.github.com/test-classification</id>
  <updated>2014-05-14 07:01:12 -0400T00:00:00-00:00</updated>
  <published>2014-05-14T00:00:00-04:00</published>
  
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;p&gt;文本分类问题在机器学习和信息检索领域都有比较广泛的运用．这里将主要介绍自己实现的一些分类模型和实践过程中应该注意的地方．&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;朴素贝叶斯&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;朴素贝叶斯模型在很多问题的处理过程中都相当有效，因为其过程和想法都很简单，只需要对数据集进行处理而不存在训练的过程，在很多场合下都作为解决问题的一个基线.&lt;/p&gt;

&lt;p&gt;对于某一文本x,x表示文本的所有词，我们需要求文档所属的类别p(y｜x),我们可以进行如下转换:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; p(y｜x) = \frac{p(x,y)}{p(x)} ＝arg\max_yp(x｜y)P(y)　\\
          =arg\max_y\prod^n_\left(i=1\right)p(x_i｜y)P(y)
&lt;/script&gt;

&lt;p&gt;公式从第一步到第二步我们做了一个假设，也就是说在给定ｙ的情况下，$x_i$的出现与否对$x_j$的出现与否没有影响，即：      &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;p(x_j|x_i,y)=P(x_j|y)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;在实现朴素贝叶斯方法有两种操作，一种是贝努利模型，一种是事件模型，下面分开讨论.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt;在计算$arg\max_y\prod^n_\left(i=1\right)p(x_i｜y)$因为分母很大，经常会出现数据过小超出所能表示的范围而使得结果为０，这里有两种处理方法:        &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;上式中分母一般为某一类别下文档数和文档类别的累加（平滑）或某一类别下文档的词数和词典大小的累加，我们可以初始设定每次都用一个常熟(100或1000)来代某一类别i的分母，而后其余类别，其分母则可以表示为100*(文档ｊ对应的词数)／（文档ｉ类别的词数）.     &lt;br /&gt;
*　也可以对上述公式&lt;script type=&quot;math/tex&quot;&gt;arg\max_y\prod^n_\left(i=1\right)p(x_i｜y)P(y)&lt;/script&gt;     &lt;br /&gt;
转换成对数公式$arg\max_y\sum^n_\left(i=1\right)\log{p(x_i｜y)}+\log{P(y)}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-1&quot;&gt;贝努利模型&lt;/h3&gt;

&lt;p&gt;我们用ｘ来表示文档，那么对文档中的词，只要出现，无论出现多少次，我们都标记词的出现次数为１．可以认为贝努利模型对文档进行了单词的去重操作.如文档为＂i like you,you like me＂，那么ｘ为{i,like,you,me}     &lt;/p&gt;

&lt;p&gt;在进行文档处理的时候，我们应该进行词条化，去除标点符号，这里我用nltk工具包进行了词的小写处理.（实验数据是路透社的新闻语料）．下图为贝努利模型下朴素贝叶斯的分类结果. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/140514/1.png&quot; alt=&quot;image&quot; /&gt;        &lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;事件模型&lt;/h3&gt;

&lt;p&gt;正常情况下，文档中有些词是出现多次的，对于这些词，我们不单考虑词是否出现，并且在计算的过程中考虑词出现的次数，这便是朴素贝叶斯的事件模型，这个时候的词可以表示为{$word_i:count_i,…,word_n:count_n$},上面的文档用事件模型应该表示为{i:1,like:2,you:2,me:1}.下图为事件模型下朴素贝叶斯的分类结果.        &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/140514/2.png&quot; alt=&quot;image&quot; /&gt;        &lt;/p&gt;

&lt;h4 id=&quot;section-3&quot;&gt;拉普拉斯平滑&lt;/h4&gt;

&lt;p&gt;我们在计算$p(x_i｜y)$的时候，如果在某一类被$y_i$中，词$x_i$没有在任何一篇文档中出现，那么显然属于此类别的概率为０．并且我们知道已经标记好的训练集不可能涵盖所有的词，所以这种情况出现的概率很高．这里我们一般运用拉普拉斯平滑来处理. 
所以$p(x_i｜y)=\frac{count(x_i)+1}{count(allwords)+v}$.      &lt;/p&gt;

&lt;p&gt;当我们使用贝努利模型的时候，$count(x_i)$表示在训练集类别ｙ的所有文档中出现词$x_i$的文档个数，$count(allwords)$表示类别ｙ的所有文档中词典的大小（文档所有词去重）.V表示类别ｙ的文档的大小．      &lt;/p&gt;

&lt;p&gt;当使用事件模型的时候，$count(x_i)$表示在训练集类别ｙ的所有文档中出现词$x_i$的次数，$count(allwords)$表示类别ｙ的所有文档中总共的词数（不去重）.V表示类别ｙ的文档的词典的大小．      &lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;总结&lt;/h3&gt;

&lt;p&gt;我们发现，采用朴素贝叶斯模型的分类效果还不错，在事件模型下F_1值能达到83%．但是贝努利模型和事件模型的效果差别比较大，并且在不同类别上的分类效果上差别比较大．我们知道事件模型考虑到了文档中词出现的次数，这是造成差异的原因，文档中的词不能单纯的只考虑词是否出现，文档中词出现的次数对文档主旨和类别的贡献还是有很大差异的．尤其是在长文档中，词频繁出现的概率会很大，所以采用贝努利模型的话会造成很大的误差．      &lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/test-classification/&quot;&gt;文本分类&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on May 14, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[阿里大数据竞赛方法总结]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/odps-model/" />
  <id>http://jwchennlp.github.com/odps-model</id>
  <updated>2014-05-09 02:17:38 -0400T00:00:00-00:00</updated>
  <published>2014-05-09T00:00:00-04:00</published>
  
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;h2 id=&quot;section&quot;&gt;逻辑回归&lt;/h2&gt;

&lt;h3 id=&quot;section-1&quot;&gt;特征抽取&lt;/h3&gt;
&lt;p&gt;将前四个月的数据切分成4:1,为了训练模型，前一部分数据用于抽取特征ｘ，后一部分用于获取类别ｙ．具体描述为，根据每个用户对每个物品的行为没一个数据，如果行为在第一部分，但是在第二部分没有购买行为，这类别为负例，若在第二部分出现购买行为，这此行为为正例．如果行为只在第二部分产生，则次用户对物品的行为无效，直接剔除．  &lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;模型训练&lt;/h3&gt;
&lt;p&gt;在得出了模型的正例和负例之后，因为正例负例差很多，不能直接进行训练．这里采取的策略是按比例对负例进行采样，采样后结合正例进行训练．模型训练直接用xlab平台的逻辑回归进行训练．    &lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;预测结果&lt;/h3&gt;
&lt;p&gt;在训练完模型之后，我们按照相同的方法对所有前４个月的数据进行特征抽取．并用训练好的模型进行预测．预测后，会返回每个数据属于哪一类别的概率，我们可以通过限定概率值得到最终结果．&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;存在改进的地方&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;没有对数据进行去噪声操作．     &lt;/li&gt;
  &lt;li&gt;对负例的采样比例的设定．      &lt;/li&gt;
  &lt;li&gt;模型训练时的迭代次数，及是否可以通过多次采样对多次结果进行加权获取最终结果．&lt;/li&gt;
&lt;/ul&gt;

  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/odps-model/&quot;&gt;阿里大数据竞赛方法总结&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on May 09, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[支持向量机]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/support-vector-machine/" />
  <id>http://jwchennlp.github.com/support-vector-machine</id>
  <updated>2014-05-08 02:22:02 -0400T00:00:00-00:00</updated>
  <published>2014-05-08T00:00:00-04:00</published>
  
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;h2 id=&quot;section&quot;&gt;问题引出&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;在面对一个最简单的二分类问题，并且假设数据集可分的情况下．具体如下图所示．当我们采用逻辑回归实现分类时，我们用一个分类超平面（决策边界）对数据进行数据进行划分，并在划分后，不同类别的数据分布在分类超平面的两边，这表示分类成功．其实，在数据可分的情况下，我们发现可以有很多条这样的分类超平面，并且都能达到正确分类的效果,这个时候我们可能要问，这些分类超平面的效果一样吗？是否存在一个最优的分类超平面．
&lt;img src=&quot;../images/140508/1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;函数间隔和几何间隔&lt;/h2&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;section-2&quot;&gt;函数间隔&lt;/h3&gt;

&lt;p&gt;对于上面数据集，我们计算出了一个超平面$w^Tx+b=０$.对于某一个数据点x我们需要判断其内别,如果$w^Tx+b＞0$，则其类ｙ=1，并且如果$w^Tx+b＞0$并且$w^Tx+b$值越大，则这个点的类别为正例的置信度就越高．当$w^Tx+b＜0$时,点所属类别为－１，并且$w^Tx+b$值越小，这这个点类别为负例的置信度就越高．并且当点ｘ被正确分类时，$y(w^T+b)$为正数．从上面图中可以看出，我们设定分类超平面的上部为正例，A,B,C三个的点都被标记为正例，但是C离决策边界最近，可能稍微变化决策边界就可能导致分类错误，所以C分类正确的置信读低．A离决策边界最远，所以Ｃ被分为正例的置信读高.对于点$\left(x^\left(i\right),y^\left(i\right)\right)$为了获得更好的分类效果，我们希望$y\left(i\right)(w^Tx+b)$尽量大，则分类的置信度就越高．&lt;/p&gt;

&lt;p&gt;所以，对数据$\left(x^\left(i\right),y^\left(i\right)\right)$，我们就定义$y\left(i\right)(w^Tx+b)$为此数据点的函数间隔，并且如果使得每个点的函数间隔都倾向于一个大值，则分类置信度越高，分类效果越好．   &lt;/p&gt;

&lt;p&gt;给定训练集合S={($x^\left(i\right),y^\left(i\right)）$,i=1,…m},我们需要计算每一个样本点到分类超平面的函数间隔.在这里，我们需要求出最小的函数间隔，并且通过修改分类超平面使得最小函数间隔尽可能大，则分类效果更好．     &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\widehat{\gamma}=\min_{i=1,...,m}\widehat\gamma^\left(i\right)&lt;/script&gt;

&lt;p&gt;利用函数间隔来衡量分类效果的置信度有一个缺陷，当我们确定某一个分类超平面$w^Tx+b=0$，我们对ｗ,b同时增加ｋ倍,函数间隔由原来的$y\left(w^Tx+b\right)$变成$ky\left(w^Tx+b\right)$．也就是说某一数据点的函数间隔可以可以任意的缩放或增加．&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;几何间隔&lt;/h3&gt;

&lt;p&gt;在确定分类超平面之后，任一数据点到分类超平面的距离应该是不变的．如果我们用这个距离来衡量分类置信度的话，效果会很好．    &lt;br /&gt;
&lt;img src=&quot;../images/140508/2.png&quot; alt=&quot;image&quot; /&gt;  &lt;br /&gt;
点A到平面的距离设为$\gamma$,知道分类超平面的法向量为ｗ，那么将A投影到分类超平面上的点B的坐标为$x-\frac{w}{\left|w\right|}\gamma$,且点在决策边界$w^Tx+b=0$上，所以有:        &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w^T\left(x-\frac{w}{｜w｜}\gamma\right)+b=0&lt;/script&gt;

&lt;p&gt;求解得$\gamma=\frac{w}{｜w｜}x+\frac{b}{｜w｜}$,所以对所有的样本点点$\left(x^\left(i\right),y^\left(i\right)\right)$,我们求得每个样本点的几何间隔为:      &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma^\left(i\right)=y^\left(i\right)\left(\frac{w}{｜w｜}x^\left(i\right)+\frac{b}{｜w｜}\right)&lt;/script&gt;

&lt;p&gt;给定训练集合S={($x^\left(i\right),y^\left(i\right)）$,i=1,…m},我们需要计算每一个样本点到分类超平面的几何距离.在这里，我们需要求出最小的几何距离，并且通过修改分类超平面使得最小几何距离尽可能大，则分类效果更好．        &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma=\min_{i=1,...,m}\gamma^\left(i\right)&lt;/script&gt;

&lt;h2 id=&quot;section-4&quot;&gt;最优间隔分类器&lt;/h2&gt;

&lt;p&gt;当给定训练集之后，按照前面分析直观上最好的分类效果是找到决策边界使得(几何)间隔最大化.因为我们的决策是使得最小几何间隔最大化，则显然对所有点的分类的置信度很高．所以当对于一个线性可分的数据集，我们要通过一个分类超平面来分割所有的正例和负例．那么我们的问题可以转化成下面的优化问题:   &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max_{\gamma,w,b}   \gamma \\
 s.t.    y^\left(i\right)\left(\frac{w}{|w|}x^\left(i\right)+\frac{b}{|w|}\right)\geq\gamma,i=1,...,m
&lt;/script&gt;

&lt;p&gt;由于我们知道在确定了(w,b)之后，我们可以通过同比例的缩放或增加(w,b),所以我们可以通过相应的扩张比例使得｜w｜的值为１．所以优化问题转化成如下形式：     &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max_{\gamma,w,b}   \gamma \\
 s.t.    y^\left(i\right)\left(wx^\left(i\right)+b\right)\geq\gamma,i=1,...,m　\\
 |w|=1
&lt;/script&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;通过约束&lt;/td&gt;
      &lt;td&gt;w&lt;/td&gt;
      &lt;td&gt;=1,使得函数间隔等于几何间隔．但是因为如上的优化问题是非凸问题，我们很难通过软件来进行求解．所以我们将第一个问题转化成如下问题:&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max_{\gamma,w,b}:  \frac{\widehat\gamma}{|w|} \\
 s.t.    y^\left(i\right)\left(wx^\left(i\right)+b\right)\geq\widehat\gamma,i=1,...,m
&lt;/script&gt;

&lt;p&gt;其中$\widehat\gamma$代表的是最小函数间隔，我们知道函数间隔是可以通过(w,b)的同比例变化而变化，这里为了为了简化计算，我们将$\widehat\gamma$设为１．那么如上的优化问题变为:     &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max_{\gamma,w,b}:  \frac{1}{|w|} \\
 s.t.    y^\left(i\right)\left(wx^\left(i\right)+b\right)\geq 1,i=1,...,m
&lt;/script&gt;

&lt;p&gt;进一步转变，优化问题变成如下格式：       &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\gamma,w,b}: \frac{1}{2}w^2 \\
 s.t.    y^\left(i\right)\left(wx^\left(i\right)+b\right)\geq 1,i=1,...,m
&lt;/script&gt;

&lt;h2 id=&quot;section-5&quot;&gt;拉格朗日算子&lt;/h2&gt;

&lt;p&gt;对于如下的原始优化问题：    &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \min_w:f(w) \\
s.t.:g_i(w)\leq0,i=1,...k  \\
h_i(w)=0,i=1,...l   
&lt;/script&gt;

&lt;p&gt;其拉格朗日算子为&lt;script type=&quot;math/tex&quot;&gt;L(w,\alpha,\beta)=f(w)+\sum_\left(i=1\right)^k\alpha_ig_i(w)+\sum_\left(i=1\right)^l\beta_ih_i(w)&lt;/script&gt;,其中$\alpha_i,\beta_i$为拉格朗日乘数．      &lt;br /&gt;
考虑如下等式：     &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_p(w)=\max_\left(\alpha,\beta:\alpha_i\geq0\right)L(w,\alpha,\beta)&lt;/script&gt;

&lt;p&gt;其中ｐ表示原始的,我们发现当给定ｗ，并且ｗ满足我们原始问题的约束（$g_i(w)\leq0，h_i(w)=0$），如果ｗ违背这些约束，则显然$\theta_p(w)=\infty$,当ｗ满足原始问题约束时，$\theta_p(w)=０$．那么可以的出如下结论：  &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_w\theta_p(w)=\min_w\max_\left(\alpha,\beta:\alpha_i\geq0\right)L(w,\alpha,\beta)&lt;/script&gt;

&lt;p&gt;同时我们定义$p^*=\min_w\theta_p(w)$为原始问题的解．   &lt;br /&gt;
现在对应它的对偶问题：     &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max_\left(\alpha,\beta:\alpha_i\geq0\right)\theta_d(p)=\max_\left(\alpha,\beta:\alpha_i\geq0\right)\min_wL(w,\alpha,\beta)&lt;/script&gt;

&lt;p&gt;我们知道，最大最小问题的解小于最小最大问题的解：     &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;d^*=\max_\left(\alpha,\beta:\alpha_i\geq0\right)\min_wL(w,\alpha,\beta)\leq\min_w\max_\left(\alpha,\beta:\alpha_i\geq0\right)L(w,\alpha,\beta)=p^*&lt;/script&gt;

&lt;p&gt;当ｆ和$g_i$为凸函数时，$h_i$为仿射函数(仿射变换的定义是在几何空间中，一个向量空间进行一次线性变换并接上一个平移，变换成另一个向量空间)，有$d^＊=p^＊$，在这些约束下，一定存在一个$w^＊$是原始问题的解，$\alpha^＊,\beta^＊$是对偶问题的解，并且有$d^＊=p^＊=L(w^＊,\alpha^＊,\beta^＊)$,同时这３个参数满足KKT条件，KKT条件描述如下：    &lt;br /&gt;
&lt;img src=&quot;../images/140508/3.png&quot; alt=&quot;image&quot; /&gt;		&lt;/p&gt;

&lt;p&gt;其中第三个约束称为对偶互补条件，并且当$a_i＞０$时，$g_i(w^*)=0$,满足这些条件的点所对应的几何间隔便是最小几何间隔．这些点称为支持向量.&lt;/p&gt;

&lt;p&gt;现在回到优化边界分类器部分,我们的原始问题定义为：       &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\gamma,w,b}: \frac{1}{2}w^2 \\
 s.t.    y^\left(i\right)\left(wx^\left(i\right)+b\right)\geq 1,i=1,...,m
&lt;/script&gt;

&lt;p&gt;约束条件可以表示为：        &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g_i(w)=-y^\left(i\right)(w^Tx\left(i\right)+b)+1\leq0&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;../images/140508/4.png&quot; alt=&quot;image&quot; /&gt;    &lt;/p&gt;

&lt;p&gt;我们可以看到，有最小几何间隔的点离决策边界最近．我们知道这些点$\left(x^\left(i\right),y\left(i\right)\right)$满足$g_i(w)=0$.我们将这些点称之为支持向量．从上图知道，数据集中有３个支持向量，一般来说支持向量的个数会明显小于数据集的大小，这在后面会相当有用．      &lt;/p&gt;

&lt;p&gt;原始问题的拉格朗日算子可以表示为：      &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(w,b,\alpha)=\frac{1}{2}w^2-\sum_{i=1}^m\alpha_i[y^\left(i\right)(w^Tx\left(i\right)+b)+1]&lt;/script&gt;

&lt;p&gt;这个时候我们通过求对偶问题$\theta_p(w)$来求原始问题的解．具体方法是对Ｌ函数关于参数ｗ和ｂ求偏导数：    &lt;br /&gt;
&lt;img src=&quot;../images/140508/5.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;../images/140508/6.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;../images/140508/7.png&quot; alt=&quot;image&quot; /&gt;      &lt;br /&gt;
将得到的约束代回到上面的拉格朗日算子中．得到：          &lt;br /&gt;
&lt;img src=&quot;../images/140508/8.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;../images/140508/9.png&quot; alt=&quot;image&quot; /&gt;      &lt;br /&gt;
最后原始问题的对偶优化问题可以定义为：            &lt;br /&gt;
&lt;img src=&quot;../images/140508/10.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;现在假定我们已经求得对偶问题最优解的$\alpha_i$，那么我们是如何做预测的呢？对于一个新的数据点ｘ，我们知道预测是通过判断$w^Tx+b$的值来判定的，如果大于０，类别为正例，如果小于０，类别为负例,我们将上面求解的ｗ值代入：         &lt;br /&gt;
&lt;img src=&quot;../images/140508/11.png&quot; alt=&quot;image&quot; /&gt;		&lt;/p&gt;

&lt;p&gt;我们知道$\alpha_i\geq0$,且根据KKT约束条件中的对偶互补条件$\alpha_ig_i(w)\geq0$,并且$\alpha&amp;gt;0$时，$g_i(w)=0$,表示这些点有最小的几何间隔，也就是说这些点表示支持向量．我们知道，在判断ｘ类别的时候，我们只需要考虑$\alpha_i&amp;gt;0$的情况，也对应的我们只需要考虑数据集中的支持向量．同时，支持向量想对于数据集来说是小很多的．这样很显然可以进行高效求解.&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/support-vector-machine/&quot;&gt;支持向量机&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on May 08, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[生成模型和判别模型]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/generative-model-and-discriminative-model/" />
  <id>http://jwchennlp.github.com/generative-model-and-discriminative-model</id>
  <updated>2014-05-07 07:40:02 -0400T00:00:00-00:00</updated>
  <published>2014-05-07T00:00:00-04:00</published>
  
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;h2 id=&quot;section&quot;&gt;定义&lt;/h2&gt;
&lt;hr /&gt;
&lt;blockquote&gt;
  &lt;p&gt;生成方法由数据学习联合概率分布P(x,y)，然后求出条件概率分布P(y｜ｘ)作为预测的模
型，即成生模型:  &lt;br /&gt;
&lt;img src=&quot;../images/140507/1.png&quot; alt=&quot;image&quot; /&gt; &lt;br /&gt;
这样的方法成为生成方法，是因为模型表示了给定输入ｘ产生输出ｙ的生成关系．典型的生成模型有，朴素贝叶斯和隐马尔可夫模型．  &lt;br /&gt;
判别方法是由数据直接学习决策函数ｆ(x)或者条件概率分布P(y｜x)作为预测的模型，即判别模型，判别方法关心的是对给定的输入x,应该预测什么样的输出ｙ，典型的方法包括感知机，决策树，逻辑回归．   &lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;section-1&quot;&gt;理解&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;在面对猫狗分类问题时，我们该如何实现呢？  &lt;br /&gt;
方法一：当我们利用逻辑回归或是感知机模型时，我们需要数据集所投射的空间中，找到一个决策边界，在决策边界一边的属于一类动物，在决策边界另一边的属于另一种动物．当来一个我们不知道的动物时，我们将它放入空间中，通过判断它在决策边界的那一侧来判断是猫还是狗． &lt;br /&gt;
方法二：将数据集中的猫都拿出来，建立一个关于猫的特征的模型．按同样的方法建立一个关于狗的模型．这样，当判断一个动物时，我们分别查看它在猫模型中属于猫的概率和在狗模型中属于狗的概率，哪个值大，便说明属于哪个模型．   &lt;/p&gt;

&lt;p&gt;方法一通过对数据集训练出一个模型，并通过判断P(y|x)下的条件概率来判断ｙ的类别．这种方法成为判别方法，对应建立的模型属于判别模型．   &lt;br /&gt;
方法二对数据集的每一个类别建立一个模型，并通过联合概率P(x,y)来判断ｘ特征所应对应的类别．这种方法成为生成方法．  &lt;/p&gt;

&lt;p&gt;其实通过联合概率来判断类别进行了一个变形，一般我们是要判断P(y|x)下的概率，可以进行如下转换： &lt;br /&gt;
&lt;img src=&quot;../images/140507/1.png&quot; alt=&quot;image&quot; /&gt;  &lt;br /&gt;
对于某个参数ｘ，其概率值P(x)值在所有类别下都是相同的，所以问题便等同于如下问题：            &lt;br /&gt;
&lt;img src=&quot;../images/140507/2.png&quot; alt=&quot;image&quot; /&gt;        &lt;/p&gt;

&lt;p&gt;不妨通过一个朴素贝叶斯生成模型来了解生成模型的判定过程．
如图，训练集包含４篇文档，我们需要验证测试集中的文档类别： &lt;br /&gt;
&lt;img src=&quot;../images/140507/3.png&quot; alt=&quot;iamge&quot; /&gt;    &lt;/p&gt;

&lt;p&gt;我们需要计算每一个类别下P(x｜y)P(y)的概率，并且概率最大的那一类便是文档所属类别．即计算P((Chinese,chinese,Chinese,Tokyo,Japan)｜y=c)P(y=c)和P((Chinese,chinese,Chinese,Tokyo,Japan)｜y=$\bar{c}$)P(y=$\bar{c}$)．&lt;/p&gt;

&lt;p&gt;之后利用朴素贝叶斯的的条件独立定义进行求解便能获知测试及属于哪个类别．&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/generative-model-and-discriminative-model/&quot;&gt;生成模型和判别模型&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on May 07, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[索引压缩]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/index-compress/" />
  <id>http://jwchennlp.github.com/index-compress</id>
  <updated>2014-05-06 05:01:16 -0400T00:00:00-00:00</updated>
  <published>2014-05-06T00:00:00-04:00</published>
  
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;h3 id=&quot;section&quot;&gt;为什么要进行索引压缩？&lt;/h3&gt;
&lt;hr /&gt;
&lt;p&gt;进行索引压缩有以下优点：  &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;节省磁盘空间．  &lt;/li&gt;
  &lt;li&gt;增加高速缓存(cache)的利用率.&lt;br /&gt;
倒排索引词典是放在内存中的，倒排记录表放在磁盘上．对与到拍记录上的某些词项ｔ，我们是需要经常访问的，如果将这次词项ｔ所对应的到拍记录表压缩后放在高速缓存中，只要采用得当的解压缩算法，那么当查询词项ｔ的倒排记录表时，只需要访问cache，而不用从磁盘读取数据，能充分减少IR系统的响应时间． &lt;/li&gt;
  &lt;li&gt;压缩能够加快从磁盘读取数据的速度．&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;压缩技术分为有损压缩和无损压缩，有损压缩指的是压缩后，原始数据的所有信息都保存下来了．词干还原，大小写转换都属于有损压缩．   &lt;/p&gt;

&lt;h3 id=&quot;heaps&quot;&gt;Heaps定律：词项数目的估计&lt;/h3&gt;

&lt;p&gt;heaps定律认为，文档集大小和词汇量之间存在对数上的线性关系.它将词项的数目估计为文档集大小的函数:&lt;script type=&quot;math/tex&quot;&gt;M=kT^b&lt;/script&gt;,其中Ｔ代表文档集合中的词条的个数． &lt;/p&gt;

&lt;p&gt;不同文档集下ｋ取值差异较大，因为词汇量大小取决于文档本身以及对他进行处理的方式．当进行词干还原，大小写转换时将降低词汇量增长的速度，允许加入数字和容忍拼写错误则会增加增长率．无论参数取值如何，heaps定律满足一下两条性质：    &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;词汇量会随着文档集的增加而增加，不会趋于一个定值．     &lt;/li&gt;
  &lt;li&gt;大规模文档集的词汇量也会很大．       &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;zipf&quot;&gt;Zipf定律：词项在文档中的分布&lt;/h3&gt;
&lt;p&gt;Zipf定律用于估计词项在文档中分布，假设$t_1$用于表示文档集中出现最多的词，$t_2$用于表示文档集中出现第二多的词，文档集合中出现第i多的词的文档频率$cf_i$与$\frac{1}{i}$成正比:   &lt;br /&gt;
    &lt;script type=&quot;math/tex&quot;&gt;cf_i=k\frac{1}{i}&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;词典压缩&lt;/h2&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;section-2&quot;&gt;为什么要进行词典压缩&lt;/h3&gt;
&lt;p&gt;理想情况下在建立好索引后，我们希望将词典存放在内存中，但是这往往很难实现，尤其是对于实用的搜索引擎和嵌入式系统．限制IR系统的响应之间的一个因素包多对磁盘的访问次数．所以，如果通过压缩来讲所有的或大部分的词典存入内存，将大大加快IR系统的响应速度．    &lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;将词典看作单一字符串的压缩方法&lt;/h3&gt;
&lt;p&gt;采用如下的数据结构进行存储：一个定长的数组用于存储词项（２０Ｂ），４Ｂ的空间用于存储文档频率，４Ｂ的空间用于存储指向倒排记录表的指针．对于一个包含Ｍ个词项的文档空间来说，词典的总空间为M*(20+4+4),当Ｍ＝400,000时，占用空间为11.2MB.
&lt;img src=&quot;../images/1/dic_compress_1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这种方法存在很大的不足，首先大部分的英文词平均长度为8B,这显然造成了大部分的空间浪费，其次也存在有些词的长度超过20B,导致的结果便是不能存储这些词．&lt;/p&gt;

&lt;p&gt;我们可以采用如下的改进措施，我们建立一个字符串在存储字典中的所有词项,4B的空间存储文档频率,4B的空间存储倒排记录表的指针，这个指针指向前面所有词典构成的长字符串，在长字符串中我们需要每一个词加入一个定位指针，用于指定下一个词的开始位置和当前词的结束位置，由于有400,000个词，每个词为８B,所以寻址空间为400,000&lt;em&gt;8=3.2&lt;/em&gt;$10^6$,所以可以用一个长为$\log{3.2&lt;em&gt;10^6}$$\approx$22b，即３Ｂ的指针来表示．词典的总空间为M&lt;/em&gt;(4+4+3+8)=7.6MB．  &lt;br /&gt;
&lt;img src=&quot;../images/1/1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;按块存储&lt;/h3&gt;
&lt;p&gt;对上面的压缩方法进行一个变形，这里不再对每个词项都维护一个指向字符串(所有词的组合)的指针．我们首先将我们的词典按块进行划分，例如每５个词为一块，这样对没一个块只需要维护一个这个块指向字符串的指针，同时在长字符串中，我们需要加入一个空间用于指定当前词的长度．在这种机制下，假设一个块内有ｋ个词，我们减少了(k-1)个指针的空间，但是我们需要在字符串中对没个词增加空间以记录其词的长度．假设每个块内有４个词，减少的指针空间为9B,同时对４个词需要增加４Ｂ的空间用于记录词的长度，所以没４个词产生了5B的压缩，所以压缩的空间为400,000*$\frac{1}{4}$*5=0.5MB.    &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt;我们在这里维护了两个指针，一个用于指向倒排记录表，一个指向字符串用词项的位置，我们压缩的部分是词项指针．   &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/1/2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们发现，每个块内的词越多时，则可以压缩的空间越大．但是并非块内词越多越好，在进行词项查找时，对于块间的词我们可以通过二分查找快速定位，但是在快内查找时则是简单的线性遍历，所以我们必须在查找速度和空间压缩见进行权衡．    &lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;倒排记录表的压缩&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;倒排记录表的压缩基于下面一个前提，当用文档ID来表示倒排记录表，对与高频词来说，倒排记录表中的记录多并且相邻的记录之间差距会很小，当某高频词出现在某篇文档中时，将其相近的文档中出现高频词的概率会很大．这就给我们提供了对倒排记录表进行压缩的灵感，正常情况下我们对到拍记录表中的每个文档id,都是用定长的空间来存储的，那么对那些高频词的话，我可以通过存储他们倒排记录表相邻的距离（明显小于存储文档id的长度）来达到压缩的目的．     &lt;/p&gt;

&lt;h3 id=&quot;section-6&quot;&gt;可变字节码&lt;/h3&gt;
&lt;p&gt;VB(Variable byte,可变字节)码的思想为，我们采用整数个字节来存储文档id,每个字节的后７位为有效编码，第一位为延续位，表示本次编码的结束与否,’1’表示结束．     &lt;/p&gt;

&lt;p&gt;可变字节码的解码过程如下，根据延续位（一直获取字节直到字节的首位为１）来获取编码结果，对编码结果进行以下处理，去除所有的延续位，剩余有效编码表示间隔位，将此编码值与前一个编码的结果进行累加即表示文档的ID. 
&lt;img src=&quot;../images/1/3.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-7&quot;&gt;γ编码&lt;/h3&gt;

&lt;p&gt;一元编码：将数值为ｎ的数用ｎ个１并在之后加上一个０来表示的编码方式．  &lt;/p&gt;

&lt;p&gt;γ编码主要由两部分组成，偏移量(offset)和长度(length)．长度是数组的二进制编码，但是去除了首位１，长度则是偏移量的长度，但是是通过一元编码的方式实现．对于数值5,二进制编码是101,去掉首位的１，其偏移量是01,偏移量长度为２，则由一元编码表示为110,所以数值５的γ编码为11001.
&lt;img src=&quot;../images/1/4.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们发现对数值为Ｋ的数进行二进制编码，其偏移量的长度为$\lfloor\log{k}\rfloor$,其长度的长度为$\lfloor\log{k}\rfloor$+1,所以，数的γ编码长度为２$\lfloor\log{k}\rfloor$＋１．      &lt;/p&gt;

&lt;p&gt;有一点不太明白的是采用γ编码是如何实现数据压缩的呢？书的原文是这么说的：      &lt;br /&gt;
&lt;img src=&quot;../images/1/5.png&quot; alt=&quot;image&quot; /&gt;
按上面理解，采用ｎ为进行进行表示，那么间距在1~$2^\left(n-1\right)－１$之间都将产生浪费，并在在间距为$2^n$时不能表示．&lt;/p&gt;

&lt;p&gt;我的理解是这样的，我们有对倒排记录表的实现一般也是通过链表或是定长数组来实现的，当采用定长的编码格式来存储每个文档ＩＤ时，必然会产生很大的浪费．那么如何通过变长的编码格式并且不需要额外的数组或指针来表明文档的长度，这边是γ编码所做的事了，让我们看一下γ编码的的解码过程：11001,我们首先遍历该编码，知道遇到０时停止，发现长度为２，剩下的偏移量为01,我们知道实际的二进制数为101,也就是说我们通过编码本身可以确定文档id,不需要进行额外的存储．&lt;/p&gt;

&lt;h4 id=&quot;section-8&quot;&gt;参考资料&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://www.google.com.hk/search?q=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;amp;oq=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;amp;aqs=chrome..69i57j69i65j69i61l3j0.3218j0j1&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&quot;&gt;索引压缩&lt;/a&gt;     &lt;br /&gt;
&lt;strong&gt;说明：&lt;/strong&gt;文章主要内容和图片来自信息检索导论一书。&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/index-compress/&quot;&gt;索引压缩&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on May 06, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[odps_sql]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/odps-sql/" />
  <id>http://jwchennlp.github.com/odps-sql</id>
  <updated>2014-04-26 10:45:37 -0400T00:00:00-00:00</updated>
  <published>2014-04-26T00:00:00-04:00</published>
  
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;p&gt;odps(open data processing service，开源数据处理服务)是阿里巴巴的分布式计算平台。&lt;/p&gt;

&lt;p&gt;数据以sql表格的形式存放在odps中，我们可以是使用类似与sql命令的方式对数据进行操作。当让sql中嵌入了odps平台自己的函数和命令。&lt;/p&gt;

&lt;p&gt;文档学习ing…&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;表格建立&lt;/h2&gt;

&lt;p&gt;创建表格语句如下：   &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;sql&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;table&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exist&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sales&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shop_name&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;      
&lt;span class=&quot;lineno&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;     
&lt;span class=&quot;lineno&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partitioned&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sale_date&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;region&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;     
&lt;span class=&quot;lineno&quot;&gt;5&lt;/span&gt;     &lt;span class=&quot;c1&quot;&gt;--创建一张分区表sales&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;partitioned by指定了分区字段，采用分区字段主要是在跟新，新增和读取分区数据时不需要做全表扫描，可以提高效率。       &lt;/p&gt;

&lt;p&gt;可以用命令create table…as select…来新建表格，如：        &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;sql&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;table&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sales1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt;      
&lt;span class=&quot;lineno&quot;&gt;2&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sales&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;这样在建立表格的同时，将sales的数据复制到新表中，但是原表格的分区字段没有复制到新表中。如果希望新表格和原表格有相同的数据和表结构（分区属性）.可以用create table… like …命令：    &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;sql&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;table&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sales1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;like&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sales&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/odps-sql/&quot;&gt;odps_sql&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on April 26, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[索引构建]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/index-construction/" />
  <id>http://jwchennlp.github.com/index-construction</id>
  <updated>2014-04-24 02:09:41 -0400T00:00:00-00:00</updated>
  <published>2014-04-24T00:00:00-04:00</published>
  
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;p&gt;索引构建主要是对建立好的词典中的每个词项，构建词项关于文档集合的索引记录表。一般索引构建算法会受硬件设施的制约。    &lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;硬件基础&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;构建信息检索系统时，很多决策都依赖于系统所运行的硬件环境。与信息检索系统相关的硬件基本性能参数如下：      &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;系统访问内存中数据的速度比访问硬盘中数据要快的多，访问内存中的一个字节只需要几个时钟周期（大约5×10-9s），从磁盘传输一个字节的时间则长得多（大概2×10-8s）。因此，为了更快的响应速度，我们应该尽可能将数据放在内存中，特别是那种频繁访问的数据。这种将频繁使用的数据放入内存中的机制称为caching（缓存）。    &lt;/li&gt;
  &lt;li&gt;进行磁盘读写时，磁头移动到数据所在的磁道需要一定的时间，该时间称为寻道时间，对典型的磁盘来说平均在5ms左右。寻道过程中并不进行数据的传输。于是，为了时数据传输率最大，连续读取的数据块也应该在磁盘上连续存放。      &lt;/li&gt;
  &lt;li&gt;操作系统往往以数据块为单位进行读写。因此，从磁盘读取一个字节和一个数据块所耗费的时间可能一样多。我们将内存中保存读写块的那块区域称之为缓冲区（buffer）。       &lt;/li&gt;
  &lt;li&gt;数据从磁盘传输到内存是由系统总线而不是处理器来实现的，这以为着在磁盘I/O时处理器仍然可以处理数据。我们可以利用这一点来加速数据的传输过程，比如将数据压缩后存储在磁盘上。假定采用一种高效的解压缩算法的话，那么从磁盘读取压缩数据再解压缩所花时间往往比直接读取未压缩数据所花时间少。       &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;基于块的排序索引方法&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;第一章建立倒排索引时，所有的处理过程都是在内存中完成的。我们将文档一次性读入内存，而后建立文档的词典，并建立词典中的词项的倒排记录表。如果当文档集过大，大到难以一次性读入内存时，上述方法便失效。   &lt;/p&gt;

&lt;p&gt;由于内存的不足，我们可以采用磁盘的外部排序的方法（external sorting algorithm）。我们知道读取数据过程中的寻道时间与数据传输相比是十分耗时的，所以我们应该尽量将数据按块的方式存储以减少寻道的次数。BSBI（blocked sort-based indexing algorithm，基于块的排序索引算法）是一种解决的方法。算法实现如下：     &lt;br /&gt;
第1步：将文档切分成均匀的若干个部分。   &lt;br /&gt;
第2步：对每个部分的词项ID-文档ID对排序。  &lt;br /&gt;
第3步：将中间产生的临时排序结果存储在磁盘上。   &lt;br /&gt;
第4步：将所有的中间文件合并形成最终结果。   &lt;/p&gt;

&lt;p&gt;在第2步中我们将词项用词项id代替，词项id是能代表词项的唯一标识，这样做能提高索引构建效率。&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;      
           
&lt;span class=&quot;n&quot;&gt;BSBIndex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;construction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;     
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;      
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;documents&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;have&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;been&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;processed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;        
&lt;span class=&quot;n&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;                 
    &lt;span class=&quot;n&quot;&gt;block&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ParseNextBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;       
    &lt;span class=&quot;n&quot;&gt;BSBI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INVERT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;            
    &lt;span class=&quot;n&quot;&gt;WriteBlockToDisk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;      
    &lt;span class=&quot;n&quot;&gt;MergeBlocks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;....&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fmerge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;      
             
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;合并时，同时打开所有块对应的文件，内存中维护10个块的读缓冲区和一个为最终合并索引准备的写缓冲区。每次迭代中，利用优先级队列（即堆结构）或者类似的数据结构选择最小的未处理词项id进行处理。读入该词项的倒排记录表进行合并，合并结果返回磁盘中。需要时，再次从文件中读入数据到每个读缓冲区。  &lt;/p&gt;

&lt;p&gt;由于该算法主要的时间耗费在排序上，因此其时间复杂度为O（T*logT),其中T是要排序的项目数的上界（即词项ID-文档—ID对的个数），然而，实际的索引构建的时间往往取决与文档温习（ParseNextBlock）和最后合并（MergeBlocks）。&lt;/p&gt;

&lt;p&gt;由于我们知道，为了提高索引构建效率，我们将词项映射成词项ID，初始的倒排记录表形式为：     &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;      
    &lt;span class=&quot;n&quot;&gt;wordi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;doc2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.....|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;docn&lt;/span&gt;                    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;在进行词项id的映射之后，每个词项ID-文档ID对就是简单的（wid,did）的形式了。这样做为什么能提高效率呢？虽然映射过程需要话费一定的时间，可是映射之后，每个块得到的都是这样的二值对，这样可以以词项ID为主键，以文档ID为次键按照快速排序一类方法进行排序，这样使得倒排记录的构建变得简单。     &lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;内存式单遍扫描索引构建方法&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;BSBI方法需要将词项映射成词项ID，所以必须在内存中维护一个（词项，词项ID）表的数据结构。当对大规模文档来说，这种数据结构的大小将超过内存大小。
SPIMI（single-pass in-memory indexing,内存式单遍扫描索引构建算法）使用词项而不是词项ID作为词典，它将为个块的词典读入磁盘，对于下一个块则采用新的词典。只要硬盘空间足够大，SPIMI就能够索引任何大小的文档集。     &lt;/p&gt;

&lt;p&gt;SPIMI算法流程如下所示：      &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;c&quot;&gt;&lt;span class=&quot;lineno&quot;&gt; 1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SPIMI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Invert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;      
&lt;span class=&quot;lineno&quot;&gt; 2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NewFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;     
&lt;span class=&quot;lineno&quot;&gt; 3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NewHash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;      
&lt;span class=&quot;lineno&quot;&gt; 4&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;free&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;available&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;        
&lt;span class=&quot;lineno&quot;&gt; 5&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;      
&lt;span class=&quot;lineno&quot;&gt; 6&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;term&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;        
&lt;span class=&quot;lineno&quot;&gt; 7&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;posting_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AddToDictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;term&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;     
&lt;span class=&quot;lineno&quot;&gt; 8&lt;/span&gt;         &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;posting_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GetPostingList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;term&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;      
&lt;span class=&quot;lineno&quot;&gt; 9&lt;/span&gt;         &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;posting_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;       
&lt;span class=&quot;lineno&quot;&gt;10&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;posting_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DoublePostingList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;term&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;       
&lt;span class=&quot;lineno&quot;&gt;11&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;AddToPostingList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;posting_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dicID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;     
&lt;span class=&quot;lineno&quot;&gt;12&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sorted_term&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SortTerms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;        
&lt;span class=&quot;lineno&quot;&gt;13&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WriteBlockToDisk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sorted_term&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;        
&lt;span class=&quot;lineno&quot;&gt;14&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_file&lt;/span&gt;      
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4 id=&quot;bsbispimi&quot;&gt;BSBI和SPIMI的区别&lt;/h4&gt;
&lt;p&gt;BSBI在读入一块内存中的文档内容时，会构建这块文档的词项ID—文档ID对序列，在对序列进行排序后，构建这块文档的倒排索引表，也就是说倒排索引的构建是对读入的整个文件块这个整体。SPIMI当然也是将初始大规模文档划分成等大小的块，并按块读入内存，新建一个初始为空的字典，首先他直接以词项作为词典单位，也就是说在遍历内存中的文档时，对文档进行词条话和词干化后，查看每个词，如果这个词不再字典中，则将词加如词典中，并新建一个关于此词项的倒排记录表，如果词项在字典中存在，则需要在此词项的到拍记录表的基础上进行添加操作。由于实现并不清楚每个词项的倒排记录表的长度，所以初始设定倒排记录表的长度为某个较小的值，当倒排记录表已满时，可以按倍数进行扩展。&lt;/p&gt;

&lt;p&gt;SPIMI的倒排记录表是动态增长的，同时立刻就可以实现全体倒排记录表的收集。这样做有两个好处： &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;由于不需要排序操作，所以处理的速度更快。  &lt;/li&gt;
  &lt;li&gt;由于保留了倒排记录表对词项的归属关系，因此能够节省内存，词项的ID也不需要保存。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SPIMI算法的时间负责度是O（T），因为它不需要对词项ID-文档ID排序，所以操作最多和文档集大小成线性关系。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;分布式索引构建方法&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;实际中，文档集合一般相当大，一台计算机很难实现高效的实现索引构建。尤其是对于万维网来说。因此Web搜索引擎通常使用分布式索引构建（distribuction index）算法来构建索引，其索引结果也是分布式的，它往往按照词项或是文档分割后分布在多台计算机上。   &lt;/p&gt;

&lt;p&gt;这里介绍的分布式索引构建方法是MapReduce的一个应用。MapReduce是一个分布式的计算框架，它面向大规模计算机集群而设计。集群中有一个主控节点（master node）,主要负责任务在工作节点的分配和重分配。重分配是实现分布式框架的鲁棒性，因为集群在工作当中，可能工作节点会出现故障，这个时候主节点应当能识别这些故障并将故障机器的任务重新分配给其它可工作的工作节点。    &lt;/p&gt;

&lt;p&gt;一般来说MapReduce会通过键-值对（Key-Value pair）的转换处理，将一个大型的计算问题转换成较小的子问题。在索引构建中，键-值对就是（词项ID,文档ID）。在分布式索引构建中，词项到词项ID的映射同样要分布式进行，因此分布式的索引构建方法要比单机上的索引构建方法复杂的多。一种简单方法就是维护一张高频词到其ID的映射表，并将它复制到所有节点的计算机上，而对低频词则直接使用词项本身。&lt;/p&gt;

&lt;p&gt;MapReduce的Map过程将输入的数据片映射成键值对，这个映射过程对英语BSBI和SPIMI算法中的分析任务，执行Map过程的机器也称之为分析器（parser）。每个分析器将输出结果保存在本地的中间文件。&lt;/p&gt;

&lt;p&gt;Reduce主要是对中间结果进行合并，形成最终的索引。对每个词项（键值），获取此词项的所有文档集合并构建词项的倒排记录表主要通过倒排器来实现。   &lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;动态索引构建方法&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;上述建立索引的方法都是基于静态文档的，在很多情况下，文档都会随着时间动态变化的。那么，当文档更新速度很慢时，我们可以采用定期更新索引的策略。如果文档更新速度很快时，则实时更新索引的方法将十分耗时。&lt;/p&gt;

&lt;p&gt;可以采用如下方法实现动态索引的构建，这里我们主要维护两个索引，第一个主索引是对初始的文档集构建的索引，第二个辅助索引是在主索引建立之后随着时间推移，而更新的索引，辅助索引存放在内存中，这样实时检索时通过查询主索引和辅助索引实现。如果是对主索引在未来时间的更新，可以通过一个无效位向量实现，用无效位向量来标致文档的删除，同时在辅助索引中加入此文档的更新，便实现了主索引内容的更新。同时随着时间的推移，辅助索引的容量是不断增大的。当辅助索引长度大一某一值时，我们可以将辅助索引并入到主索引中。&lt;/p&gt;

&lt;p&gt;将辅助索引并入主索引的开销主要取决于索引为文件中的存储方式。如果将每个词项对应的倒排记录表存储为一个文件，则此词项的辅助索引和主索引的合并通过简单的将辅助索引扩展到主索引的倒排记录表即可。显示情况是因为文件管理的各种限制，将所有词项的倒排记录表分别存储为文件是不可行的。替代方案是将所有词项的倒排记录表存储为一个大的文件。&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/index-construction/&quot;&gt;索引构建&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on April 24, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[词典及容错式检索]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/dic-and-fault-tolerance-retrieve/" />
  <id>http://jwchennlp.github.com/dic-and-fault-tolerance-retrieve</id>
  <updated>2014-04-22T00:00:00-00:00</updated>
  <published>2014-04-22T00:00:00-04:00</published>
  
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;h2 id=&quot;section&quot;&gt;词典搜索的数据结构&lt;/h2&gt;

&lt;p&gt;在给定倒排索引和查询，首要任务是确定查询中的各个查询词是否在词汇表中，如果在则返回该词所对应的倒排记录表的指针。词汇表的查找操作通常采用一种称之为词典的数据结构，主要有两种解决方案：&lt;strong&gt;哈希表方式&lt;/strong&gt;和&lt;strong&gt;搜索树方式&lt;/strong&gt;。通常在选择选用何种解决方式时，我们需要考虑如下问题：                       &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;关键字的数目    &lt;/li&gt;
  &lt;li&gt;关键字的数目是经常变化还是相对固定，在变化的情况下，是只插入新关键字还是同时要删除某些旧关键字。  &lt;/li&gt;
  &lt;li&gt;不同关键字的相对访问频率如何。   &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于哈希表，词汇表中的每个词通过哈希函数映射成一个数，可以认为这个数代表这个词的存储地址。所以对于query里面的查询词来说，同样通过哈希函数应查看查询词映射到的地址，如果此地址存在数，则表示该查询词存在词典中。采用哈希表方式时，存在以下问题：          &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;哈希函数的空间要足够大，并且不易扩展。哈希函数必须要有足够大的空间来               存储字典，同时它的空间很难实时扩展，如需扩展，需要更改哈希映射函数，使得整个数据结构都发生变化。 &lt;/li&gt;
  &lt;li&gt;冲突问题的解决，因为哈希函数可能使得两个不同的词映射到统一地址，如何减少映射冲突也是一个要考虑的问题。   &lt;/li&gt;
  &lt;li&gt;哈希表方式很难解决前缀式查询，因为在不知道整体词的情况下，哈希映射函数是失效的。  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;搜索树可以很好的解决上述问题，它支持前缀式查询。最出名的搜索树是二叉树，每个内部节点都有两个字节点。在二叉树中搜索词要从根节点开始，每个内部节点代表一个二值测试，测试的结果用于确定下一步应该搜索的子树。二叉树的平衡性是实现高效搜索的关键，，平衡二叉树指的是任何节点的两个子树的高度相差小于等于1.下图为一个二叉树表示的词典的例子。为了实现搜索树的平衡性，我们必须在加入增加或删除节点时对树进行处理以保持树的平衡性，这里用B-树实现。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/binary-tree.png&quot; alt=&quot;image&quot; /&gt;   	&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;通配符查询&lt;/h2&gt;

&lt;p&gt;通配符通常用于以下情形：  &lt;br /&gt;
1. 用户不确定查询查询词的拼写。 &lt;br /&gt;
2. 用户知道某个查询词项可能有不同的拼写版本，并且要把包含这些版本的文档都查找出来。       &lt;br /&gt;
3. 用户查找某个查询词项的所有变形，这些词项还做了词干还原，但是用户并不知道搜索引擎是否做了词干还原。  &lt;br /&gt;
4. 用户不确定一个外来词或者短语的正确拼写形式。   &lt;/p&gt;

&lt;p&gt;当通配符出现在一个查询词的尾部时，如ca&lt;em&gt;，则是需要查找词典中所有词前两个字母是ca的所有词的文档。我们可以通过搜索树来实现这一查找，在搜索树的根（root）节点,首先我们确定首字母为c所指定的分支，而后在以分支作为搜索树查询a所对应的分支，这样这个分支下的所有单词都为符号ca&lt;/em&gt;查询的单词。   &lt;br /&gt;
然后，当通配符出现在词的首部时，如&lt;em&gt;ay,需要查找词典中后两个字母是ay的所有词项，显然用之前的搜索树不能实现这一查询。这里我们可以引入词典的反向B-树结构。前面的词典的B-树的构建是从词项的首字母开始，接着词的第二个字母知道最后一个字母构建B-树。反向B-树恰恰相反，它是从词典的尾字母开始，依次到倒数第二个字母直到第一个字母构建B-树。这样的反向B-树便能匹配通配符出现在词首部的查询。
那么对于通配符出现在查询词中间的查询，如t&lt;/em&gt;o,我们可以采用如下策略，首先用构建的B-树查找t&lt;em&gt;的所有词，而后采用构建的反向B-树查找符合&lt;/em&gt;o的所有词，最后两个查询的词求交集便是所查找的词。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;轮排索引&lt;/h3&gt;
&lt;p&gt;轮排索引是一种用于一般通配符查询的索引，它是倒排索引的一种特殊方式。它的工作原理为，首先引入一终结符$，对词项集合中的每个词在其末尾增加$符号。如词项hello扩展成hello$,随后我们需要按如下方式建立词的轮排索引，对词hello$连续进行首位翻转，将出现的所有形式记录的集合称之为轮排词汇表。hello$的轮排索引如图所示: &lt;br /&gt;
&lt;img src=&quot;../images/permutern-index.png&quot; alt=&quot;image&quot; /&gt;   &lt;br /&gt;
那么如何用轮排索引实现通配符查询呢，由上图我们知道轮排索引中的任何一个状态都指向词项hello，也就是说ello$h或者llo$he的查询过程都会通过轮排索引指向词项hello的查询过程。所以例如查询通配符h&lt;em&gt;llo,处理的关键是将通配符&lt;/em&gt;移动到词的尾部，将h&lt;em&gt;llo转换成h&lt;/em&gt;llo$,接着进行翻转得到llo$h&lt;em&gt;,接着在轮排索引中查找该字符串，我们发现llo$h&lt;/em&gt;与hello词的轮排索引中的llo$he一致，所以hello是满足条件的查询结果。     &lt;br /&gt;
对于查询中存在多个通配符的情况，如查询（fi&lt;em&gt;mo&lt;/em&gt;er）,我们可以按如下方式进行处理，首先查找er$fi*的所有结果，接着可以通过穷举法过滤出包含mo的词，这些词便是符合通配符查询的结果。 &lt;/p&gt;

&lt;h3 id=&quot;k-gram&quot;&gt;支持通配符查询的k-gram索引&lt;/h3&gt;
&lt;p&gt;上面介绍的轮排索引结构简单，但是在构建轮排索引的过程中，我们需要对词进行旋转并记录所有旋转的结果，这会引起存储空间的急剧增加。     &lt;/p&gt;

&lt;p&gt;k-gram索引是如下的倒排索引机制，它将原始词典中的所有词项进行拆分，每个词项都拆分成若干个长度为k的新的词项，并根据这些新的词项构建倒排索引，如happy按照3-gram拆分成的新词词项有$ha,hap,app,ppy,py$,这里用$来对词的开始和结束进行标记。这里倒排索引的构建方式与第一章提到的略微不同，之前的倒排索引是词典是文档中经过词条化和语言话处理的所有词，而倒排记录表是这些词所出现的文档。而这里词典则是文档中的所有词根据k-gram拆分的所有新词，倒排记录表这是包含这些长度为k的新词的原始词。如3-gram的新词etr对应的倒排记录表为,词项为etr，倒排记录表为所有包含etr的词：    &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/k-gram-index.png&quot; alt=&quot;image&quot; /&gt;    &lt;/p&gt;

&lt;p&gt;那么k-gram是如何实现通配符查询的呢，如查询he&lt;em&gt;lo,是要查询首字符为he，尾字符为lo的所有词，根据3-gram索引，我们可以够找如下的布尔查询$heANDlo$,则3-gram的查询词便是所期望的
结果。k-gram索引有时也会导致非预期的结果，如查询red&lt;/em&gt;,根据3-gram索引构建的布尔查询为$reANDred,其返回结果可能包含retired，但显然这个词并不符合初始期望。为了解决这一问题，我们可以引入一个后过滤的步骤，实现方式很简单，用初始的查询词与返回的词进行匹配，那些成功匹配的词便是符合要求的词。&lt;br /&gt;
通配符查找往往是非常耗时的，对于单个通配符查询，我们可能要构建轮排索引或者k-gram索引来返回中间结果，并且对这中间结果要求交集来返回确切的要查找的词，最后才依据这些词通过倒排索引来查找这些词所对应的文档。    &lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;拼写校正&lt;/h2&gt;
&lt;p&gt;拼写校正是在用户输入某个查询词或查询短语时，用户能识别其中词的拼写错误并返回正确词的查询结果。&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;拼写校正的实现&lt;/h3&gt;

&lt;p&gt;对于大多数拼写校正算法而言，有以下两条基本规则：        &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对于一个错误拼写的查询中，则需要在其所有正确的拼写中，返回最近的正确拼写的查询。&lt;/li&gt;
  &lt;li&gt;当两个正确拼写查询临近度相等时，则需要返回更常见的那个正确查询。更常见可以通过以下两个方式衡量，可以统计文档集合中两个查询出现的次数，出现次数高的标记为“更常见”。也可以统计用户查询日志中两个查询的出现次数，出现次数更高的标记为”更常见”。  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-5&quot;&gt;拼写校正的方法&lt;/h3&gt;
&lt;p&gt;词独立校正:不管查询是单个词还是多个词构成的短语，对查询的词的拼写校正是独立进行的，也就是说是上下文独立的，即某个词是否校正与上下文语境没有关联。校正方法主要有编辑距离方法和k-gram重合度方法。 &lt;br /&gt;
上下文敏感校正:则是在校正过程中，会根据上下文信息来决定词的校正。&lt;/p&gt;

&lt;h4 id=&quot;section-6&quot;&gt;编辑距离&lt;/h4&gt;

&lt;p&gt;给定两个字符串S1和S2，两者的编辑距离定义为由S1转换成S2的最小编辑操作数。通常这些编辑操作包括：     &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;将一个字符插入字符串   &lt;/li&gt;
  &lt;li&gt;将一个字符从字符串中删除  &lt;/li&gt;
  &lt;li&gt;将字符串中的一个字符替换成另一个字符    &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可以在O(S1*S2）的时间复杂度下计算S1和S2之间的编辑距离，主要方法是采用动态规划的思想（类似于动态规划中的求最长公共子串问题），其中S1和S2以字符数组方式进行存放。整数矩阵m的行数和列书分表代表两个字符串的长度，算法在运行过程中不断填写矩阵元素。例如，在算法结束时，m[i,j]表示S1的前i个字符和S2的前j个字符的编辑距离。其代码实现如下：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt; 
       
&lt;span class=&quot;n&quot;&gt;EditDistance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;    
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;—&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;      
&lt;span class=&quot;n&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;       
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;—&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;      
&lt;span class=&quot;n&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;—&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;      
&lt;span class=&quot;n&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;—&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;       
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;       
        &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;       
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;        
        &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;       
    &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;        
&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;加入对于某个拼写错误查询q，我们需要从词典W中找出与q相邻最近的正确词项，最简单的方法是便利W中的所有词项wi，计算wi和q之间的编辑距离，最后返回和q最近编辑距离的词项wi，并返回wi所指向的文档。很显然，这种遍历的方法是十分低效的，我们可以采用如下的启发式优化策略，我们将搜索限定在首字母相同的词典词项上，对于查询q，我们认为错误不出现在首字符上，这样对于词典W，我们只计算与q有相同首字符的词项与q之间的编辑距离。当然在此基础上更复杂的方法是加入轮排索引，对于词错误拼写查询helo，忽略词的终结符$,构建词的轮排索引{helo，ohel，lohe，eloh}，对轮排索引中的每个词，按照上述的启发式规则与词典W中查找最近编辑距离的正确拼写。（个人理解）&lt;/p&gt;

&lt;h4 id=&quot;k-gram-1&quot;&gt;拼写校正中的k-gram索引&lt;/h4&gt;

&lt;p&gt;对与某一个错误拼写查询，我们可以根据之前的构建k-gram索引来实现拼写校正，过程如下：
对于错误的拼写单词，我们可以将此单词拆分成长度为k的多个字符串，并查找这些字符串所对应的倒排索引表，这些倒排索引表分别表示包含这些字符串的拼写正确的单词，这里我们认为，只要一个单词在在写倒排索引表中出现次数超过某一阀值m，则认为这个词是原错误拼写的正确拼写结果。例如错误拼写bord，其2-gram索引拆分成的新词有{$b,bo,or,rd,d$},这里去除词$b和d$,从文档集合中查找bo，or，rd对应的倒排记录表，如下所示：    &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/sc-k-gram-index.png&quot; alt=&quot;image&quot; /&gt; &lt;/p&gt;

&lt;p&gt;随后，我们只要遍历这些倒排记录表，找到那些在倒排记录表中出现次数高的词，便是正确的拼写词。k-gram索引的缺点像boardroom这种不可能是bord的正确拼写形式的词也会被检索出来。所以我们需要计算词汇表中词项与查询q之间的更精确的重合度计算方法。可以采用雅可比系数对先前的线性扫描合并方法进行修正。雅可比系数的计算公式是length(AandB)/length(AorB)，其中A和B分别表示查询q和词汇表词项中的k-gram集合。当扫描到词t时，计算出q和t的雅可比系数，如果系数大于某一阀值，则将词t返回。
采用雅可比系数进行验证的时候，我们需要知道q和t的k-gram索引，首先q的k-gram索引是已知的，那么在验证的过程中我们需要遍历所有q的k-gram索引中出现的词t的k-gram索引，如果穷举词t的k-gram索引是个缓慢的过程。我们可以通过一下方式来进行简化处理，当我们知道词t的长度时，可以认为他的k-gram长度为length(t)-k+1,这样能快速计算出雅可比系数。&lt;/p&gt;

&lt;h4 id=&quot;section-7&quot;&gt;上下文敏感的拼写校正&lt;/h4&gt;

&lt;p&gt;当查询的短语中每个单词都是正确的单词，但是返回的查询结果很少时，我们可以认为单词中存在拼写错误，并对其中的单词查找其正确的拼写结果，并返回修正后的短语的查询结果。当采用这种穷举法对词语中的词进行拼写校正时，工作量大，效率低。这时可以采用启发式的方法通过用户的查询日志来统计最有查询短语拼写校正后最有可能出现的短语。&lt;/p&gt;

&lt;h4 id=&quot;section-8&quot;&gt;参考资料&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://www.google.com.hk/search?q=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;amp;oq=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;amp;aqs=chrome..69i57j69i65j69i61l3j0.3218j0j1&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&quot;&gt;信息检索导论-词典及容错式检索&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;说明：&lt;/strong&gt;文章主要内容和图片来自信息检索导论一书。&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/dic-and-fault-tolerance-retrieve/&quot;&gt;词典及容错式检索&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on April 22, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[词典和倒排记录表的建立]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/ir-dic-inverted-index/" />
  <id>http://jwchennlp.github.com/ir-dic-inverted-index</id>
  <published>2014-04-18T00:00:00-04:00</published>
  <updated>2014-04-18T00:00:00-04:00</updated>
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;p&gt;建立倒排索引的过程可概括为:&lt;br /&gt;
* 收集用于建立索引的文档&lt;br /&gt;
* 词条化&lt;br /&gt;
* 对词条进行处理,得到词项&lt;br /&gt;
* 根据词项和文档建立索引  &lt;/p&gt;

&lt;h4 id=&quot;section&quot;&gt;文档单位的选择&lt;/h4&gt;
&lt;p&gt;在收集索引文档的过程中,会比较直观的理解每篇文档是便是用于建立索引的索引单位.但很多情况下并非如此,如传统的Unix文件系统将某个目录下的文件都放在一个文件中,我们更倾向以对每个邮件建立一个文档索引,其中邮件中存在附件时,我们希望将附件解压缩并将解压缩文件中的每个文件作为文档建立索引.所以,对收集的文档集合我们应该确定用于建立索引的最小单位(文档).
在长文档中,更一般的说法是存在一个&lt;strong&gt;“索引粒度”&lt;/strong&gt;的问题,对一个书库而言,将一本书当作索引单位(文档)效果会很不理想.例如,查询query是”chinese toys”,那么可能返回这样一本书,第一章中出现”chinese”,最后一章中出现”toys”词,但是这本书跟query的相关性应该是很低的.所以,一个比较可取的方法是对书的每一章或每一段看作文档来建立索引,这样的匹配结果会跟query更相关.当然,索引粒度也不是越小越好,比如,如果我们以句子作为索引单位时,可能要查找的query的分布在几个句子中,这样这几个句子形成的段落是比较相关的结果.这种细粒度的索引会使得准确率升高而召回率降低,索引粒度太效果相反.所以为了权衡召回率和准确率,我们应该设选择合理的索引粒度.&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;词条化&lt;/h4&gt;
&lt;p&gt;词条化主要是将文档中的字符序列才分成一系列子序列的过程,其中每个子序列称之为(token).在这个过程中会进行一系列特殊处理,如删除标点符号等. &lt;br /&gt;
&lt;code&gt;
      输入: I hava a dream,become a good programer!       
      输出: I have a dream become a good programer
&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;词条指的是文档集合中出现的字符序列的实例,词条类是指相同词条构成的集合.  &lt;/li&gt;
  &lt;li&gt;词项指的是在信息检索系统词典中所包含的某个经过归一化处理的词条类&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;词项集合和词条集合可以完全不同,例如词条集合是篮球,足球一类的词汇,此类词条集合的词项可以表示为体育.在实际的信息检索系统中,词项往往和词条密切相关.但是,词项未必就是原始的词条,实际上它往往需要对词条进行归一化处理来得到.&lt;/p&gt;

&lt;p&gt;词条化的主要任务是确定哪些才是正确的词条.在英文文档中,大多是简单的按空格将字符序列进行划分生成词条.可是在有些情况处理会变得复杂,如应为中的上撇号”’“,它可以用来代表所有关系,也有用来代表缩写.如:&lt;br /&gt;
&lt;code&gt;
		Mr.O'Neill thinks that the boy's stories about the Chile's capital aren't amusing.  
&lt;/code&gt;
对于O’Neill来说,词条化结果可以是:{neil},{oneill},{o’neill},{o’,neil},{o,neil}.   &lt;br /&gt;
这里就需要采用词条化工具来对这类情况进行字符序列的词条化,应该注意的是,&lt;strong&gt;对query的词条化处理和对文档集合的词条化处理应该采用相同的机制&lt;/strong&gt;.  &lt;/p&gt;

&lt;p&gt;在进行词条化的时候要考虑”C++”,”C#”等一类的特定领域词,”New York”,”Los Angle”名称词,新出现的词或连字符连接的词.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;停用词:&lt;/strong&gt;指的是在文档中出现频率高,但是与文档主题关系不大的词.在某些情况下,停用词在文档和用户query进行匹配时价值不大,可以用词汇表中去除.&lt;/p&gt;

&lt;h4 id=&quot;section-2&quot;&gt;词条归一化&lt;/h4&gt;
&lt;p&gt;词条归一化(token normalization)是指将看起来不完全一致,但表述意思相近的多个词条归纳成一个等价类,以便在它们之间进行匹配的过程.主要有以下两种方法:  &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;隐式的建立等价类:例如,采用去掉链接字符的映射规则来建立等价类,将moto-car和motocar映射成词项motocar,这样,对任何一个词进行查询,都会返回包含另一个词的文档.  &lt;/li&gt;
  &lt;li&gt;维护多个非归一化词条之间的关联关系,可以进一步扩展为构建同义词词表的手工构建.比如将car和automobile归为同义词.有以下两种方法: &lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;采用非归一化的词条建立索引,并为某个查询词项维护一个有多个词组成的查询扩展词表.当输入一个查询词项时,则根据扩展词表进行扩展并将扩展后得到的所有词的倒排记录表进行合并.   &lt;/li&gt;
      &lt;li&gt;在索引构建时就对词进行扩展.比如对于包含automobile的文档(文档中不包括词car),正常只建立automobile的索引,此时需要同时建立
  automobilt和car的索引.第一种方法需要维护一个词的扩展词表,在查询时需要访问该词表,更耗时.第二个则实在建立索引时便已经构建扩展词表的索引,更好空间.   &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-3&quot;&gt;词干还原和词形归并&lt;/h4&gt;
&lt;p&gt;词干还原和词形归并是为了减少词的曲折变化的形式，并且有时候会将派生词转化成基本形式。因为在语法中，有些词在不同的语境中包含不同的形态，如origanize，oraginizes，originizing。同时语言中也存在大量的同源词，如democracy，democratic，democratization。那么，在检索过程同，如果根据搜索词返回其同源词的文档，能返回更多相关的结果。	
&lt;strong&gt;词干还原&lt;/strong&gt;通常是用启发式的规则对词两端的词缀进行粗略进行处理的过程。&lt;strong&gt;词形归并&lt;/strong&gt;通常是利用词汇表和词形分析来去除屈折词缀，从而返回词的原型或词典中的词的过程，返回的结果称为词元。两者的&lt;em&gt;区别&lt;/em&gt;还在于：词干还原一般情况下会将多个派生相关词合并在一起，词形归并通常只将同一词元的不同曲折形式进行合并。	&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;基于跳表的倒排记录表快速合并算法&lt;/h3&gt;
&lt;p&gt;我们知道，初始的最两个排序好的倒排记录表进行合并（求交集）时，只要维护倒排记录表的指针，并遍历两个表，时间复杂度为O（m+n），（m，n为表的长度）。
跳表的主要思想时，我们在表中加入一些位置加入一些跳表指针，这样在比较的过程中可以考虑是否从当前的条表指针直接跳到下一个跳表指针，忽略中间的那些倒排记录。这里需要考虑的问题是条表步长的问题（相邻条表指针间元素个数），步长短，则所需的存储空间大，能通过跳转指针进行跳转的机会变大。步长长，所需存储空间小，在遍历过程中进行跳转的几率小。主要以空间换取时间来提高合并效率。		&lt;/p&gt;

&lt;p&gt;索引相对固定时，建立有效的跳表比较固定。但是如果倒排记录表由于经常更新而发生变化，那么条表指针的建立比较困难。		&lt;/p&gt;

&lt;h4 id=&quot;section-5&quot;&gt;处理短语查询&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;二元词索引&lt;/strong&gt;是将检索短语且分成相邻两个词的元组，并对这些元组当做词项进行检索，将所有词项检索所的结果通过布尔查询的与操作获取结果。如：		
&lt;code&gt;
	短语：$w_1$,$w_2$,...,$w_m$	
	二元词词项：$w_1$$w_2$,$w_2$$W_3$,...,$w_m-1$$w_m$	
&lt;/code&gt;
其实，在检索过程中，用名词或名词短语来表示用户查询的概念具有相当特殊的地位。但是相关的名词往往被各种虚词隔开，所以在处理短语查询时，可以先对短语进行词条化，之后再进行词性标注，这样在对里面的名词按照二元词索引的思想来查找结果。	&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;位置信息索引&lt;/strong&gt;,采用位置信息索引时，需要建立每个文档中的每个词在文档中出现的位置的索引。词wi在dj中的倒排索引可以表述为：		
&lt;code&gt;
	文档dj：（位置1,位置2,....）
&lt;/code&gt;
那么利用位置信息索引如何实现短语查询呢？对于相邻的两个词，我们首先计算出他们的倒排记录表并进行合并，对合并的结果进行如下处理，对结果中的每一个文档，我们需要查找两个词在文档中的位置索引表，如果两个词各自的位置索引表中存在两个词的位置也是相邻的并且次序准确的话（计算词之间的偏移距离），这这个文档符合要求。如此往复便能得到结果。		
采用位置索引会增加存储空间，并且会使倒排记录表的合并复杂性增加。	&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;混合索引机制&lt;/strong&gt;，对有些查询使用二元词索引，而对其它短语查询使用位置信息索引。二元词索引中的短语可以根据用户日志统计得出。处理开销最大的短语往往是这样的短语，短语中的每个词在文档中十分常见，但是组合起来却很少见。	&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参考资料&lt;/strong&gt;		
&lt;a href=&quot;https://www.google.com.hk/search?q=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;amp;oq=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;amp;aqs=chrome..69i57j69i65j69i61l3j0.3218j0j1&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&quot;&gt;信息检索-词项词典及倒排索引表&lt;/a&gt;		&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/ir-dic-inverted-index/&quot;&gt;词典和倒排记录表的建立&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on April 18, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[IR 倒排索引和布尔查询]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/ir-inverted-index/" />
  <id>http://jwchennlp.github.com/ir-inverted-index</id>
  <updated>2014-04-16T00:00:00-00:00</updated>
  <published>2014-04-16T00:00:00-04:00</published>
  
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;p&gt;&lt;strong&gt;信息检索&lt;/strong&gt;的定义是从大规模非结构化数据(通常为文本)的集合中找出满足用户需求的资料(通常是文档)的过程.
检索的过程可理解为用户输入query,从文档集合中找出与query相关的文档并展示.最简单的检索方式是线性查询,根据query遍历所有的文档集合,	查找出那些文档集合与query是相关的.这种方式存在以下问题:                                                                  &lt;br /&gt;
  1. 文档规模很大时,对所有文档的遍历查找是个费时的过程&lt;br /&gt;
  2. 不能满足灵活匹配方式的要求(如两个词在同一句话中出现).&lt;br /&gt;
  3. 无法对结果进行排序.检索结果应该按照相关性等需求返回最佳答案.  &lt;/p&gt;

&lt;p&gt;可以采用如下方法替代线性扫描方式,对于所有的文档集合,构建一个词项-文档的矩阵,其中词项表示文档集合中的所有词列表,
文档表示所有集合中的文档,矩阵中的元素M(i,j)则表示词i是否在文档j中出现(M(i,j)为1时表示出现).矩阵的每一行表示词在文档集合中的出现情况,矩阵的没一列表示相应的文档中的词的集合.这样当检索query的时候,只需要根据query中的词在矩阵中查找词在文档中的出现情况,最后进行结果合并便可的出结果.&lt;/p&gt;

&lt;p&gt;上述方法相对现行扫描在时间效率上有了很大提升.如果文档集合有1000万,文档集合的词集合有50万个,则需要维护一个50万*1000万的矩阵,		真实情况是这个矩阵会非常洗漱,因为一篇文档只包含50万词表中的少数词,同时一个词只在少数的文档中出现.而我们关心的只是M(i,j)不为0的元素.采用只存储词项-文档矩阵中元素不为0的数据结构效果会更好.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;倒排索引&lt;/strong&gt;是一种索引方法,被用来存储在全文搜索下某个单词在一个文档或一组文档中的存储位置的映射.
假设有三个检索文本:   &lt;br /&gt;
* $T_0$=”it is what it is”&lt;br /&gt;
* $T_1$=”what is it”    &lt;br /&gt;
* $T_2$=”it is a banana” &lt;br /&gt;
根据以上文档当初的到拍索引为:   &lt;/p&gt;

&lt;p&gt;```
 “a”:      {2}    &lt;/p&gt;

&lt;p&gt;“banana”: {2}   &lt;/p&gt;

&lt;p&gt;“is”:     {0, 1, 2}    &lt;/p&gt;

&lt;p&gt;“it”:     {0, 1, 2}  &lt;br /&gt;
```   &lt;/p&gt;

&lt;p&gt;倒排索引的建立规则如下,首先要建立词典,词典中的词为文档中词的集合,对于词典中的每个词都有一个记录该词在文档中的出现列表,这个表成为倒排记录.倒排索引的词典部分存放在内存中,而每个词指向的倒排记录表存放在词典中.&lt;/p&gt;

&lt;p&gt;**用倒排索引和基本布尔检索模型来处理一个查询 **
一般的检索的query由若干个词组成,可以用基本的布尔查询(与,或,非)在连接这些词.
对于查询 word1 And word2
其检索过程如下:
  * 在词典中定位world1
  * 返回词word1的倒排记录表
  * 在词典中定位word2
  * 返回词word2的倒排记录表
  * 对倒排记录表求交集,所对应的文档既为结果&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;交集处理:&lt;/strong&gt;对于倒排索引表求交集的过程,可以通过以下方法进行优化&lt;br /&gt;
  * 构建倒排索引时,倒排记录表是有序构建的,如对文档进行编号,,倒排记录表按照文档标号从小到达顺序生成&lt;br /&gt;
  * 在求交集的过程中,对两个倒排记录表分别维护一个指针,当指针所指元素相同时,保留元素并同时指针后移,当指针所指元素不同时,将元素较小指针向后移动,最后保留的元素既为交集,时间复杂度为O($N_1$+$N_2$),N为两个索引记录表的长度.&lt;br /&gt;
  * 对于多个倒排索引表,可以按照倒排记录表的长度从小到大进行处理,优先对两个最短的倒排记录表求交集.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;布尔检索优缺点&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;优点:&lt;/strong&gt;
  1.布尔检索表达上精确,文档要么满足,要么不满足,可以让用户对返回结果拥有更好的控制力和透明度.
  2.对某些领域信息,布尔检索内部也可以提供排序机制
&lt;strong&gt;缺点:&lt;/strong&gt;
  1.布尔检索不能满足灵活匹配方式的要求.&lt;br /&gt;
  2.采用AND操作符产生的结果正确率高而召回率低,采用OR操作符召回率高而正确率低.&lt;br /&gt;
  3.只记录词项存在文档中存在或不存在,但是我们往往需要累加各种证据来得到文档相关性的可信度.&lt;br /&gt;
  4.布尔模型返回的是一个无序的结果,但是我们往往需要对返回的结果进行排序.  &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参考资料&lt;/strong&gt;
&lt;a href=&quot;http://zh.wikipedia.org/wiki/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95#.E4.BE.8B.E5.AD.90&quot;&gt;倒排索引-维基百科&lt;/a&gt;
&lt;a href=&quot;https://www.google.com.hk/search?q=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;amp;oq=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;amp;aqs=chrome..69i57j69i65j69i61l3j0.3218j0j1&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&quot;&gt;信息检索导论-布尔检索&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/ir-inverted-index/&quot;&gt;IR 倒排索引和布尔查询&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on April 16, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Sample Post]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/hello-world/" />
  <id>http://jwchennlp.github.com/hello-world</id>
  <updated>2014-04-13T00:00:00-00:00</updated>
  <published>2014-04-13T00:00:00-04:00</published>
  
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;p&gt;你好，哈尔滨！&lt;/p&gt;

&lt;h2 id=&quot;why&quot;&gt;why&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;怎么感觉有些问题呢    &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;c&quot;&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  

&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/hello-world/&quot;&gt;Sample Post&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on April 13, 2014.&lt;/p&gt;</content>
</entry>

</feed>
