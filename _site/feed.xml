<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">My blog</title>
<generator uri="https://github.com/mojombo/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="http://jwchennlp.github.com/feed.xml" />
<link rel="alternate" type="text/html" href="http://jwchennlp.github.com" />
<updated>2014-05-07T03:17:22-04:00</updated>
<id>http://jwchennlp.github.com/</id>
<author>
  <name>jwchen</name>
  <uri>http://jwchennlp.github.com/</uri>
  <email>hit1093710417@email.com</email>
</author>


<entry>
  <title type="html"><![CDATA[索引压缩]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/index-compress/" />
  <id>http://jwchennlp.github.com/index-compress</id>
  <updated>2014-05-06 09:01:16 UTCT00:00:00-00:00</updated>
  <published>2014-05-06T00:00:00-04:00</published>
  
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;h3 id=&quot;section&quot;&gt;为什么要进行索引压缩？&lt;/h3&gt;
&lt;hr /&gt;
&lt;p&gt;进行索引压缩有以下优点：  &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;节省磁盘空间．  &lt;/li&gt;
  &lt;li&gt;增加高速缓存(cache)的利用率.&lt;br /&gt;
倒排索引词典是放在内存中的，倒排记录表放在磁盘上．对与到拍记录上的某些词项ｔ，我们是需要经常访问的，如果将这次词项ｔ所对应的到拍记录表压缩后放在高速缓存中，只要采用得当的解压缩算法，那么当查询词项ｔ的倒排记录表时，只需要访问cache，而不用从磁盘读取数据，能充分减少IR系统的响应时间． &lt;/li&gt;
  &lt;li&gt;压缩能够加快从磁盘读取数据的速度．&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;压缩技术分为有损压缩和无损压缩，有损压缩指的是压缩后，原始数据的所有信息都保存下来了．词干还原，大小写转换都属于有损压缩．   &lt;/p&gt;

&lt;h3 id=&quot;heaps&quot;&gt;Heaps定律：词项数目的估计&lt;/h3&gt;

&lt;p&gt;heaps定律认为，文档集大小和词汇量之间存在对数上的线性关系.它将词项的数目估计为文档集大小的函数:&lt;script type=&quot;math/tex&quot;&gt;M=kT^b&lt;/script&gt;,其中Ｔ代表文档集合中的词条的个数． &lt;/p&gt;

&lt;p&gt;不同文档集下ｋ取值差异较大，因为词汇量大小取决于文档本身以及对他进行处理的方式．当进行词干还原，大小写转换时将降低词汇量增长的速度，允许加入数字和容忍拼写错误则会增加增长率．无论参数取值如何，heaps定律满足一下两条性质：    &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;词汇量会随着文档集的增加而增加，不会趋于一个定值．     &lt;/li&gt;
  &lt;li&gt;大规模文档集的词汇量也会很大．       &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;zipf&quot;&gt;Zipf定律：词项在文档中的分布&lt;/h3&gt;
&lt;p&gt;Zipf定律用于估计词项在文档中分布，假设$t_1$用于表示文档集中出现最多的词，$t_2$用于表示文档集中出现第二多的词，文档集合中出现第i多的词的文档频率$cf_i$与$\frac{1}{i}$成正比:   &lt;br /&gt;
    &lt;script type=&quot;math/tex&quot;&gt;cf_i=k\frac{1}{i}&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;词典压缩&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;###为什么要进行词典压缩
理想情况下在建立好索引后，我们希望将词典存放在内存中，但是这往往很难实现，尤其是对于实用的搜索引擎和嵌入式系统．限制IR系统的响应之间的一个因素包多对磁盘的访问次数．所以，如果通过压缩来讲所有的或大部分的词典存入内存，将大大加快IR系统的响应速度．    &lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;将词典看作单一字符串的压缩方法&lt;/h3&gt;
&lt;p&gt;采用如下的数据结构进行存储：一个定长的数组用于存储词项（２０Ｂ），４Ｂ的空间用于存储文档频率，４Ｂ的空间用于存储指向倒排记录表的指针．对于一个包含Ｍ个词项的文档空间来说，词典的总空间为M*(20+4+4),当Ｍ＝400,000时，占用空间为11.2MB.
&lt;img src=&quot;../images/1/dic_compress_1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这种方法存在很大的不足，首先大部分的英文词平均长度为8B,这显然造成了大部分的空间浪费，其次也存在有些词的长度超过20B,导致的结果便是不能存储这些词．&lt;/p&gt;

&lt;p&gt;我们可以采用如下的改进措施，我们建立一个字符串在存储字典中的所有词项,4B的空间存储文档频率,4B的空间存储倒排记录表的指针，这个指针指向前面所有词典构成的长字符串，在长字符串中我们需要每一个词加入一个定位指针，用于指定下一个词的开始位置和当前词的结束位置，由于有400,000个词，每个词为８B,所以寻址空间为400,000&lt;em&gt;8=3.2&lt;/em&gt;$10^6$,所以可以用一个长为$\log{3.2&lt;em&gt;10^6}\approx22b$，即３Ｂ的指针来表示．词典的总空间为M&lt;/em&gt;(4+4+3+8)=7.6MB．  &lt;br /&gt;
&lt;img src=&quot;../images/1/1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;按块存储&lt;/h3&gt;
&lt;p&gt;对上面的压缩方法进行一个变形，这里不再对每个词项都维护一个指向字符串(所有词的组合)的指针．我们首先将我们的词典按块进行划分，例如每５个词为一块，这样对没一个块只需要维护一个这个块指向字符串的指针，同时在长字符串中，我们需要加入一个空间用于指定当前词的长度．在这种机制下，假设一个块内有ｋ个词，我们减少了(k-1)个指针的空间，但是我们需要在字符串中对没个词增加空间以记录其词的长度．假设每个块内有４个词，减少的指针空间为9B,同时对４个词需要增加４Ｂ的空间用于记录词的长度，所以没４个词产生了5B的压缩，所以压缩的空间为400,000*$\frac{1}{4}$*5=0.5MB.    &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt;我们在这里维护了两个指针，一个用于指向倒排记录表，一个指向字符串用词项的位置，我们压缩的部分是词项指针．   &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/1/2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们发现，每个块内的词越多时，则可以压缩的空间越大．但是并非块内词越多越好，在进行词项查找时，对于块间的词我们可以通过二分查找快速定位，但是在快内查找时则是简单的线性遍历，所以我们必须在查找速度和空间压缩见进行权衡．    &lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;倒排记录表的压缩&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;倒排记录表的压缩基于下面一个前提，当用文档ID来表示倒排记录表，对与高频词来说，倒排记录表中的记录多并且相邻的记录之间差距会很小，当某高频词出现在某篇文档中时，将其相近的文档中出现高频词的概率会很大．这就给我们提供了对倒排记录表进行压缩的灵感，正常情况下我们对到拍记录表中的每个文档id,都是用定长的空间来存储的，那么对那些高频词的话，我可以通过存储他们倒排记录表相邻的距离（明显小于存储文档id的长度）来达到压缩的目的．     &lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;可变字节码&lt;/h3&gt;
&lt;p&gt;VB(Variable byte,可变字节)码的思想为，我们采用整数个字节来存储文档id,每个字节的后７位为有效编码，第一位为延续位，表示本次编码的结束与否,’1’表示结束．     &lt;/p&gt;

&lt;p&gt;可变字节码的解码过程如下，根据延续位（一直获取字节直到字节的首位为１）来获取编码结果，对编码结果进行以下处理，去除所有的延续位，剩余有效编码表示间隔位，将此编码值与前一个编码的结果进行累加即表示文档的ID. 
&lt;img src=&quot;../images/1/3.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-6&quot;&gt;γ编码&lt;/h3&gt;

&lt;p&gt;一元编码：将数值为ｎ的数用ｎ个１并在之后加上一个０来表示的编码方式．  &lt;/p&gt;

&lt;p&gt;γ编码主要由两部分组成，偏移量(offset)和长度(length)．长度是数组的二进制编码，但是去除了首位１，长度则是偏移量的长度，但是是通过一元编码的方式实现．对于数值5,二进制编码是101,去掉首位的１，其偏移量是01,偏移量长度为２，则由一元编码表示为110,所以数值５的γ编码为11001.
&lt;img src=&quot;../images/1/4.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们发现对数值为Ｋ的数进行二进制编码，其偏移量的长度为$\lfloor\log{k}\rfloor$,其长度的长度为$\lfloor\log{k}\rfloor$+1,所以，数的γ编码长度为２$\lfloor\log{k}\rfloor$＋１．      &lt;/p&gt;

&lt;p&gt;有一点不太明白的是采用γ编码是如何实现数据压缩的呢？书的原文是这么说的：      &lt;br /&gt;
&lt;img src=&quot;../images/1/5.png&quot; alt=&quot;image&quot; /&gt;
按上面理解，采用ｎ为进行进行表示，那么间距在1~$2^\left(n-1\right)－１$之间都将产生浪费，并在在间距为$2^n$时不能表示．&lt;/p&gt;

&lt;p&gt;我的理解是这样的，我们有对倒排记录表的实现一般也是通过链表或是定长数组来实现的，当采用定长的编码格式来存储每个文档ＩＤ时，必然会产生很大的浪费．那么如何通过变长的编码格式并且不需要额外的数组或指针来表明文档的长度，这边是γ编码所做的事了，让我们看一下γ编码的的解码过程：11001,我们首先遍历该编码，知道遇到０时停止，发现长度为２，剩下的偏移量为01,我们知道实际的二进制数为101,也就是说我们通过编码本身可以确定文档id,不需要进行额外的存储．&lt;/p&gt;

&lt;h4 id=&quot;section-7&quot;&gt;参考资料&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://www.google.com.hk/search?q=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;amp;oq=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;amp;aqs=chrome..69i57j69i65j69i61l3j0.3218j0j1&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&quot;&gt;索引压缩&lt;/a&gt;     &lt;br /&gt;
&lt;strong&gt;说明：&lt;/strong&gt;文章主要内容和图片来自信息检索导论一书。&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/index-compress/&quot;&gt;索引压缩&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on May 06, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[odps_sql]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/odps-sql/" />
  <id>http://jwchennlp.github.com/odps-sql</id>
  <updated>2014-04-26 14:45:37 UTCT00:00:00-00:00</updated>
  <published>2014-04-26T00:00:00-04:00</published>
  
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;p&gt;odps(open data processing service，开源数据处理服务)是阿里巴巴的分布式计算平台。&lt;/p&gt;

&lt;p&gt;数据以sql表格的形式存放在odps中，我们可以是使用类似与sql命令的方式对数据进行操作。当让sql中嵌入了odps平台自己的函数和命令。&lt;/p&gt;

&lt;p&gt;文档学习ing…&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;表格建立&lt;/h2&gt;

&lt;p&gt;创建表格语句如下：   &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;sql&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;table&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exist&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sales&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shop_name&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;      
&lt;span class=&quot;lineno&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;     
&lt;span class=&quot;lineno&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partitioned&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sale_date&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;region&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;     
&lt;span class=&quot;lineno&quot;&gt;5&lt;/span&gt;     &lt;span class=&quot;c1&quot;&gt;--创建一张分区表sales&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;partitioned by指定了分区字段，采用分区字段主要是在跟新，新增和读取分区数据时不需要做全表扫描，可以提高效率。       &lt;/p&gt;

&lt;p&gt;可以用命令create table…as select…来新建表格，如：        &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;sql&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;table&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sales1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt;      
&lt;span class=&quot;lineno&quot;&gt;2&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sales&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;这样在建立表格的同时，将sales的数据复制到新表中，但是原表格的分区字段没有复制到新表中。如果希望新表格和原表格有相同的数据和表结构（分区属性）.可以用create table… like …命令：    &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;sql&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;table&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sales1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;like&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sales&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/odps-sql/&quot;&gt;odps_sql&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on April 26, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[索引构建]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/index-construction/" />
  <id>http://jwchennlp.github.com/index-construction</id>
  <updated>2014-04-24 06:09:41 UTCT00:00:00-00:00</updated>
  <published>2014-04-24T00:00:00-04:00</published>
  
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;p&gt;索引构建主要是对建立好的词典中的每个词项，构建词项关于文档集合的索引记录表。一般索引构建算法会受硬件设施的制约。    &lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;硬件基础&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;构建信息检索系统时，很多决策都依赖于系统所运行的硬件环境。与信息检索系统相关的硬件基本性能参数如下：      &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;系统访问内存中数据的速度比访问硬盘中数据要快的多，访问内存中的一个字节只需要几个时钟周期（大约5×10-9s），从磁盘传输一个字节的时间则长得多（大概2×10-8s）。因此，为了更快的响应速度，我们应该尽可能将数据放在内存中，特别是那种频繁访问的数据。这种将频繁使用的数据放入内存中的机制称为caching（缓存）。    &lt;/li&gt;
  &lt;li&gt;进行磁盘读写时，磁头移动到数据所在的磁道需要一定的时间，该时间称为寻道时间，对典型的磁盘来说平均在5ms左右。寻道过程中并不进行数据的传输。于是，为了时数据传输率最大，连续读取的数据块也应该在磁盘上连续存放。      &lt;/li&gt;
  &lt;li&gt;操作系统往往以数据块为单位进行读写。因此，从磁盘读取一个字节和一个数据块所耗费的时间可能一样多。我们将内存中保存读写块的那块区域称之为缓冲区（buffer）。       &lt;/li&gt;
  &lt;li&gt;数据从磁盘传输到内存是由系统总线而不是处理器来实现的，这以为着在磁盘I/O时处理器仍然可以处理数据。我们可以利用这一点来加速数据的传输过程，比如将数据压缩后存储在磁盘上。假定采用一种高效的解压缩算法的话，那么从磁盘读取压缩数据再解压缩所花时间往往比直接读取未压缩数据所花时间少。       &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;基于块的排序索引方法&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;第一章建立倒排索引时，所有的处理过程都是在内存中完成的。我们将文档一次性读入内存，而后建立文档的词典，并建立词典中的词项的倒排记录表。如果当文档集过大，大到难以一次性读入内存时，上述方法便失效。   &lt;/p&gt;

&lt;p&gt;由于内存的不足，我们可以采用磁盘的外部排序的方法（external sorting algorithm）。我们知道读取数据过程中的寻道时间与数据传输相比是十分耗时的，所以我们应该尽量将数据按块的方式存储以减少寻道的次数。BSBI（blocked sort-based indexing algorithm，基于块的排序索引算法）是一种解决的方法。算法实现如下：     &lt;br /&gt;
第1步：将文档切分成均匀的若干个部分。   &lt;br /&gt;
第2步：对每个部分的词项ID-文档ID对排序。  &lt;br /&gt;
第3步：将中间产生的临时排序结果存储在磁盘上。   &lt;br /&gt;
第4步：将所有的中间文件合并形成最终结果。   &lt;/p&gt;

&lt;p&gt;在第2步中我们将词项用词项id代替，词项id是能代表词项的唯一标识，这样做能提高索引构建效率。&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;      
           
&lt;span class=&quot;n&quot;&gt;BSBIndex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;construction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;     
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;      
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;documents&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;have&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;been&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;processed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;        
&lt;span class=&quot;n&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;                 
    &lt;span class=&quot;n&quot;&gt;block&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ParseNextBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;       
    &lt;span class=&quot;n&quot;&gt;BSBI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INVERT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;            
    &lt;span class=&quot;n&quot;&gt;WriteBlockToDisk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;      
    &lt;span class=&quot;n&quot;&gt;MergeBlocks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;....&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fmerge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;      
             
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;合并时，同时打开所有块对应的文件，内存中维护10个块的读缓冲区和一个为最终合并索引准备的写缓冲区。每次迭代中，利用优先级队列（即堆结构）或者类似的数据结构选择最小的未处理词项id进行处理。读入该词项的倒排记录表进行合并，合并结果返回磁盘中。需要时，再次从文件中读入数据到每个读缓冲区。  &lt;/p&gt;

&lt;p&gt;由于该算法主要的时间耗费在排序上，因此其时间复杂度为O（T*logT),其中T是要排序的项目数的上界（即词项ID-文档—ID对的个数），然而，实际的索引构建的时间往往取决与文档温习（ParseNextBlock）和最后合并（MergeBlocks）。&lt;/p&gt;

&lt;p&gt;由于我们知道，为了提高索引构建效率，我们将词项映射成词项ID，初始的倒排记录表形式为：     &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;      
    &lt;span class=&quot;n&quot;&gt;wordi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;doc2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.....|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;docn&lt;/span&gt;                    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;在进行词项id的映射之后，每个词项ID-文档ID对就是简单的（wid,did）的形式了。这样做为什么能提高效率呢？虽然映射过程需要话费一定的时间，可是映射之后，每个块得到的都是这样的二值对，这样可以以词项ID为主键，以文档ID为次键按照快速排序一类方法进行排序，这样使得倒排记录的构建变得简单。     &lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;内存式单遍扫描索引构建方法&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;BSBI方法需要将词项映射成词项ID，所以必须在内存中维护一个（词项，词项ID）表的数据结构。当对大规模文档来说，这种数据结构的大小将超过内存大小。
SPIMI（single-pass in-memory indexing,内存式单遍扫描索引构建算法）使用词项而不是词项ID作为词典，它将为个块的词典读入磁盘，对于下一个块则采用新的词典。只要硬盘空间足够大，SPIMI就能够索引任何大小的文档集。     &lt;/p&gt;

&lt;p&gt;SPIMI算法流程如下所示：      &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;c&quot;&gt;&lt;span class=&quot;lineno&quot;&gt; 1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SPIMI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Invert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;      
&lt;span class=&quot;lineno&quot;&gt; 2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NewFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;     
&lt;span class=&quot;lineno&quot;&gt; 3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NewHash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;      
&lt;span class=&quot;lineno&quot;&gt; 4&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;free&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;available&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;        
&lt;span class=&quot;lineno&quot;&gt; 5&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;      
&lt;span class=&quot;lineno&quot;&gt; 6&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;term&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;        
&lt;span class=&quot;lineno&quot;&gt; 7&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;posting_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AddToDictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;term&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;     
&lt;span class=&quot;lineno&quot;&gt; 8&lt;/span&gt;         &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;posting_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GetPostingList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;term&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;      
&lt;span class=&quot;lineno&quot;&gt; 9&lt;/span&gt;         &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;posting_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;       
&lt;span class=&quot;lineno&quot;&gt;10&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;posting_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DoublePostingList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;term&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;       
&lt;span class=&quot;lineno&quot;&gt;11&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;AddToPostingList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;posting_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dicID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;     
&lt;span class=&quot;lineno&quot;&gt;12&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sorted_term&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SortTerms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;        
&lt;span class=&quot;lineno&quot;&gt;13&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WriteBlockToDisk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sorted_term&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;        
&lt;span class=&quot;lineno&quot;&gt;14&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_file&lt;/span&gt;      
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4 id=&quot;bsbispimi&quot;&gt;BSBI和SPIMI的区别&lt;/h4&gt;
&lt;p&gt;BSBI在读入一块内存中的文档内容时，会构建这块文档的词项ID—文档ID对序列，在对序列进行排序后，构建这块文档的倒排索引表，也就是说倒排索引的构建是对读入的整个文件块这个整体。SPIMI当然也是将初始大规模文档划分成等大小的块，并按块读入内存，新建一个初始为空的字典，首先他直接以词项作为词典单位，也就是说在遍历内存中的文档时，对文档进行词条话和词干化后，查看每个词，如果这个词不再字典中，则将词加如词典中，并新建一个关于此词项的倒排记录表，如果词项在字典中存在，则需要在此词项的到拍记录表的基础上进行添加操作。由于实现并不清楚每个词项的倒排记录表的长度，所以初始设定倒排记录表的长度为某个较小的值，当倒排记录表已满时，可以按倍数进行扩展。&lt;/p&gt;

&lt;p&gt;SPIMI的倒排记录表是动态增长的，同时立刻就可以实现全体倒排记录表的收集。这样做有两个好处： &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;由于不需要排序操作，所以处理的速度更快。  &lt;/li&gt;
  &lt;li&gt;由于保留了倒排记录表对词项的归属关系，因此能够节省内存，词项的ID也不需要保存。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SPIMI算法的时间负责度是O（T），因为它不需要对词项ID-文档ID排序，所以操作最多和文档集大小成线性关系。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;分布式索引构建方法&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;实际中，文档集合一般相当大，一台计算机很难实现高效的实现索引构建。尤其是对于万维网来说。因此Web搜索引擎通常使用分布式索引构建（distribuction index）算法来构建索引，其索引结果也是分布式的，它往往按照词项或是文档分割后分布在多台计算机上。   &lt;/p&gt;

&lt;p&gt;这里介绍的分布式索引构建方法是MapReduce的一个应用。MapReduce是一个分布式的计算框架，它面向大规模计算机集群而设计。集群中有一个主控节点（master node）,主要负责任务在工作节点的分配和重分配。重分配是实现分布式框架的鲁棒性，因为集群在工作当中，可能工作节点会出现故障，这个时候主节点应当能识别这些故障并将故障机器的任务重新分配给其它可工作的工作节点。    &lt;/p&gt;

&lt;p&gt;一般来说MapReduce会通过键-值对（Key-Value pair）的转换处理，将一个大型的计算问题转换成较小的子问题。在索引构建中，键-值对就是（词项ID,文档ID）。在分布式索引构建中，词项到词项ID的映射同样要分布式进行，因此分布式的索引构建方法要比单机上的索引构建方法复杂的多。一种简单方法就是维护一张高频词到其ID的映射表，并将它复制到所有节点的计算机上，而对低频词则直接使用词项本身。&lt;/p&gt;

&lt;p&gt;MapReduce的Map过程将输入的数据片映射成键值对，这个映射过程对英语BSBI和SPIMI算法中的分析任务，执行Map过程的机器也称之为分析器（parser）。每个分析器将输出结果保存在本地的中间文件。&lt;/p&gt;

&lt;p&gt;Reduce主要是对中间结果进行合并，形成最终的索引。对每个词项（键值），获取此词项的所有文档集合并构建词项的倒排记录表主要通过倒排器来实现。   &lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;动态索引构建方法&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;上述建立索引的方法都是基于静态文档的，在很多情况下，文档都会随着时间动态变化的。那么，当文档更新速度很慢时，我们可以采用定期更新索引的策略。如果文档更新速度很快时，则实时更新索引的方法将十分耗时。&lt;/p&gt;

&lt;p&gt;可以采用如下方法实现动态索引的构建，这里我们主要维护两个索引，第一个主索引是对初始的文档集构建的索引，第二个辅助索引是在主索引建立之后随着时间推移，而更新的索引，辅助索引存放在内存中，这样实时检索时通过查询主索引和辅助索引实现。如果是对主索引在未来时间的更新，可以通过一个无效位向量实现，用无效位向量来标致文档的删除，同时在辅助索引中加入此文档的更新，便实现了主索引内容的更新。同时随着时间的推移，辅助索引的容量是不断增大的。当辅助索引长度大一某一值时，我们可以将辅助索引并入到主索引中。&lt;/p&gt;

&lt;p&gt;将辅助索引并入主索引的开销主要取决于索引为文件中的存储方式。如果将每个词项对应的倒排记录表存储为一个文件，则此词项的辅助索引和主索引的合并通过简单的将辅助索引扩展到主索引的倒排记录表即可。显示情况是因为文件管理的各种限制，将所有词项的倒排记录表分别存储为文件是不可行的。替代方案是将所有词项的倒排记录表存储为一个大的文件。&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/index-construction/&quot;&gt;索引构建&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on April 24, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[词典及容错式检索]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/dic-and-fault-tolerance-retrieve/" />
  <id>http://jwchennlp.github.com/dic-and-fault-tolerance-retrieve</id>
  <updated>2014-04-22T00:00:00-00:00</updated>
  <published>2014-04-22T00:00:00-04:00</published>
  
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;h2 id=&quot;section&quot;&gt;词典搜索的数据结构&lt;/h2&gt;

&lt;p&gt;在给定倒排索引和查询，首要任务是确定查询中的各个查询词是否在词汇表中，如果在则返回该词所对应的倒排记录表的指针。词汇表的查找操作通常采用一种称之为词典的数据结构，主要有两种解决方案：&lt;strong&gt;哈希表方式&lt;/strong&gt;和&lt;strong&gt;搜索树方式&lt;/strong&gt;。通常在选择选用何种解决方式时，我们需要考虑如下问题：                       &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;关键字的数目    &lt;/li&gt;
  &lt;li&gt;关键字的数目是经常变化还是相对固定，在变化的情况下，是只插入新关键字还是同时要删除某些旧关键字。  &lt;/li&gt;
  &lt;li&gt;不同关键字的相对访问频率如何。   &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于哈希表，词汇表中的每个词通过哈希函数映射成一个数，可以认为这个数代表这个词的存储地址。所以对于query里面的查询词来说，同样通过哈希函数应查看查询词映射到的地址，如果此地址存在数，则表示该查询词存在词典中。采用哈希表方式时，存在以下问题：          &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;哈希函数的空间要足够大，并且不易扩展。哈希函数必须要有足够大的空间来               存储字典，同时它的空间很难实时扩展，如需扩展，需要更改哈希映射函数，使得整个数据结构都发生变化。 &lt;/li&gt;
  &lt;li&gt;冲突问题的解决，因为哈希函数可能使得两个不同的词映射到统一地址，如何减少映射冲突也是一个要考虑的问题。   &lt;/li&gt;
  &lt;li&gt;哈希表方式很难解决前缀式查询，因为在不知道整体词的情况下，哈希映射函数是失效的。  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;搜索树可以很好的解决上述问题，它支持前缀式查询。最出名的搜索树是二叉树，每个内部节点都有两个字节点。在二叉树中搜索词要从根节点开始，每个内部节点代表一个二值测试，测试的结果用于确定下一步应该搜索的子树。二叉树的平衡性是实现高效搜索的关键，，平衡二叉树指的是任何节点的两个子树的高度相差小于等于1.下图为一个二叉树表示的词典的例子。为了实现搜索树的平衡性，我们必须在加入增加或删除节点时对树进行处理以保持树的平衡性，这里用B-树实现。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/binary-tree.png&quot; alt=&quot;image&quot; /&gt;   	&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;通配符查询&lt;/h2&gt;

&lt;p&gt;通配符通常用于以下情形：  &lt;br /&gt;
1. 用户不确定查询查询词的拼写。 &lt;br /&gt;
2. 用户知道某个查询词项可能有不同的拼写版本，并且要把包含这些版本的文档都查找出来。       &lt;br /&gt;
3. 用户查找某个查询词项的所有变形，这些词项还做了词干还原，但是用户并不知道搜索引擎是否做了词干还原。  &lt;br /&gt;
4. 用户不确定一个外来词或者短语的正确拼写形式。   &lt;/p&gt;

&lt;p&gt;当通配符出现在一个查询词的尾部时，如ca&lt;em&gt;，则是需要查找词典中所有词前两个字母是ca的所有词的文档。我们可以通过搜索树来实现这一查找，在搜索树的根（root）节点,首先我们确定首字母为c所指定的分支，而后在以分支作为搜索树查询a所对应的分支，这样这个分支下的所有单词都为符号ca&lt;/em&gt;查询的单词。   &lt;br /&gt;
然后，当通配符出现在词的首部时，如&lt;em&gt;ay,需要查找词典中后两个字母是ay的所有词项，显然用之前的搜索树不能实现这一查询。这里我们可以引入词典的反向B-树结构。前面的词典的B-树的构建是从词项的首字母开始，接着词的第二个字母知道最后一个字母构建B-树。反向B-树恰恰相反，它是从词典的尾字母开始，依次到倒数第二个字母直到第一个字母构建B-树。这样的反向B-树便能匹配通配符出现在词首部的查询。
那么对于通配符出现在查询词中间的查询，如t&lt;/em&gt;o,我们可以采用如下策略，首先用构建的B-树查找t&lt;em&gt;的所有词，而后采用构建的反向B-树查找符合&lt;/em&gt;o的所有词，最后两个查询的词求交集便是所查找的词。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;轮排索引&lt;/h3&gt;
&lt;p&gt;轮排索引是一种用于一般通配符查询的索引，它是倒排索引的一种特殊方式。它的工作原理为，首先引入一终结符$，对词项集合中的每个词在其末尾增加$符号。如词项hello扩展成hello$,随后我们需要按如下方式建立词的轮排索引，对词hello$连续进行首位翻转，将出现的所有形式记录的集合称之为轮排词汇表。hello$的轮排索引如图所示: &lt;br /&gt;
&lt;img src=&quot;../images/permutern-index.png&quot; alt=&quot;image&quot; /&gt;   &lt;br /&gt;
那么如何用轮排索引实现通配符查询呢，由上图我们知道轮排索引中的任何一个状态都指向词项hello，也就是说ello$h或者llo$he的查询过程都会通过轮排索引指向词项hello的查询过程。所以例如查询通配符h&lt;em&gt;llo,处理的关键是将通配符&lt;/em&gt;移动到词的尾部，将h&lt;em&gt;llo转换成h&lt;/em&gt;llo$,接着进行翻转得到llo$h&lt;em&gt;,接着在轮排索引中查找该字符串，我们发现llo$h&lt;/em&gt;与hello词的轮排索引中的llo$he一致，所以hello是满足条件的查询结果。     &lt;br /&gt;
对于查询中存在多个通配符的情况，如查询（fi&lt;em&gt;mo&lt;/em&gt;er）,我们可以按如下方式进行处理，首先查找er$fi*的所有结果，接着可以通过穷举法过滤出包含mo的词，这些词便是符合通配符查询的结果。 &lt;/p&gt;

&lt;h3 id=&quot;k-gram&quot;&gt;支持通配符查询的k-gram索引&lt;/h3&gt;
&lt;p&gt;上面介绍的轮排索引结构简单，但是在构建轮排索引的过程中，我们需要对词进行旋转并记录所有旋转的结果，这会引起存储空间的急剧增加。     &lt;/p&gt;

&lt;p&gt;k-gram索引是如下的倒排索引机制，它将原始词典中的所有词项进行拆分，每个词项都拆分成若干个长度为k的新的词项，并根据这些新的词项构建倒排索引，如happy按照3-gram拆分成的新词词项有$ha,hap,app,ppy,py$,这里用$来对词的开始和结束进行标记。这里倒排索引的构建方式与第一章提到的略微不同，之前的倒排索引是词典是文档中经过词条化和语言话处理的所有词，而倒排记录表是这些词所出现的文档。而这里词典则是文档中的所有词根据k-gram拆分的所有新词，倒排记录表这是包含这些长度为k的新词的原始词。如3-gram的新词etr对应的倒排记录表为,词项为etr，倒排记录表为所有包含etr的词：    &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/k-gram-index.png&quot; alt=&quot;image&quot; /&gt;    &lt;/p&gt;

&lt;p&gt;那么k-gram是如何实现通配符查询的呢，如查询he&lt;em&gt;lo,是要查询首字符为he，尾字符为lo的所有词，根据3-gram索引，我们可以够找如下的布尔查询$heANDlo$,则3-gram的查询词便是所期望的
结果。k-gram索引有时也会导致非预期的结果，如查询red&lt;/em&gt;,根据3-gram索引构建的布尔查询为$reANDred,其返回结果可能包含retired，但显然这个词并不符合初始期望。为了解决这一问题，我们可以引入一个后过滤的步骤，实现方式很简单，用初始的查询词与返回的词进行匹配，那些成功匹配的词便是符合要求的词。&lt;br /&gt;
通配符查找往往是非常耗时的，对于单个通配符查询，我们可能要构建轮排索引或者k-gram索引来返回中间结果，并且对这中间结果要求交集来返回确切的要查找的词，最后才依据这些词通过倒排索引来查找这些词所对应的文档。    &lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;拼写校正&lt;/h2&gt;
&lt;p&gt;拼写校正是在用户输入某个查询词或查询短语时，用户能识别其中词的拼写错误并返回正确词的查询结果。&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;拼写校正的实现&lt;/h3&gt;

&lt;p&gt;对于大多数拼写校正算法而言，有以下两条基本规则：        &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对于一个错误拼写的查询中，则需要在其所有正确的拼写中，返回最近的正确拼写的查询。&lt;/li&gt;
  &lt;li&gt;当两个正确拼写查询临近度相等时，则需要返回更常见的那个正确查询。更常见可以通过以下两个方式衡量，可以统计文档集合中两个查询出现的次数，出现次数高的标记为“更常见”。也可以统计用户查询日志中两个查询的出现次数，出现次数更高的标记为”更常见”。  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-5&quot;&gt;拼写校正的方法&lt;/h3&gt;
&lt;p&gt;词独立校正:不管查询是单个词还是多个词构成的短语，对查询的词的拼写校正是独立进行的，也就是说是上下文独立的，即某个词是否校正与上下文语境没有关联。校正方法主要有编辑距离方法和k-gram重合度方法。 &lt;br /&gt;
上下文敏感校正:则是在校正过程中，会根据上下文信息来决定词的校正。&lt;/p&gt;

&lt;h4 id=&quot;section-6&quot;&gt;编辑距离&lt;/h4&gt;

&lt;p&gt;给定两个字符串S1和S2，两者的编辑距离定义为由S1转换成S2的最小编辑操作数。通常这些编辑操作包括：     &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;将一个字符插入字符串   &lt;/li&gt;
  &lt;li&gt;将一个字符从字符串中删除  &lt;/li&gt;
  &lt;li&gt;将字符串中的一个字符替换成另一个字符    &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可以在O(S1*S2）的时间复杂度下计算S1和S2之间的编辑距离，主要方法是采用动态规划的思想（类似于动态规划中的求最长公共子串问题），其中S1和S2以字符数组方式进行存放。整数矩阵m的行数和列书分表代表两个字符串的长度，算法在运行过程中不断填写矩阵元素。例如，在算法结束时，m[i,j]表示S1的前i个字符和S2的前j个字符的编辑距离。其代码实现如下：&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt; 
       
&lt;span class=&quot;n&quot;&gt;EditDistance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;    
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;—&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;      
&lt;span class=&quot;n&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;       
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;—&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;      
&lt;span class=&quot;n&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;—&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;      
&lt;span class=&quot;n&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;—&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;       
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;       
        &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;       
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;        
        &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;       
    &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;        
&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;加入对于某个拼写错误查询q，我们需要从词典W中找出与q相邻最近的正确词项，最简单的方法是便利W中的所有词项wi，计算wi和q之间的编辑距离，最后返回和q最近编辑距离的词项wi，并返回wi所指向的文档。很显然，这种遍历的方法是十分低效的，我们可以采用如下的启发式优化策略，我们将搜索限定在首字母相同的词典词项上，对于查询q，我们认为错误不出现在首字符上，这样对于词典W，我们只计算与q有相同首字符的词项与q之间的编辑距离。当然在此基础上更复杂的方法是加入轮排索引，对于词错误拼写查询helo，忽略词的终结符$,构建词的轮排索引{helo，ohel，lohe，eloh}，对轮排索引中的每个词，按照上述的启发式规则与词典W中查找最近编辑距离的正确拼写。（个人理解）&lt;/p&gt;

&lt;h4 id=&quot;k-gram-1&quot;&gt;拼写校正中的k-gram索引&lt;/h4&gt;

&lt;p&gt;对与某一个错误拼写查询，我们可以根据之前的构建k-gram索引来实现拼写校正，过程如下：
对于错误的拼写单词，我们可以将此单词拆分成长度为k的多个字符串，并查找这些字符串所对应的倒排索引表，这些倒排索引表分别表示包含这些字符串的拼写正确的单词，这里我们认为，只要一个单词在在写倒排索引表中出现次数超过某一阀值m，则认为这个词是原错误拼写的正确拼写结果。例如错误拼写bord，其2-gram索引拆分成的新词有{$b,bo,or,rd,d$},这里去除词$b和d$,从文档集合中查找bo，or，rd对应的倒排记录表，如下所示：    &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/sc-k-gram-index.png&quot; alt=&quot;image&quot; /&gt; &lt;/p&gt;

&lt;p&gt;随后，我们只要遍历这些倒排记录表，找到那些在倒排记录表中出现次数高的词，便是正确的拼写词。k-gram索引的缺点像boardroom这种不可能是bord的正确拼写形式的词也会被检索出来。所以我们需要计算词汇表中词项与查询q之间的更精确的重合度计算方法。可以采用雅可比系数对先前的线性扫描合并方法进行修正。雅可比系数的计算公式是length(AandB)/length(AorB)，其中A和B分别表示查询q和词汇表词项中的k-gram集合。当扫描到词t时，计算出q和t的雅可比系数，如果系数大于某一阀值，则将词t返回。
采用雅可比系数进行验证的时候，我们需要知道q和t的k-gram索引，首先q的k-gram索引是已知的，那么在验证的过程中我们需要遍历所有q的k-gram索引中出现的词t的k-gram索引，如果穷举词t的k-gram索引是个缓慢的过程。我们可以通过一下方式来进行简化处理，当我们知道词t的长度时，可以认为他的k-gram长度为length(t)-k+1,这样能快速计算出雅可比系数。&lt;/p&gt;

&lt;h4 id=&quot;section-7&quot;&gt;上下文敏感的拼写校正&lt;/h4&gt;

&lt;p&gt;当查询的短语中每个单词都是正确的单词，但是返回的查询结果很少时，我们可以认为单词中存在拼写错误，并对其中的单词查找其正确的拼写结果，并返回修正后的短语的查询结果。当采用这种穷举法对词语中的词进行拼写校正时，工作量大，效率低。这时可以采用启发式的方法通过用户的查询日志来统计最有查询短语拼写校正后最有可能出现的短语。&lt;/p&gt;

&lt;h4 id=&quot;section-8&quot;&gt;参考资料&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://www.google.com.hk/search?q=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;amp;oq=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;amp;aqs=chrome..69i57j69i65j69i61l3j0.3218j0j1&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&quot;&gt;信息检索导论-词典及容错式检索&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;说明：&lt;/strong&gt;文章主要内容和图片来自信息检索导论一书。&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/dic-and-fault-tolerance-retrieve/&quot;&gt;词典及容错式检索&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on April 22, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[词典和倒排记录表的建立]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/ir-dic-inverted-index/" />
  <id>http://jwchennlp.github.com/ir-dic-inverted-index</id>
  <published>2014-04-18T00:00:00-04:00</published>
  <updated>2014-04-18T00:00:00-04:00</updated>
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;p&gt;建立倒排索引的过程可概括为:&lt;br /&gt;
* 收集用于建立索引的文档&lt;br /&gt;
* 词条化&lt;br /&gt;
* 对词条进行处理,得到词项&lt;br /&gt;
* 根据词项和文档建立索引  &lt;/p&gt;

&lt;h4 id=&quot;section&quot;&gt;文档单位的选择&lt;/h4&gt;
&lt;p&gt;在收集索引文档的过程中,会比较直观的理解每篇文档是便是用于建立索引的索引单位.但很多情况下并非如此,如传统的Unix文件系统将某个目录下的文件都放在一个文件中,我们更倾向以对每个邮件建立一个文档索引,其中邮件中存在附件时,我们希望将附件解压缩并将解压缩文件中的每个文件作为文档建立索引.所以,对收集的文档集合我们应该确定用于建立索引的最小单位(文档).
在长文档中,更一般的说法是存在一个&lt;strong&gt;“索引粒度”&lt;/strong&gt;的问题,对一个书库而言,将一本书当作索引单位(文档)效果会很不理想.例如,查询query是”chinese toys”,那么可能返回这样一本书,第一章中出现”chinese”,最后一章中出现”toys”词,但是这本书跟query的相关性应该是很低的.所以,一个比较可取的方法是对书的每一章或每一段看作文档来建立索引,这样的匹配结果会跟query更相关.当然,索引粒度也不是越小越好,比如,如果我们以句子作为索引单位时,可能要查找的query的分布在几个句子中,这样这几个句子形成的段落是比较相关的结果.这种细粒度的索引会使得准确率升高而召回率降低,索引粒度太效果相反.所以为了权衡召回率和准确率,我们应该设选择合理的索引粒度.&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;词条化&lt;/h4&gt;
&lt;p&gt;词条化主要是将文档中的字符序列才分成一系列子序列的过程,其中每个子序列称之为(token).在这个过程中会进行一系列特殊处理,如删除标点符号等. &lt;br /&gt;
&lt;code&gt;
      输入: I hava a dream,become a good programer!       
      输出: I have a dream become a good programer
&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;词条指的是文档集合中出现的字符序列的实例,词条类是指相同词条构成的集合.  &lt;/li&gt;
  &lt;li&gt;词项指的是在信息检索系统词典中所包含的某个经过归一化处理的词条类&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;词项集合和词条集合可以完全不同,例如词条集合是篮球,足球一类的词汇,此类词条集合的词项可以表示为体育.在实际的信息检索系统中,词项往往和词条密切相关.但是,词项未必就是原始的词条,实际上它往往需要对词条进行归一化处理来得到.&lt;/p&gt;

&lt;p&gt;词条化的主要任务是确定哪些才是正确的词条.在英文文档中,大多是简单的按空格将字符序列进行划分生成词条.可是在有些情况处理会变得复杂,如应为中的上撇号”’“,它可以用来代表所有关系,也有用来代表缩写.如:&lt;br /&gt;
&lt;code&gt;
		Mr.O'Neill thinks that the boy's stories about the Chile's capital aren't amusing.  
&lt;/code&gt;
对于O’Neill来说,词条化结果可以是:{neil},{oneill},{o’neill},{o’,neil},{o,neil}.   &lt;br /&gt;
这里就需要采用词条化工具来对这类情况进行字符序列的词条化,应该注意的是,&lt;strong&gt;对query的词条化处理和对文档集合的词条化处理应该采用相同的机制&lt;/strong&gt;.  &lt;/p&gt;

&lt;p&gt;在进行词条化的时候要考虑”C++”,”C#”等一类的特定领域词,”New York”,”Los Angle”名称词,新出现的词或连字符连接的词.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;停用词:&lt;/strong&gt;指的是在文档中出现频率高,但是与文档主题关系不大的词.在某些情况下,停用词在文档和用户query进行匹配时价值不大,可以用词汇表中去除.&lt;/p&gt;

&lt;h4 id=&quot;section-2&quot;&gt;词条归一化&lt;/h4&gt;
&lt;p&gt;词条归一化(token normalization)是指将看起来不完全一致,但表述意思相近的多个词条归纳成一个等价类,以便在它们之间进行匹配的过程.主要有以下两种方法:  &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;隐式的建立等价类:例如,采用去掉链接字符的映射规则来建立等价类,将moto-car和motocar映射成词项motocar,这样,对任何一个词进行查询,都会返回包含另一个词的文档.  &lt;/li&gt;
  &lt;li&gt;维护多个非归一化词条之间的关联关系,可以进一步扩展为构建同义词词表的手工构建.比如将car和automobile归为同义词.有以下两种方法: &lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;采用非归一化的词条建立索引,并为某个查询词项维护一个有多个词组成的查询扩展词表.当输入一个查询词项时,则根据扩展词表进行扩展并将扩展后得到的所有词的倒排记录表进行合并.   &lt;/li&gt;
      &lt;li&gt;在索引构建时就对词进行扩展.比如对于包含automobile的文档(文档中不包括词car),正常只建立automobile的索引,此时需要同时建立
  automobilt和car的索引.第一种方法需要维护一个词的扩展词表,在查询时需要访问该词表,更耗时.第二个则实在建立索引时便已经构建扩展词表的索引,更好空间.   &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-3&quot;&gt;词干还原和词形归并&lt;/h4&gt;
&lt;p&gt;词干还原和词形归并是为了减少词的曲折变化的形式，并且有时候会将派生词转化成基本形式。因为在语法中，有些词在不同的语境中包含不同的形态，如origanize，oraginizes，originizing。同时语言中也存在大量的同源词，如democracy，democratic，democratization。那么，在检索过程同，如果根据搜索词返回其同源词的文档，能返回更多相关的结果。	
&lt;strong&gt;词干还原&lt;/strong&gt;通常是用启发式的规则对词两端的词缀进行粗略进行处理的过程。&lt;strong&gt;词形归并&lt;/strong&gt;通常是利用词汇表和词形分析来去除屈折词缀，从而返回词的原型或词典中的词的过程，返回的结果称为词元。两者的&lt;em&gt;区别&lt;/em&gt;还在于：词干还原一般情况下会将多个派生相关词合并在一起，词形归并通常只将同一词元的不同曲折形式进行合并。	&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;基于跳表的倒排记录表快速合并算法&lt;/h3&gt;
&lt;p&gt;我们知道，初始的最两个排序好的倒排记录表进行合并（求交集）时，只要维护倒排记录表的指针，并遍历两个表，时间复杂度为O（m+n），（m，n为表的长度）。
跳表的主要思想时，我们在表中加入一些位置加入一些跳表指针，这样在比较的过程中可以考虑是否从当前的条表指针直接跳到下一个跳表指针，忽略中间的那些倒排记录。这里需要考虑的问题是条表步长的问题（相邻条表指针间元素个数），步长短，则所需的存储空间大，能通过跳转指针进行跳转的机会变大。步长长，所需存储空间小，在遍历过程中进行跳转的几率小。主要以空间换取时间来提高合并效率。		&lt;/p&gt;

&lt;p&gt;索引相对固定时，建立有效的跳表比较固定。但是如果倒排记录表由于经常更新而发生变化，那么条表指针的建立比较困难。		&lt;/p&gt;

&lt;h4 id=&quot;section-5&quot;&gt;处理短语查询&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;二元词索引&lt;/strong&gt;是将检索短语且分成相邻两个词的元组，并对这些元组当做词项进行检索，将所有词项检索所的结果通过布尔查询的与操作获取结果。如：		
&lt;code&gt;
	短语：$w_1$,$w_2$,...,$w_m$	
	二元词词项：$w_1$$w_2$,$w_2$$W_3$,...,$w_m-1$$w_m$	
&lt;/code&gt;
其实，在检索过程中，用名词或名词短语来表示用户查询的概念具有相当特殊的地位。但是相关的名词往往被各种虚词隔开，所以在处理短语查询时，可以先对短语进行词条化，之后再进行词性标注，这样在对里面的名词按照二元词索引的思想来查找结果。	&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;位置信息索引&lt;/strong&gt;,采用位置信息索引时，需要建立每个文档中的每个词在文档中出现的位置的索引。词wi在dj中的倒排索引可以表述为：		
&lt;code&gt;
	文档dj：（位置1,位置2,....）
&lt;/code&gt;
那么利用位置信息索引如何实现短语查询呢？对于相邻的两个词，我们首先计算出他们的倒排记录表并进行合并，对合并的结果进行如下处理，对结果中的每一个文档，我们需要查找两个词在文档中的位置索引表，如果两个词各自的位置索引表中存在两个词的位置也是相邻的并且次序准确的话（计算词之间的偏移距离），这这个文档符合要求。如此往复便能得到结果。		
采用位置索引会增加存储空间，并且会使倒排记录表的合并复杂性增加。	&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;混合索引机制&lt;/strong&gt;，对有些查询使用二元词索引，而对其它短语查询使用位置信息索引。二元词索引中的短语可以根据用户日志统计得出。处理开销最大的短语往往是这样的短语，短语中的每个词在文档中十分常见，但是组合起来却很少见。	&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参考资料&lt;/strong&gt;		
&lt;a href=&quot;https://www.google.com.hk/search?q=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;amp;oq=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;amp;aqs=chrome..69i57j69i65j69i61l3j0.3218j0j1&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&quot;&gt;信息检索-词项词典及倒排索引表&lt;/a&gt;		&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/ir-dic-inverted-index/&quot;&gt;词典和倒排记录表的建立&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on April 18, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[IR 倒排索引和布尔查询]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/ir-inverted-index/" />
  <id>http://jwchennlp.github.com/ir-inverted-index</id>
  <updated>2014-04-16T00:00:00-00:00</updated>
  <published>2014-04-16T00:00:00-04:00</published>
  
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;p&gt;&lt;strong&gt;信息检索&lt;/strong&gt;的定义是从大规模非结构化数据(通常为文本)的集合中找出满足用户需求的资料(通常是文档)的过程.
检索的过程可理解为用户输入query,从文档集合中找出与query相关的文档并展示.最简单的检索方式是线性查询,根据query遍历所有的文档集合,	查找出那些文档集合与query是相关的.这种方式存在以下问题:                                                                  &lt;br /&gt;
  1. 文档规模很大时,对所有文档的遍历查找是个费时的过程&lt;br /&gt;
  2. 不能满足灵活匹配方式的要求(如两个词在同一句话中出现).&lt;br /&gt;
  3. 无法对结果进行排序.检索结果应该按照相关性等需求返回最佳答案.  &lt;/p&gt;

&lt;p&gt;可以采用如下方法替代线性扫描方式,对于所有的文档集合,构建一个词项-文档的矩阵,其中词项表示文档集合中的所有词列表,
文档表示所有集合中的文档,矩阵中的元素M(i,j)则表示词i是否在文档j中出现(M(i,j)为1时表示出现).矩阵的每一行表示词在文档集合中的出现情况,矩阵的没一列表示相应的文档中的词的集合.这样当检索query的时候,只需要根据query中的词在矩阵中查找词在文档中的出现情况,最后进行结果合并便可的出结果.&lt;/p&gt;

&lt;p&gt;上述方法相对现行扫描在时间效率上有了很大提升.如果文档集合有1000万,文档集合的词集合有50万个,则需要维护一个50万*1000万的矩阵,		真实情况是这个矩阵会非常洗漱,因为一篇文档只包含50万词表中的少数词,同时一个词只在少数的文档中出现.而我们关心的只是M(i,j)不为0的元素.采用只存储词项-文档矩阵中元素不为0的数据结构效果会更好.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;倒排索引&lt;/strong&gt;是一种索引方法,被用来存储在全文搜索下某个单词在一个文档或一组文档中的存储位置的映射.
假设有三个检索文本:   &lt;br /&gt;
* $T_0$=”it is what it is”&lt;br /&gt;
* $T_1$=”what is it”    &lt;br /&gt;
* $T_2$=”it is a banana” &lt;br /&gt;
根据以上文档当初的到拍索引为:   &lt;/p&gt;

&lt;p&gt;```
 “a”:      {2}    &lt;/p&gt;

&lt;p&gt;“banana”: {2}   &lt;/p&gt;

&lt;p&gt;“is”:     {0, 1, 2}    &lt;/p&gt;

&lt;p&gt;“it”:     {0, 1, 2}  &lt;br /&gt;
```   &lt;/p&gt;

&lt;p&gt;倒排索引的建立规则如下,首先要建立词典,词典中的词为文档中词的集合,对于词典中的每个词都有一个记录该词在文档中的出现列表,这个表成为倒排记录.倒排索引的词典部分存放在内存中,而每个词指向的倒排记录表存放在词典中.&lt;/p&gt;

&lt;p&gt;**用倒排索引和基本布尔检索模型来处理一个查询 **
一般的检索的query由若干个词组成,可以用基本的布尔查询(与,或,非)在连接这些词.
对于查询 word1 And word2
其检索过程如下:
  * 在词典中定位world1
  * 返回词word1的倒排记录表
  * 在词典中定位word2
  * 返回词word2的倒排记录表
  * 对倒排记录表求交集,所对应的文档既为结果&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;交集处理:&lt;/strong&gt;对于倒排索引表求交集的过程,可以通过以下方法进行优化&lt;br /&gt;
  * 构建倒排索引时,倒排记录表是有序构建的,如对文档进行编号,,倒排记录表按照文档标号从小到达顺序生成&lt;br /&gt;
  * 在求交集的过程中,对两个倒排记录表分别维护一个指针,当指针所指元素相同时,保留元素并同时指针后移,当指针所指元素不同时,将元素较小指针向后移动,最后保留的元素既为交集,时间复杂度为O($N_1$+$N_2$),N为两个索引记录表的长度.&lt;br /&gt;
  * 对于多个倒排索引表,可以按照倒排记录表的长度从小到大进行处理,优先对两个最短的倒排记录表求交集.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;布尔检索优缺点&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;优点:&lt;/strong&gt;
  1.布尔检索表达上精确,文档要么满足,要么不满足,可以让用户对返回结果拥有更好的控制力和透明度.
  2.对某些领域信息,布尔检索内部也可以提供排序机制
&lt;strong&gt;缺点:&lt;/strong&gt;
  1.布尔检索不能满足灵活匹配方式的要求.&lt;br /&gt;
  2.采用AND操作符产生的结果正确率高而召回率低,采用OR操作符召回率高而正确率低.&lt;br /&gt;
  3.只记录词项存在文档中存在或不存在,但是我们往往需要累加各种证据来得到文档相关性的可信度.&lt;br /&gt;
  4.布尔模型返回的是一个无序的结果,但是我们往往需要对返回的结果进行排序.  &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参考资料&lt;/strong&gt;
&lt;a href=&quot;http://zh.wikipedia.org/wiki/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95#.E4.BE.8B.E5.AD.90&quot;&gt;倒排索引-维基百科&lt;/a&gt;
&lt;a href=&quot;https://www.google.com.hk/search?q=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;amp;oq=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;amp;aqs=chrome..69i57j69i65j69i61l3j0.3218j0j1&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&quot;&gt;信息检索导论-布尔检索&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/ir-inverted-index/&quot;&gt;IR 倒排索引和布尔查询&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on April 16, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Sample Post]]></title>
 <link rel="alternate" type="text/html" href="http://jwchennlp.github.com/hello-world/" />
  <id>http://jwchennlp.github.com/hello-world</id>
  <updated>2014-04-13T00:00:00-00:00</updated>
  <published>2014-04-13T00:00:00-04:00</published>
  
  <author>
    <name>jwchen</name>
    <uri>http://jwchennlp.github.com</uri>
    <email>hit1093710417@email.com</email>
  </author>
  <content type="html">&lt;p&gt;你好，哈尔滨！&lt;/p&gt;

&lt;h2 id=&quot;why&quot;&gt;why&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;怎么感觉有些问题呢    &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;c&quot;&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  

&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


  &lt;p&gt;&lt;a href=&quot;http://jwchennlp.github.com/hello-world/&quot;&gt;Sample Post&lt;/a&gt; was originally published by jwchen at &lt;a href=&quot;http://jwchennlp.github.com&quot;&gt;My blog&lt;/a&gt; on April 13, 2014.&lt;/p&gt;</content>
</entry>

</feed>
