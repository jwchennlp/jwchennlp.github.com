<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Latest Posts &#8211; My blog</title>
<meta name="description" content="Describe this nonsense.">
<meta name="keywords" content="Jekyll, theme, themes, responsive, blog, modern">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Latest Posts">
<meta property="og:description" content="Describe this nonsense.">
<meta property="og:url" content="http://jwchennlp.github.com/index.html">
<meta property="og:site_name" content="My blog">





<link rel="canonical" href="http://jwchennlp.github.com/">
<link href="http://jwchennlp.github.com/feed.xml" type="application/atom+xml" rel="alternate" title="My blog Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://jwchennlp.github.com/assets/css/main.min.css">
<!-- Webfonts -->
<link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://jwchennlp.github.com/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://jwchennlp.github.com/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://jwchennlp.github.com/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://jwchennlp.github.com/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://jwchennlp.github.com/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://jwchennlp.github.com/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://jwchennlp.github.com/images/apple-touch-icon-144x144-precomposed.png">





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
                  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                          });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
<!--
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
-->

</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://jwchennlp.github.com">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://jwchennlp.github.com/images/avatar.jpg" alt="jwchen photo" class="author-photo">
					<h4>jwchen</h4>
					<p>不积跬步，无以至千里;不积小流，无以成江海</p>
				</li>
				<li><a href="http://jwchennlp.github.com/about/">Learn More</a></li>
				<li>
					<a href="mailto:hit1093710417@email.com"><i class="icon-envelope"></i> Email</a>
				</li>
				
				
				
				
				<li>
					<a href="http://github.com/jwchennlp"><i class="icon-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://jwchennlp.github.com/posts/">All Posts</a></li>
				<li><a href="http://jwchennlp.github.com/tags/">All Tags</a></li>
			</ul>
		</li>
		<li><a href="http://jwchennlp.github.com/theme-setup">Theme Setup</a></li><li><a href="http://mademistakes.com">External Link</a></li>
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  <div class="image-credit">Image source: <a href="http://www.dargadgetz.com/ios-7-abstract-wallpaper-pack-for-iphone-5-and-ipod-touch-retina/">dargadgetz</a></div><!-- /.image-credit -->
  
    <div class="entry-image">
      <img src="http://jwchennlp.github.com/images/abstract-1.jpg" alt="Latest Posts">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>My blog</h1>
      <h2>Latest Posts</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-06-04T00:00:00-04:00"><a href="http://jwchennlp.github.com/hmm-hidden-markov-model/">June 04, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/hmm-hidden-markov-model/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/hmm-hidden-markov-model/" rel="bookmark" title="HMM(Hidden Markov Model)" itemprop="url">HMM(Hidden Markov Model)</a></h1>
    
  </header>
  <div class="entry-content">
    <h2 id="section">1.介绍</h2>
<hr />

<p>隐马尔可夫模型(Hidden Markov Model,HMM)是一种统计模型,用它来描述一个含有未知参数的马尔可夫过程.在模型中包括可观察的参数和隐藏的参数,一般要求解的问题是通过观察到的参数来评估过程中的隐含参数.隐马尔可夫模型在语音识别,中文分词,词性标注,机器翻译等领域都有十分广泛的运用.		</p>

<h3 id="section-1">模型表示</h3>

<p>隐马尔可夫模型(HMM)可以用如下的五元组来表示,包括2状态集合和概率矩阵:</p>

<p>(1).隐含状态S={$s_1,s_2,…,s_n$}			
隐含状态是马尔可夫模型中所隐含的状态.这些状态无法通过直接观测而得到.		
(2).可观测状态O={$o_1,o_2,…,o_m$}		
可观察状态是马尔可夫模型中可以观测到的状态.(可观察状态数目和隐含状态数目可以不相等).		
(3).初始状态概率矩阵$\pi$		
表示隐含状态在初始时刻t=1时的概率矩阵.初始状态概率矩阵为$\pi=[p(s_1),p(s_2),…,p(s_n)]$			
(4).隐含状态的概率转移矩阵$A<em>{ij}$		
表示在HMM模型在各个隐含状态之间的转移概率.其中$A</em>{ij}=p(s_j｜s_i)$表示在t时刻隐含状态为$s_i时,t+1时刻隐含状态为s_j$的概率.		
(5).隐含状态到观察状态的概率转移矩阵$B<em>{ij}$		
$B</em>{ij}=p(o_j｜s_i)表示在隐含状态s_i的情况下,观察状态为o_j的概率$.		</p>

<h3 id="section-2">实例</h3>

<p>考虑这样一个例子,有一个隐士,他不知道自己所在地的天气情况(天晴,下雨,多云,对应于隐含状态S).但是他养了一株水藻,他可以观察到水藻的状态(干燥,湿润,湿透,对应于观察状态O).并且对每一个天气,水藻呈现不同状态的概率已知,例如天晴时水藻干燥,湿润和湿透的概率(对应于隐状态转移矩阵A)分别为0.4,0.3,0.3.同时你知道,今天的天气情况下明天的天气情况的概率(如今天天晴时,明天天晴,下雨,多云的概率分别为0.5,0.1,0.对应于观察状态在应状态下的概率转移矩阵B).并且,我们假定从以某一天为开始,这一天的天气情况概率分布(对应于初始概率$\pi$)我们也知道.		</p>

<p>假设隐士观察了水藻一个礼拜,这一个礼拜水藻的状为(干燥,干燥,湿润,湿润,干燥,湿透,干燥).现在问题来了,你能不能计算出这一系列状态出现的概率?能不能计算出隐士所在地方这一个礼拜最可能天气情况?	</p>

<p>带着这些问题你将更好的理解HMM模型的应用.</p>

<h3 id="section-3">基本问题</h3>

<p>HMM可用于解决如下三个问题:</p>

<p>1.评估问题(evaluate) 		</p>

<p>给定观察序列$O=o_1o_2…o_t和模型参数\lambda=(S,O,A,B,\pi)$,计算在HMM模型下观察序列出现的概率.运用前向算法(forwad algorithm)进行求解.	</p>

<p>2.解码问题(decode)</p>

<p>给定观察序列$O=o_1o_2…o_t和模型参数\lambda=(S,O,A,B,\pi)$,根据观察序列计算对应的最有可能的隐状态序列.在实际问题中,我们往往更关心的是马尔可夫模型中的隐含状态,可以运用viterbi算法进行求解.		</p>

<p>3.学习问题 		</p>

<p>HMM模型的参数$\lambda=(A,B,\pi)$未知,如何调整这些参数以使得观察序列O的概率尽可能的大,通常采用Baum-Welch算法解决.	</p>

<p>下面我们详细讨论这3个问题.	</p>

<h2 id="evaluate">2.评估问题(evaluate)</h2>
<hr />

<p>假设模型参数$\lambda=(A,B,\pi)$已知,考虑上面提到的例子,假设3天间我们观察到的水藻状态为(干燥,湿润,湿透),这三天中的任一天都可能是多云,晴天或是雨天,对于观察序列及隐藏序列,我们可以用如下的网格来表示:			</p>

<p><img src="../images/1406/hmm1.png" alt="image" />			</p>

<p>网格中的每一列表示可能的天气状态,并且每一天中的每个天气都与一个到相近天气状态相连,表示为在当前天气状态下到下一个天气状态的概率.每一列的下面是某个时间点上的观察状态.我们发现,对于如上的描述,这三天可能出现的天气序列有$3^3$=27种,这27种天气情况转化为观察序列的概率和便是观察序列在模型下出现的概率,表示为:</p>

<script type="math/tex; mode=display">P(O｜HMM) = p(O｜sunny,sunny,sunny)+p(O｜sunny,sunny,rainy) \\
+...+P(O｜cloudy,cloudy,cloudy)</script>

<p>$O=(dry,damp,soggy)$,即为观察到的状态序列.这种基于穷举的方法求解效率很低,假设观察序列长度为T,隐含状态数目为N,所有可能的状态序列为$N^T$,没一个状态序列的时间复杂度为$O(T)$,所以总时间复杂度为$O(TN^T)$.		</p>

<p>下面采用前向算法(foward algorithm)进行优化求解.		</p>

<h3 id="section-4">局部概率</h3>
<p>首先定义局部概率(partial probability),它是指到达某个中间状态的概率.</p>

<p>对于观察序列$O=o_1o_2…o_n,o_i可以理解为t=i时刻观察到的观察状态$,关于观察状态和隐含状态用网格表示为:	</p>

<p><img src="../images/1406/hmm2.png" alt="image" />		</p>

<p>那么t=2时,隐状态为”cloudy”的局部概率可以表示为:		</p>

<p><img src="../images/1406/hmm3.png" alt="image" />	</p>

<p>公式表述为$\alpha_{t=2}(c)=p(damp｜c)*(p(c｜s)+p(c｜c)+p(c｜r))$,其中c,r,s分别是cloudy,rainy和sunny的简称.		</p>

<p>那么延伸一下,假设有N个隐含状态,这t+1时刻处于隐含状态j的隐含概率可以表述为:		</p>

<p><img src="../images/1406/hmm4.png" alt="image" />	</p>

<script type="math/tex; mode=display">\alpha_{t+1}(j) = b(o_{t+1}｜j)\sum_{i=1}^N\alpha_t(i)a_{ij}</script>

<p>其中<script type="math/tex">b(o_{t+1}｜j)</script>表示的是在t+1时刻,观察到的状态<script type="math/tex">o_{t+1}</script>在隐含状态j下的概率,<script type="math/tex">a_{ij}</script>表示隐含状态i到j的转移概率 .</p>

<p>在t=1时,没有路径指向当前时间的隐含状态,t=1时的局部概率定义为:		</p>

<script type="math/tex; mode=display">\alpha_{1}(j)=b(o_1｜j)\pi(j)</script>

<p>在我们求出t=1时个隐含状态的局部概率之后,就可以递归的计算t=2,t=3,…时各个隐含状态的局部概率,直到求得t=T(T为观察序列的长度)时为止.</p>

<p>在t=1时,求每个隐含状态的局部概率时间复杂度为O(1),在t&gt;1时,球每个隐含状态的时间复杂度为O(N),则t&gt;1时求每一列的时间复杂度为O(N*N),T为观察序列的长度,则前向算法的时间复杂度为O(TN^2).	</p>

<h2 id="decode">解码问题(decode)</h2>
<hr />

<p>假设模型参数$\lambda=(A,B,\pi)$已知,考虑上面提到的例子,假设3天间我们观察到的水藻状态为(干燥,湿润,湿透),需要求解与观察状态对应的最有可能的隐含状态序列.对与上面描述的天气的例子,其网格图为:	</p>

<p><img src="../images/1406/hmm1.png" alt="image" />		</p>

<p>可以知道,最有可能的天气(隐含)序列是每一列中天气组合(总共27种)中的一项,所以最有可能的隐含序列$\hat S$可以表述为:	</p>

<script type="math/tex; mode=display">\hat S=argmax_{s\in S}p(O｜s)</script>

<p><script type="math/tex">其中观察O=(dry,damp,soggy),s为27种隐含序列中的一种</script>		
同样,我们可以通过穷举法计算每一种可能出现的隐状态序列的概率,概率值最高的便是要求的隐含状态序列.这显然又是十分耗时的工程,我们可以采用viterbi算法优化求解.		</p>

<h3 id="section-5">局部概率和局部最佳路径</h3>

<p>对于上面的程序,先假设在t=3时,天气状态为cloudy时概率最大,那么从t=2到t=3的cloudy状态有3条路径,最佳路径必定是这三条路径中的一条.假设t=2时的从状态rainy到t=3的cloudy的概率最大,则可以说明t=2时的cloudy路径为局部最佳路径,路径对应的概率称为局部概率($\delta$).</p>

<p><img src="../images/1406/hmm5.png" alt="image" /></p>

<center><img src="../images/1406/hmm6.png" /></center>

<p>延伸一下,假设有N+1个隐含状态,在t+1时刻的位于隐含状态j的局部概率为:	</p>

<p><img src="../images/1406/hmm6.png" alt="image" />	</p>

<script type="math/tex; mode=display">\delta_t(j) = max_{i \in N}(\delta_{t-1}(i)a_{ij}b_{jo_t})</script>

<p>其中$b_{jo_t}指的是在t时刻的观察值o_t在隐含状态j下的概率$,在t=1时,没有路径指向当前的隐含状态,其局部概率为:		</p>

<script type="math/tex; mode=display">\delta_1{j} = b_{jo_1}\pi(j)</script>

<p>这样通过计算t=1时各个隐含状态的局部概率,就可以计算t=2,…t=T时各状态的局部概率.其实,在t=T时,计算出来概率$\delta_t(j)$便是最后与观察序列对应的且最后一个隐含状态为j的最终概率,<script type="math/tex">j=max_{j\in N} \delta_t(j)</script>可以求出最后一个观察状态对应的隐含状态.那么如何求之前T-1个观察状态对应的隐含状态?这里我们需要借用反向指针来实现.		</p>

<p>我们知道计算t时刻隐含状态j的局部概率$\delta<em>{t}(j)$,需要的知道t-1时刻的$的\delta</em>{t-1}$,我们需要记录t-1时刻的某个状态i,有i状态到达t时刻的状态j是到达状态j的最佳路径.通过反向指针来记录这种状态:		</p>

<script type="math/tex; mode=display">\phi_t(j) = argmax_{i\in N}(\delta_{t-1}(i)aij)</script>

<p>其中,<script type="math/tex">\phi_t(j)表示指向t时刻状态j的隐含状态,具体所指应该是t-1时刻的某个状态i</script>.		</p>

<p>其实初看这个公式的有些奇怪,我们在计算局部概率$\delta$的时候,同时考虑了局部概率$\delta和转移概率a及混淆概率(从隐含概率到观察概率,b)$.这是因为,我们希望$\phi$可以能回答这个问题”如果我们在这里,最可能通过哪条路径到达下一个状态?”,这个问题与隐藏有关,而与观察状态无关,故而可以忽略混淆矩阵的影响.		</p>

<h3 id="section-6">小结</h3>

<p>viterbi算法提供了一种有效的计算方法来分析HMM模型的观察序列,并捕获最有可能的隐藏序列.它利用递归减少计算两,并使用整个序列的上下文来做判断,从而对包含”噪声”的序列也能进行良好的分析.		</p>

<p>在使用时,viterbi算法对于网格中的没一个隐状态都计算一个局部概率,同时包含一个反向指针来指示最有可能的到达该单元的路径.当完成整个计算之后,首先在终止时刻找到最可能的隐含状态,然后通过反响指针回溯到t=1时刻,这样回溯路径上的状态序列就是最可能的隐含状态序列.		</p>

<h3 id="section-7">参考内容</h3>

<p><a href="www.52nlp.cn/category/hidden-markov-model">我爱自然语言处理-隐马尔可夫模型</a></p>

<h4 id="section-8">维基百科-隐马尔可夫模型</h4>

<p><a href="https://github.com/jwchennlp/HMM">HMM实现源码(python) github.com/jwchennlp/HMM</a></p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-06-02T00:00:00-04:00"><a href="http://jwchennlp.github.com/plsa/">June 02, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/plsa/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/plsa/" rel="bookmark" title="pLSA" itemprop="url">pLSA</a></h1>
    
  </header>
  <div class="entry-content">
    <p>在对LSA的介绍中,我们知道LSA的核心思想是将建立的文档-词项矩阵运用SVD将高维空间映射到到隐语义空间,这样可以较好的解决同义词的问题.但语义的权重不好解释.</p>

<h2 id="aspect-model">1.层面模型(aspect model)</h2>
<hr />

<p>pLSA是以层面模型进行建模,层面模型是一个统计模型.它是关联于潜在类别Z的共现数据(co-occurence)的潜在变量模型.关于D*W(文档-词项)的联合概率定义如下:		</p>

<script type="math/tex; mode=display">P(d,w) = p(d)p(w｜d),p(w｜d)=\sum_{z\in Z}p(w｜z)p(z｜d)</script>

<p>当然我们也可以对P(w,d)做如下变化:		</p>

<script type="math/tex; mode=display">p(d,w) =\sum_{z\in Z}p(z)p(w｜z)p(d｜z)</script>

<p>用概率图表示为:		</p>

<p><img src="../images/140530/plsa-1.png" align="center" /></p>

<p>对于图(a),d代表文档,Z代表隐变量(文档主题),W为观察到的单词.$p(d_i)表示单词出现在文档d_i的概率$,$p(z_k｜d_i)表示在文档d_i中$,		
出现$主题z_k下的单词的序列(可以理解为主题z_k也是有一系列表现此主题的单词构成)$,$P(w_j｜z_k)表示在主题z_k下出现单词w_j的$			
概率.并且每个主题上的所有词服从多项式分布,每个文档上的所有主题服从多项式分布.整个文档的生成过程为:		</p>

<ul>
  <li>以$p(d_i)的概率选中文档d_i$		</li>
  <li>以$p(z_k｜d_i)的概率选中主题z_k$		</li>
  <li>以$p(w_j｜z_k)的概率选中词w_j$		</li>
</ul>

<p>其中$(d_i,w_j)是可以可观测值,z_k是隐变量,我们的工作便是估计P(w_j｜z_k)和p(z_k｜d_i)的参数$.</p>

<p>假设$\theta_i表示所有主题在文档d_i的一个多项式分布,则\theta_i可以表示成一个向量,每个元素\theta_{ik}$表示主题k在出现在文档i的概率,即:		</p>

<script type="math/tex; mode=display">p(z_k｜d_i) = \theta_{ik}, \sum_{z_k\in Z}\theta_{ik}=1</script>

<p>假设$\phi_k表示所有词在主题z_k上的一个多项式分布,则\phi_k可以表示成一个向量,每个元素\phi_{kj}$表示单词j出现在主题k的概率,即:		</p>

<script type="math/tex; mode=display">p(w_j｜z_k) = \phi_{kj},\sum_{w_j\in W}\phi_{kj} = 1</script>

<p>所以参数评估可以形式化表现为评估参数$\Theta,\Phi$:		</p>

<script type="math/tex; mode=display">\Theta=[\theta_1,\theta_2,...,\theta_N],d_i\in D</script>

<script type="math/tex; mode=display">\Phi = [\phi_1,\phi_2,...,\phi_k],z_k\in Z</script>

<p>由于词与词之间是相互独立的,且文档与文档之间也是相互独立的.所以我们可以得到整个语料库的词的分布:		</p>

<script type="math/tex; mode=display">p(W｜d_i) =\prod_{j=1}^Mp(d_i,w_j)^{n(d_i,w_j)}</script>

<script type="math/tex; mode=display">p(W｜D) = \prod_{i=1}^N\prod_{j=1}^Mp(d_i,w_j)^{n(d_i,w_j)}</script>

<p>其中,$n(d_i,w_j)表示在文档i中词j出现的次数$,当我们采用极大使然估计来实现参数评估时:		</p>

<script type="math/tex; mode=display">l(\Theta,\Phi)=\sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)\log{p(d_i,w_j)} \\
					  =\sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)\log{p(d_i)\sum_{z_k\in Z}p(w_j｜z_k)p(z_k｜d_i)} \\
					  =\sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)(\log{p(d_i)+\sum_{z_k\in Z}\theta_{ik}\phi_{kj}})</script>

<p>显然,对于含有隐变量的极大使然估计,因为我们不知道隐变量的分布,所以极大似然估计方法得不到参数解.这里我们可以采用EM算法进行参数评估.		</p>

<h2 id="em">2.EM参数评估</h2>
<hr />

<p>EM算法可以用于含隐变量的参数评估,它是一种近似求解方法,主要是通过迭代的方法来获取近似最优解.每次迭代包含E步和M步,E步是要建立极大似然函数的下界,求得隐变量的后验分布.M步则是根据隐变量的后验分布来优化要估计的参数.</p>

<h3 id="e">E步</h3>

<p>在pLSA的参数估计中,可见变量是d和w,隐含变量是主题z,所以隐含变量关于d和w的后验概率为:		</p>

<script type="math/tex; mode=display">p(z_k｜d_i,w_j) = \frac{p(z_k,d_i,w_j)}{\sum_{z_k\in Z}p(d_i,w_j,z_k)} \\
				 =\frac{p(d_i)p(z_k｜d_i)p(w_j｜z_k,d_i)}{\sum_{z_k\in Z}p(d_i)p(z_k｜d_i)p(w_j｜z_k,d_i)} \\
				 =\frac{p(z_k｜d_i)p(w_j｜z_k)}{\sum_{z_k\in Z}p(z_k｜d_i)p(w_j｜z_k)} \\
				 =\frac{\theta_{ik}\phi_{kj}}{\sum_{z_k\in Z}\theta_{ik}\phi_{kj}}</script>

<p>在第一次的迭代时,会基于猜测或其他方法假定参数<script type="math/tex">\theta_{ik},\phi_{kj}</script>的值.这样便能获得隐变量的后验分布.		</p>

<h3 id="m">M步</h3>

<p>在M步,将E步得到的隐变量的后验分布代入似然估计中,通过极大似然估计,更新参数<script type="math/tex">\theta_{ik},\phi_{kj}</script>的值.似然函数的期望为:			</p>

<script type="math/tex; mode=display">E[l]=\sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)\sum_{k=1}^Kp(z_k｜d_i,w_j)log[\theta_{ik}\phi_{kj}]</script>

<p>这是一个多元函数求极值的问题,其中约束条件有:		</p>

<script type="math/tex; mode=display">\sum_{z_k\in Z}\theta_{ik}=1</script>

<script type="math/tex; mode=display">\sum_{w_j\in W}\phi_{kj} = 1</script>

<p>将问题转化成拉格朗日乘法,得到的拉格朗日函数为:			</p>

<script type="math/tex; mode=display">H=E[l]+\sum_{k=1}^K\gamma_k(1-\sum_{j=1}^M\phi_{kj}+\sum_{i=1}^N\rho_i(1-\sum_{k=1}^K\phi_ik)</script>

<p>这是一个关于<script type="math/tex">\theta_{ik}和\phi_{kj}</script>的函数,分别对其求偏导,得到:		</p>

<p><img src="../images/140530/plsa-2.png" alt="image" />		</p>

<p>最后求出期望最大化的新的参数值为:			</p>

<p><img src="../images/140530/plsa-3.png" alt="image" />		</p>

<h2 id="section">3.总结</h2>
<hr />

<p>从前面的推理来看,pLSA和LSA好像并没有什么联系.LSA是基于向量空间模型的SVD分解来进行隐空间投射,来挖掘文档之间的语义层的联系.而pLSA模型是基于层面模型的关于潜在变量的统计建模过程.我们知道pLSA是在文档和词项之间加入了一层隐含变量(主题),我们不妨做如下假定:		</p>

<script type="math/tex; mode=display">U=(p(d_i｜z_k))_{i,k},V=(p(w_j｜z_k))_{j,k},\overline\sum=diag(p(z_k))_k</script>

<p>则有<script type="math/tex">p=U\overline\sum V=(\sum_kp(d_i｜z_k)p(z_k)p(w_j｜z_k))_{i,j}=(p(d_j,w_j))_{i,j}</script>,可见,			
<script type="math/tex">[U,\overline\sum,V]正是p的svd分解,p(z_k)是p的k个特征值.</script>		</p>

<p>不同的是,LSA使用特征值进进行SVD分解,则实际上是L2范数意义下对N的最好估计,而pLSA使用EM算法,使似然函数的期望达到最大.并且,pLSA的P矩阵有明确的统计意义,而LSA的这种意义不明显.</p>

<h3 id="section-1">参考内容:</h3>

<p><a href="http://blog.jqian.net/post/plsa.html">主题模型之pLSA   http://blog.jqian.net/post/plsa.html</a>		</p>

<p><a href="http://blog.csdn.net/yangliuy/article/details/8330640">pLSA及EM算法    http://blog.csdn.net/yangliuy/article/details/8330640</a></p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-06-02T00:00:00-04:00"><a href="http://jwchennlp.github.com/parameter-estimation-approaches/">June 02, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/parameter-estimation-approaches/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/parameter-estimation-approaches/" rel="bookmark" title="参数评估方法" itemprop="url">参数评估方法</a></h1>
    
  </header>
  <div class="entry-content">
    <p>pLSA和LDA主题模型是当前统计自然语言处理领域非常热门的问题,这些主题模型一般都是对文本的生成过程提出自己的概率图模型,然后利用已有的文本数据做参数评估.本文主要介绍其中会用到的三种参数评估方法,包括极大似然估计(MLE),最大后验(MAL)和贝叶斯估计.	</p>

<p>我们主要考虑两个推理问题:		</p>

<p>(1). 评估参数$\theta$的值以最好的拟合观察到的数据集X.		
(2). 根据已观测到的数据集X计算新的观察值$\widetilde x$的概率		</p>

<p>第一个问题可以看成是估计问题,第二个问题可以看成是预测或是回归问题.	在贝叶斯统计中,参数估计问题可以表述为:		</p>

<script type="math/tex; mode=display">p(\theta｜X) = \frac{p(\theta)p(X｜\theta)}{p(X)}</script>

<p>并且我们可以用如下术语定义:</p>

<script type="math/tex; mode=display">posterior=\frac{likelihood*prior}{evidence}</script>

<h2 id="mle">1.极大似然估计(MLE)</h2>
<hr />

<p>极大似然估计主要是求使似然函数值最大的参数值($\theta$),似然函数为:		</p>

<script type="math/tex; mode=display">L(\theta｜X) = \prod_{x\in X}p(x｜\theta)</script>

<p>对似然函数取对数并另偏导数为0,则可求得参数的解:		</p>

<script type="math/tex; mode=display">\theta_{ML} = argmax_{\theta}L(\theta｜X) = argmax_{\theta}\sum_{x\in X}\log{p(x｜\theta)}</script>

<p>对上述函数关于参数$\theta_k$求偏导并是的偏导值为0:		</p>

<script type="math/tex; mode=display">\frac{\partial L(\theta｜X)}{\partial \theta_k} = 0, \forall \theta_k\in theta</script>

<p>根据已观测到的数据集X计算新的观察值$\widetilde x$的概率为:		</p>

<p><img src="../images/140530/mle.png" alt="image" /></p>

<p>以抛硬币的贝努利实验为例,每次抛硬币时正面出现的概率为p(未知),假设进行抛硬币N次得到结果集合C.用极大似然估计来求解参数p:		</p>

<script type="math/tex; mode=display">l(p)=\sum_{i=1}^N\log{p(C=c_i｜p)} \\
	  =n^{(1)}log{p(C=1｜p)}	+n^{(0)}log{p(C=0｜p)} \\
	  =n^{(1)}log{p}+n^{(0)}log{(1-p)}</script>

<p>其中$n^{(1)}表示N次实验结果中正面出现的次数,n^{(0)}$表示反面出现的次数.对似然函数求偏导得:		</p>

<script type="math/tex; mode=display">\frac{\partial l}{\partial p} = \frac{n^{(1)}}{p}-\frac{n^{(0)}}{1-p}=0</script>

<script type="math/tex; mode=display">\hat{p}_{ML} = \frac{n^{(1)}}{n^{(1)}+n^{(0)}}=\frac{n^{(1)}}{N}</script>

<p>假设抛硬币20次,其中正面出现的次数为12次,则由极大似然估计得出正面出现的概率p=0.6,并且可以预测下一次抛硬币正面向上的概率为0.6．</p>

<h2 id="map">2.最大后验估计(MAP)</h2>
<hr />

<p>最大后验估计(Maximum a posteriori estimation,MAP)和极大似然估计十分相似,但是最大后验估计中加入了对参数的先验信念(priori belief),它的权重设定为先验分布$p(\theta)$,最大后验估计不是要求似然函数最大化,而是要求由贝叶斯公式算出的整个后验概率最大.		</p>

<p><img src="../images/140530/map.png" align="center" /></p>

<p>和极大似然估计相比,在最大后验估计中我们需要加入先验分布的对数.先验分布可以理解为我们对事物约定俗成的看法或普遍接受的规律.例如在抛硬币的过程中,如果硬币是一枚正常的硬币,我们认为每次抛硬币正面发生的概率应该服从一个概率分布,这个概率在0.5出取得最大值,这个分布就是先验分布.先验分布的参数我们成为超参数(hyperparameter),即:	</p>

<script type="math/tex; mode=display">p(\theta) = p(\theta｜\alpha)</script>

<p>通过最大化<script type="math/tex">L(\theta｜X)+\log{p(\theta)},可以得到MAP的参数估计值\hat{\theta}_{MAP}</script>.当根据已有的数据预测新数据x的概率时:		</p>

<script type="math/tex; mode=display">p(\hat{x}｜X) \approx \int_{\theta\in \Theta}p(\hat{x}｜\hat{\theta}_{MAP})p(\theta｜X)d\theta=p(\hat{x}｜\hat{\theta}_{MAP})</script>

<p>这里用beta分布来描述硬币的先验分布:		</p>

<script type="math/tex; mode=display">p(p｜\alpha,\beta) = \frac{1}{B(\alpha,\beta)}p^{\alpha-1}(1-p)^{\beta-1}</script>

<p>其中beta函数<script type="math/tex">B(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}</script>,函数<script type="math/tex">\Gamma</script>是Gamma函数,可以理解为实数域的阶乘函数:<script type="math/tex">x!=\Gamma(x+1)</script>,beta分布的变量取值范围为[0,1].下面为beta函数在不同参数($\alpha,\beta$)下的概率密度函数:		</p>

<p><img src="../images/140530/map2.png" alt="image" /></p>

<p>从图中可以看出,当参数取不同值时,beta函数的概率密度函数差异很大,基本上beta函数可以通过调节参数来拟合很多的概率分布.beta算是”万能”的概率分布函数.		
在前面的例子中,相信正常硬币正面发生的概率在0.5处取得最大值,所以设定$\alpha=\beta=5(其实,只要\beta=\alpha)$就能保证在0.5处的概率最大,他们的取值只是限定了他们的收敛速度(值越大概率值越密集).		</p>

<p><img src="../images/140530/map3.png" alt="image" /></p>

<p>我们可以看出与极大似然估计相比,分子项多了$\alpha-1,分母项多了\alpha+\beta-2$,这就是我们的先验信念(priori belief)在起作用,当我们对先验分布有一个很确信的认知时,如我们假设银币实验中,正面出现的概率在0.5处取最大值,并且左右波动的可能性很小,这样为了表示我们的强先验信念,我们可以将$\alpha=\beta$设定为一个大的值.这表现在公式中就可以表现为,当实验次数不多时,参数估计结果会更多的偏向于先验分布,只有当实验次数足够多时,先验估计的影响才会减弱.</p>

<p>仍采用上面的样例,20次实验中,正面出现的次数为12次,则<script type="math/tex">\hat{\theta}_{MAP} = \frac{12+4}{20+8}=0.571</script>,这表明”硬币是均匀的”这一先验对参数估计有影响.</p>

<p>上面的实验是$\alpha=\beta=5$的情况,我们假设先验分布在最大值左右波动概率很大,如设定$\alpha=\beta=2$,则<script type="math/tex">\hat{\theta}_{MAP} = \frac{12+1}{20+2}=0.591</script>,先验对参数估计的影响减弱.		</p>

<h2 id="section">3.贝叶斯估计</h2>
<hr />

<p>贝叶斯估计对MAP做了如下扩展,参数$\theta$由一个具体的值变成参数上个的一个概率分布.这里就不再是单纯的考虑后验概率最大时的参数值,而是将参数的期望和方差信息一同考虑在内.首先通过贝叶斯准则计算后验分布:		</p>

<script type="math/tex; mode=display">p(\theta｜X) = \frac{p(\theta)p(X｜\theta)}{p(X)}</script>

<p>由于我们并不是要找后验分布的最大值,所以我们需要计算P(X),由全概率公式展开的:		</p>

<script type="math/tex; mode=display">p(X) = \int_{\theta \in \Theta}p(X｜\theta)p(\theta)d\theta</script>

<p>当观测到新数据时,参数的后验概率会自动调整,并且通过统计分析可以最终得出后验概率分布.但是,P(X)的积分的求解十分复杂.			</p>

<p>根据已观察到的数据来预测观测一个新数据的概率为:			</p>

<p><img src="../images/140530/map4.png" alt="image" />		</p>

<p>对于抛硬币的贝努利实验,假设N次试验得到的试验结果集合C,这里我们加入beta(5,5)的先验信念,在MAP中我们要求后验概率的最大值,在贝叶斯估计中我们要求	满足beta分布的参数的期望.		</p>

<p><img src="../images/140530/map5.png" alt="image" /></p>

<p>$Beta(\alpha,\beta)分布的均值,&lt;p｜\alpha,\beta&gt;=\alpha(\alpha+\beta)^{-1},方差V(p｜\alpha,\beta)=\alpha\beta(\alpha+\beta+1)^{-1}(\alpha+\beta)^{-2}$,根据之前的统计,评估结果为:		</p>

<script type="math/tex; mode=display">% <![CDATA[
<P｜C> = \frac{n^{1}+\alpha}{n^{1}+n^{0}+\alpha+\beta}=\frac{n^{1}+5}{N+10} %]]></script>

<script type="math/tex; mode=display">V(p｜\alpha,\beta)=\frac{(n^{1}+\alpha)(n^{0}+\beta)}{(N+\alpha+\beta+1)(N+\alpha+\beta)^2}=\frac{(n^{1}+5)(n^{0}+5)}{(N+11)(N+10)^2}</script>

<p>当20次试验中有12次出现正面时,均值为17/30=0.567,方差为17<em>13/(30$</em>31^2$)=0.0079.		</p>

<p>对于上述三种方法的参数估计结果如下图所示:		</p>

<p><img src="../images/140530/map6.png" alt="image" />		</p>

<h3 id="section-1">参考内容:</h3>

<p>Gregor Heinrich:Parameter estimation for text analysis	</p>

<p><a href="http://blog.csdn.net/yangliuy/article/details/8296481">文本语言模型的参数估计 http://blog.csdn.net/yangliuy/article/details/8296481</a>		</p>

<p><a href="http://blog.jqian.net/post/lda.html">主题模型之lda http://blog.jqian.net/post/lda.html</a></p>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-05-30T00:00:00-04:00"><a href="http://jwchennlp.github.com/lsa-topic-model/">May 30, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/lsa-topic-model/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/lsa-topic-model/" rel="bookmark" title="Latent Semantic Analysis(LSA的目标是在找到一个数据映射之后能很好的词汇层面信息的同时能够表示不同实体间的语义关系)" itemprop="url">Latent Semantic Analysis(LSA的目标是在找到一个数据映射之后能很好的词汇层面信息的同时能够表示不同实体间的语义关系)</a></h1>
    
  </header>
  <div class="entry-content">
    <h2 id="section">1.向量空间模型</h2>
<hr />

<p>在信息检索领域,需要对文本语料进行建模,使得能够实现高效进行语料检索的同时保留文本的主要信息.当前采用较多的是向量空间模型.向量空间模型可以看作是一个文本-词项矩阵(matrix<em>{mn}),假设整个语料库有m篇文档,语料库的词项总数为n个,则对每一篇文档,我们用一个n维的向量来描述这个文档,向量中的每一维表示文档中词的权重,权重的度量方法可以有多种(tf,tf-idf),当我们采用tf-idf方法时,对于得到的矩阵中的某一元素,m</em>{ij}表示的是此项w_j在文档d_i中的tf-idf值.向量空间模型基于词袋方法(bag of words)构建文档-词项矩阵,词袋方法也就是说我们之关注文档中出现的词,而不关注文档中词的顺序.</p>

<p>对于用响亮空间模型来描述的文档,每个文档都是一个关于词项的向量,当我们要衡量两个文档的相似度时,我们只要求两个向量之间的相似度.当两个文档向量的相似度为1时,说明两个文档完全相同,当两个文档向量的相似度为0时,说明两个向量完全不相关.		</p>

<p>假设我们有两篇文档$d_1,d_2$:		</p>

<p>$d_1为”The product of apple is excellent”,d_2为”Iphone is popular in world”$,两篇文档的词袋为{the product of apple is excellent iphone,popular,in world}.如果我们按照词袋的词序用tf(词项在文档中出现的次数)描述词的权重时,两篇文档的文档向量可以表述为:		</p>

<p>$d_1=[1,1,1,1,1,1,0,0,0,0],d_2=[0,0,0,0,1,0,1,1,1,1]$,当我们要衡量两个文档的相似度时,两个文档向量的相似度为$\frac{d_1*d_2}{｜d_1｜｜d_2｜}=0.182$,可以认为两篇文档相似度很低.其实返回文档,我们发现文档1说的是苹果公司的产品很好,文档2说的是Iphone手机很受欢迎,可以理解为apple和iphone来说是比较相近的.		</p>

<p>向量空间模型用规范化的格式(每个文档都是一个定长的向量)来对文档进行建模,且每个词的权重可以通过tf-idf等方式进行很好的度量,所以向量空间模型在描述文档信息方面是比较有效的.但是向量空间模型很难识别文档中的同义词和一词多义情况,这从文档的向量空间模型表述中可以看出来,文档中的每一个词项都在文档向量的某一维中表述出来,所以当文档中出现相似词时,相似词的权重是在不同维度中描述的,并且当一个词在文档中有多个含义时,词的多个含义在文档向量中也只是在某一维中描述.</p>

<h2 id="latent-semantic-analysis">2.Latent Semantic Analysis</h2>
<hr />

<p>针对向量空间模型的这些缺点,我们可以采用LSA方法.LSA主要是对按向量空间模型构建的文档-词项高维矩阵映射到一个低维的隐语义空间.LSA的目标是在找到一个数据映射之后能很好的词汇层面信息的同时能够表示不同实体间的语义关系.		</p>

<p>LSA依靠奇异值分解(SVD)将文档的向量空间模型矩阵映射到低维空间.这里我们先介绍一下SVD的概念.		</p>

<h3 id="svd">奇异值分解(SVD)</h3>

<p>当矩阵是方阵的时候,我们可以通过球特征值,来描述矩阵中的重要特征.奇异值分解能描述任意矩阵中的重要特征,对于矩阵A:		</p>

<script type="math/tex; mode=display">A_{mn}=U_{mm}\sum_{mn}V^T_{mm}</script>

<p>其中U和V都是正交矩阵,即$UU^T=VV^T=I,\sum中对角线外的其他元素都为0,对角线上元素为奇异值$.		</p>

<script type="math/tex; mode=display">A^TA=(U\sum{V^T})^T(U\sum{V^T}) \\
  	 =V\sum{^T}U^TU\sum{V^T} \\
  	 =V\sum{^T}\sum{V^T}</script>

<p>因为U为正交矩阵,所以$A^TA的的特征值为\sum{^T}\sum中对脚线上的非0值$.		</p>

<script type="math/tex; mode=display">A^TAv_i=\lambda_iv_i</script>

<p>$v_i表示上面的右奇异响亮V^T$,此外我们可以得到:		</p>

<script type="math/tex; mode=display">\sigma_i=\sqrt{\lambda_i}  \\
  u_i = \frac{1}{\lambda_i}Av_i</script>

<p>这里的$\sigma就是上面说的奇异值,u_i是有奇异向量U,奇异值矩阵\sum$中对角线上的奇异值是按从大到小顺序排列的,奇异值的大小可以理解为特征的重要程度,奇异值越大,描述的特征就越重要.</p>

<p>现在回到LSA的讨论中,我们知道LSA通过SVD来将向量空间映射到隐语义空间,主要实现过程是,对奇异值矩阵,我们只取对角线上的前K个奇异值,其余奇异值设为0,现在我们得到的向量空间为:		</p>

<script type="math/tex; mode=display">\overline{A} =U_{mk}\sum_{kk}V^T_{kn}\approx 	U_{mm}\sum_{mn}V^T_{nn}=A</script>

<p>映射后的向量空间会近视等于原始的向量空间是因为,对奇异值矩阵进行了处理,只保留了前k个最大的奇异值,奇异值中值越大的说明所描述的特征越重要,奇异值越小说明所描述的特征贡献越弱,且奇异值在对角线上的减少程度很大,所以所除去前k个最大的奇异值,剩下的非0的奇异值说描述的特征贡献很弱.所以映射后的向量空间与初始向量空间接近.		</p>

<p>说明一点:正常情况下,按照向量空间模型构建的向量空间是非常稀疏的,当采用LSA进行隐语义空间映射后,向量空间的稀疏性会减弱,这可能有助于计算文档之间的联系,尽管文档之间不存在很多相同的词项.	</p>

<h3 id="section-1">参考资料</h3>

<p><a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html">机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 http://leftnoteasy.cnblogs.com</a>		</p>

<p><a href="http://www.cnblogs.com/kemaswill/archive/2013/04/17/3022100.html">Latent Semantic Analysis(LSA/ LSI)算法简介 http://www.cnblogs.com/kemaswill/</a>	</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-05-20T00:00:00-04:00"><a href="http://jwchennlp.github.com/em-algorithm/">May 20, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/em-algorithm/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/em-algorithm/" rel="bookmark" title="EM算法" itemprop="url">EM算法</a></h1>
    
  </header>
  <div class="entry-content">
    <p>EM算法可以用于含有隐含变量的参数评估问题.		</p>

<h2 id="jesen-">Jesen 不等式</h2>
<hr />

<p>当函数f为凸函数时,我们知道$f^{\prime\prime}(x)\geq0,如果函数f的输入为向量时,则其半正定矩阵H\geq0$Jesen不等式可以描述为:		</p>

<p><strong>定理</strong>  f是一个凸函数,X是一个随机变量,则:		</p>

<script type="math/tex; mode=display">E[f(X)]\geq{f(EX)}</script>

<p>并且如果f是严格凸函数($f^{\prime\prime}(x)&gt;0),则E[f(X)]\geq{f(EX)}$当且仅当p(X)=P(EX)=1,即X是一个常数.		</p>

<p>通过下面的图可以有个更清晰的认知:			</p>

<p><img src="../images/0520/1.png" alt="image" /></p>

<p>如图可知f为严格凸函数,假设x有0.5的概率取值为a(p(x=a)=0.5),有0.5的概率取值为b,则变量x的期望应当为E[X]=$\frac{a+b}{2}$,期望x值应该是a和b的中点,E(f(x))=$\frac{f(a)+f(b)}{2}$,可以知道$E[f(X)]\geq{f(EX)}$,并且我们知道,只有当X取一个值的概率为1时,有$E[f(X)]={f(EX)}$</p>

<h2 id="em">EM算法</h2>
<hr />

<p>假设我们有一个关于m个独立样本集${x^{(i)},…,x^{(m)}}$的参数评估问题,样本集中包含了隐含变量z,我们需要估计模型P(x,z)的参数$\theta,参数\theta$的最大似然估计为:		
<script type="math/tex">l(\theta) = \sum_{i=1}^m\log{p(x^{(i)};\theta)} \\
			=\sum_{i=1}^m\log\sum_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)</script>		</p>

<p>其中$z^{(i)}是隐含随机变量,如果z^{(i)}是可观测的$,那么通过极大似然估计便可以求得参数解.		</p>

<p>EM算法给出了实现参数评估的一种有效的方法,精确的最大化$l(\theta)可能很困难,这里我们可以采用如下的替代策略:在E步不断的建立l(\theta)的下界$,并且在M步优化下界.通过这两个步骤的迭代过程来实现$l(\theta)的最大化$.</p>

<p>假设$Q_i是变量z的分布,则有\sum_zQ_i(z)=1,_i(z)\geq0$,则可以得到:</p>

<script type="math/tex; mode=display">l(\theta) = \sum_{i=1}^m\log{p(x^{(i)};\theta)}  \\
			=\sum_{i=1}^m\log\sum_{z^{(i)}}p(x^{(i)},z^{(i)};\theta) \\
			=\sum_{i=1}^m\log\sum_{z^{(i)}}Q_i{z^{(i)}}\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i{z^{(i)}}}  \\
			\geq\sum_{i=1}^m\sum_{z^{(i)}}Q_i{z^{(i)}}\log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i{z^{(i)}}}</script>

<p>公式的第三步到第四步运用到了Jesen不等式,$\log(x)函数是严格凹函数,所以E[f(X)]\leq{f(EX)},其中Q_i{z^{(i)}}表示变量z^{i}$的概率分布,$\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i{z^{(i)}}}表示变量为z^{i}的函数$,第三步表示的是函数的期望,又由于函数是凹函数,所以其值要大于等于期望的函数.</p>

<p>现在给定了关于隐含变量的分布$Q_i,我们可以得知l(\theta)的下界,对于Q_i有许多选择,我们该如何选择呢?当我们猜测一个初始的\theta值时$,E步所要实现的就是希望我们的替换的$l(\theta)不断的贴近真实l(\theta)的下界,我们从Jesen不等式可以知道E[f(X)]={f(EX)}$的充分必要条件是x为一个常数,也就是说:		</p>

<script type="math/tex; mode=display">\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i{z^{(i)}}}=c(常数)</script>

<p>又$\sum_{z}Q_i(z^{(i)})=1$		</p>

<script type="math/tex; mode=display">\sum_{z}p(x^{(i)},z^{(i)};\theta)=c\sum_{z}Q_i(z^{(i)})=c</script>

<p>可以得到:			</p>

<script type="math/tex; mode=display">Q_i(z^{(i)})=\frac{p(x^{(i)},z^{(i)};\theta)}{\sum_{z}p(x^{(i)},z^{(i)};\theta)} \\
			  =\frac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)} \\
			  = p(z^{(i)}｜x^{(i)};\theta)</script>

<p>这样在初始猜测一个参数$\theta后,的出z^{(i)}在x^{(i)}和参数\theta下的后验概率分布,即可以得到隐含变量的分布Q^{(i)}$.		</p>

<p><img src="../images/0520/2.png" alt="image" /></p>

<h2 id="section">证明收敛</h2>
<hr />

<p>这里需要证明算法最终是否会收敛,假设$\theta^{(t)}和\theta^{(t+1))}为两个EM算法成功迭代的参数值,我们需要证明l(\theta^{(t)})\leq{l(\theta^{(t)})}$,如果不等式成立的话,也就是说每次迭代都使得似然估计的值变大.当我们已经通过迭代获得了$\theta^{(i)}$,我们将通过E步选择$Q_i(z^{(i)})=p(z^{(i)}｜x^{(i)};\theta),由于我们知道在E步的时候因为要建立l(\theta^{(t)})的下界$,所有根据Jesen不等式必须有:		</p>

<p><img src="../images/0520/3.png" alt="images" /></p>

<p>对上面等式的右边通过极大似然估计获得$\theta^{(t+1)}$,并且:			</p>

<p><img src="../images/0520/4.png" alt="images" />		</p>

<p>第一个不等式是是由于:		</p>

<p><img src="../images/0520/5.png" alt="images" />	</p>

<p>对任意的$Q^{(i)}和\theta都成立,这里设定Q^{(i)}=Q_t{(t)},\theta=\theta^{(t+1)},第二个不等式成立是因为\theta^{(t+1)}$是通过如下计算获得:			</p>

<p><img src="../images/0520/6.png" alt="images" /></p>

<p>如果我们定义:		</p>

<p><img src="../images/0520/7.png" alt="images" /></p>

<p>通过之前的推导我们知道$l(\theta)\geq{J(Q,\theta)}$,那么EM算法也可以看作是J函数上的坐标上升算法,E步相当依据猜测或上步迭代的$\theta来最大化Q_i,M步相当于根据根据Q_i优化\theta$</p>

<p><strong>参考内容</strong>		</p>

<p><a href="http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html">JeeryLead(EM算法)</a>	</p>

<p><a href="http://blog.csdn.net/abcjennifer/article/details/8170378">Rachel-Zhang(EM算法原理)</a>		</p>

<p><a href="http://blog.csdn.net/zouxy09/article/details/8537620">zouxy09(从最大似然到EM算法浅析)</a>			</p>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->


<div class="pagination">
  
    Previous
  
  <ul class="inline-list">
    <li>
      
        <span class="current-page">1</span>
      
    </li>
    
      <li>
        
          <a href="http://jwchennlp.github.com/page2">2</a>
        
      </li>
    
      <li>
        
          <a href="http://jwchennlp.github.com/page3">3</a>
        
      </li>
    
      <li>
        
          <a href="http://jwchennlp.github.com/page4">4</a>
        
      </li>
    
  </ul>
  
    <a href="http://jwchennlp.github.com/page2" class="btn">Next</a>
  
</div><!-- /.pagination -->
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2014 jwchen. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://jwchennlp.github.com/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://jwchennlp.github.com/assets/js/scripts.min.js"></script>

          

</body>
</html>