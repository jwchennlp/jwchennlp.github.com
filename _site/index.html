<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Latest Posts &#8211; My blog</title>
<meta name="description" content="Describe this nonsense.">
<meta name="keywords" content="Jekyll, theme, themes, responsive, blog, modern">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Latest Posts">
<meta property="og:description" content="Describe this nonsense.">
<meta property="og:url" content="http://jwchennlp.github.com/index.html">
<meta property="og:site_name" content="My blog">





<link rel="canonical" href="http://jwchennlp.github.com/">
<link href="http://jwchennlp.github.com/feed.xml" type="application/atom+xml" rel="alternate" title="My blog Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://jwchennlp.github.com/assets/css/main.min.css">
<!-- Webfonts -->
<link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://jwchennlp.github.com/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://jwchennlp.github.com/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://jwchennlp.github.com/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://jwchennlp.github.com/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://jwchennlp.github.com/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://jwchennlp.github.com/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://jwchennlp.github.com/images/apple-touch-icon-144x144-precomposed.png">



<!--
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
                  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                          });
</script>
-->
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://jwchennlp.github.com">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://jwchennlp.github.com/images/avatar.jpg" alt="jwchen photo" class="author-photo">
					<h4>jwchen</h4>
					<p>不积跬步，无以至千里;不积小流，无以成江海</p>
				</li>
				<li><a href="http://jwchennlp.github.com/about/">Learn More</a></li>
				<li>
					<a href="mailto:hit1093710417@email.com"><i class="icon-envelope"></i> Email</a>
				</li>
				
				
				
				
				<li>
					<a href="http://github.com/jwchennlp.github.com"><i class="icon-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://jwchennlp.github.com/posts/">All Posts</a></li>
				<li><a href="http://jwchennlp.github.com/tags/">All Tags</a></li>
			</ul>
		</li>
		<li><a href="http://jwchennlp.github.com/theme-setup">Theme Setup</a></li><li><a href="http://mademistakes.com">External Link</a></li>
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  <div class="image-credit">Image source: <a href="http://www.dargadgetz.com/ios-7-abstract-wallpaper-pack-for-iphone-5-and-ipod-touch-retina/">dargadgetz</a></div><!-- /.image-credit -->
  
    <div class="entry-image">
      <img src="http://jwchennlp.github.com/images/abstract-1.jpg" alt="Latest Posts">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>My blog</h1>
      <h2>Latest Posts</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-05-09T00:00:00-04:00"><a href="http://jwchennlp.github.com/odps-model/">May 09, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/odps-model/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/odps-model/" rel="bookmark" title="阿里大数据竞赛方法总结" itemprop="url">阿里大数据竞赛方法总结</a></h1>
    
  </header>
  <div class="entry-content">
    <h2 id="section">逻辑回归</h2>

<h3 id="section-1">特征抽取</h3>
<p>将前四个月的数据切分成4:1,为了训练模型，前一部分数据用于抽取特征ｘ，后一部分用于获取类别ｙ．具体描述为，根据每个用户对每个物品的行为没一个数据，如果行为在第一部分，但是在第二部分没有购买行为，这类别为负例，若在第二部分出现购买行为，这此行为为正例．如果行为只在第二部分产生，则次用户对物品的行为无效，直接剔除．  </p>

<h3 id="section-2">模型训练</h3>
<p>在得出了模型的正例和负例之后，因为正例负例差很多，不能直接进行训练．这里采取的策略是按比例对负例进行采样，采样后结合正例进行训练．模型训练直接用xlab平台的逻辑回归进行训练．    </p>

<h3 id="section-3">预测结果</h3>
<p>在训练完模型之后，我们按照相同的方法对所有前４个月的数据进行特征抽取．并用训练好的模型进行预测．预测后，会返回每个数据属于哪一类别的概率，我们可以通过限定概率值得到最终结果．</p>

<h2 id="section-4">存在改进的地方</h2>
<ul>
  <li>没有对数据进行去噪声操作．     </li>
  <li>对负例的采样比例的设定．      </li>
  <li>模型训练时的迭代次数，及是否可以通过多次采样对多次结果进行加权获取最终结果．</li>
</ul>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-05-08T00:00:00-04:00"><a href="http://jwchennlp.github.com/support-vector-machine/">May 08, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/support-vector-machine/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/support-vector-machine/" rel="bookmark" title="支持向量机" itemprop="url">支持向量机</a></h1>
    
  </header>
  <div class="entry-content">
    <h2 id="section">问题引出</h2>
<p>在面对一个最简单的二分类问题，并且假设数据集可分的情况下．具体如下图所示．当我们采用逻辑回归实现分类时，我们用一个分类超平面（决策边界）对数据进行数据进行划分，并在划分后，不同类别的数据分布在分类超平面的两边，这分类成功．其实，在数据可分的情况下，我们发现可以有很多条这样的分类超平面，并且都能达到正确分类的效果,这个时候我们可能要问，这些分类超平面的效果一样吗？是否存在一个最优的分类超平面．
<img src="../images/140508/1.png" alt="image" /></p>

<h2 id="section-1">函数间隔和几何间隔</h2>

<h3 id="section-2">函数间隔</h3>

<p>对于上面数据集，我们计算出了一个超平面$w^Tx+b=1$.对于某一个数据点x我们需要判断其内别,如果$w^Tx+b＞0$，则其类ｙ=1，并且如果$w^Tx+b＞0$并且$w^Tx+b$值越大，则这个点的类别为正例的置信度就越高．当$w^Tx+b＜0$时,点所属类别为－１，并且$w^Tx+b$值越小，这这个点类别为负例的置信度就越高．并且当点ｘ被正确分类时，$y(w^T+b)$为正数．从上面图中可以看出，我们设定分类超平面的上部为正例，A,B,C三个的点都被标记为正例，但是C离决策边界最近，可能稍微变化决策边界就可能导致分类错误，所以C分类正确的置信读低．A离决策边界最远，所以Ｃ被分为正例的置信读高.对于点$\left(x^\left(i\right),y^\left(i\right)\right)$为了获得更好的分类效果，我们希望$y\left(i\right)(w^Tx+b)$尽量大，则分类的置信度就越高．</p>

<p>所以，对数据$\left(x^\left(i\right),y^\left(i\right)\right)$，我们就定义$y\left(i\right)(w^Tx+b)$为此数据点的函数间隔，并且如果使得每个点的函数间隔都倾向于一个大值，则分类置信度越高，分类效果越好．   </p>

<p>给定训练集合S={($x^\left(i\right),y^\left(i\right)）$,i=1,…m},我们需要计算每一个样本点到分类超平面的几何距离.在这里，我们需要求出最小的几何距离，并且通过修改分类超平面使得最小几何距离尽可能大，则分类效果更好．      <br />
<script type="math/tex">\widehat{\gamma}=\min_{i=1,...,m}\widehat\gamma^\left(i\right)</script>  </p>

<p>利用函数间隔来衡量分类效果的置信度有一个缺陷，当我们确定某一个分类超平面$w^Tx+b=0$，我们对ｗ,b同时增加ｋ倍,函数间隔由原来的$y\left(w^Tx+b\right)$变成$ky\left(w^Tx+b\right)$．也就是说某一数据点的函数间隔可以可以任意的缩放或增加．</p>

<h2 id="section-3">几何间隔</h2>

<p>在确定分类超平面之后，任一数据点到分类超平面的距离应该是不变的．如果我们用这个距离来衡量分类置信度的话，效果会很好．    <br />
<img src="../images/140508/2.png" alt="image" />  <br />
点A到平面的距离设为$\gamma$,知道分类超平面的法向量为ｗ，那么将A投影到分类超平面上的点B的坐标为$x-\frac{w}{\left|w\right|}\gamma$,$w^Tx+b=0$上，所以有:      <br />
<script type="math/tex">w^T\left(x-\frac{w}{\left|w\right|}\gamma\right)+b=0</script>
求解得$\gamma=\frac{w}{|w|}x+\frac{b}{|w|}$,所以对所有的样本点点$\left(x^\left(i\right),y^\left(i\right)\right)$,我们求得每个样本点的几何边界为:     <br />
<script type="math/tex">\gamma^\left(i\right)=y^\left(i\right)\left(\frac{w}{|w|}x^\left(i\right)+\frac{b}{|w|}\right)</script></p>

<p>给定训练集合S={($x^\left(i\right),y^\left(i\right)）$,i=1,…m},我们需要计算每一个样本点到分类超平面的几何距离.在这里，我们需要求出最小的几何距离，并且通过修改分类超平面使得最小几何距离尽可能大，则分类效果更好．      <br />
<script type="math/tex">\gamma=\min_{i=1,...,m}\gamma^\left(i\right)</script>    </p>

<h2 id="section-4">最优间隔分类器</h2>

<p>当给定训练集之后，按照前面分析直观上最好的分类效果是找到决策边界使得(几何)间隔最大化.因为我们的决策是使得最小几何间隔最大化，则显然对所有点的分类的置信度很高．所以当对于一个线性可分的数据集，我们要通过一个分类超平面来分割所有的正例和负例．那么我们的问题可以转化成下面的优化问题: <br />
<script type="math/tex">\max_{\gamma,w,b}   \gamma \\
 s.t.    y^\left(i\right)\left(\frac{w}{|w|}x^\left(i\right)+\frac{b}{|w|}\right)\geq\gamma,i=1,...,m
</script>    <br />
由于我们知道在确定了(w,b)之后，我们可以通过同比例的缩放或增加(w,b),所以我们可以通过相应的扩张比例使得|w|的值为１．所以优化问题转化成如下形式：   <br />
<script type="math/tex">\max_{\gamma,w,b}   \gamma \\
 s.t.    y^\left(i\right)\left(wx^\left(i\right)+b\right)\geq\gamma,i=1,...,m　\\
 |w|=1
</script>  </p>

<table>
  <tbody>
    <tr>
      <td>通过约束</td>
      <td>w</td>
      <td>=1,使得函数间隔等于几何间隔．但是因为如上的优化问题是非凸问题，我们很难通过软件来进行求解．所以我们将第一个问题转化成如下问题:</td>
    </tr>
  </tbody>
</table>

<p><script type="math/tex">\max_{\gamma,w,b}:  \frac{\widehat\gamma}{|w|} \\
 s.t.    y^\left(i\right)\left(wx^\left(i\right)+b\right)\geq\widehat\gamma,i=1,...,m
</script>    <br />
其中$\widehat\gamma$代表的是最小函数间隔，我们知道函数间隔是可以通过(w,b)的同比例变化而变化，这里为了为了简化计算，我们将$\widehat\gamma$设为１．那么如上的优化问题变为: <br />
<script type="math/tex">\max_{\gamma,w,b}:  \frac{1}{|w|} \\
 s.t.    y^\left(i\right)\left(wx^\left(i\right)+b\right)\geq 1,i=1,...,m
</script>
进一步转变，优化问题变成如下格式：    <br />
<script type="math/tex">\min_{\gamma,w,b}: \frac{1}{2}w^2 \\
 s.t.    y^\left(i\right)\left(wx^\left(i\right)+b\right)\geq 1,i=1,...,m
</script></p>

<h2 id="section-5">拉格朗日算子</h2>

<p>对于如下的原始优化问题：      <br />
<script type="math/tex"> \min_w:f(w) \\
s.t.:g_i(w)\leq0,i=1,...k  \\
h_i(w)=0,i=1,...l   
</script>    <br />
其拉格朗日算子为$L(w,\alpha,\beta)=f(w)+\sum<em>\left(i=1\right)^k\alpha_ig_i(w)+\sum</em>\left(i=1\right)^l\beta_ih_i(w)
$,其中$\alpha_i,\beta_i$为拉格朗日乘数．      <br />
考虑如下等式：<br />
<script type="math/tex">\theta_p(w)=\max_\left(\alpha,\beta:\alpha_i\geq0\right)L(w,\alpha,\beta)</script>  <br />
其中ｐ表示原始的,我们发现当给定ｗ，并且ｗ满足我们原始问题的约束（$g_i(w)\leq0，h_i(w)=0$），如果ｗ违背这些约束，则显然$\theta_p(w)=\infty$,当ｗ满足原始问题约束时，$\theta_p(w)=０$．那么可以的出如下结论：      <br />
<script type="math/tex">\min_w\theta_p(w)=\min_w\max_\left(\alpha,\beta:\alpha_i\geq0\right)L(w,\alpha,\beta)</script>  <br />
同时我们定义$p^*=\min_w\theta_p(w)$为原始问题的解．   <br />
现在对应它的对偶问题：
<script type="math/tex">\max_\left(\alpha,\beta:\alpha_i\geq0\right)\theta_d(p)=\max_\left(\alpha,\beta:\alpha_i\geq0\right)\min_wL(w,\alpha,\beta)</script> <br />
我们知道，最大最小问题的解小于最小最大问题的解：      <br />
<script type="math/tex">d^*=\max_\left(\alpha,\beta:\alpha_i\geq0\right)\min_wL(w,\alpha,\beta)\leq\min_w\max_\left(\alpha,\beta:\alpha_i\geq0\right)L(w,\alpha,\beta)=p^*</script>    </p>

<p>当ｆ和$g_i$为凸函数时，$h_i$为仿射函数(仿射变换的定义是在几何空间中，一个向量空间进行一次线性变换并接上一个平移，变换成另一个向量空间)，有$d^＊=p^＊$，在这些约束下，一定存在一个$w^＊$是原始问题的解，$\alpha^＊,\beta^＊$是对偶问题的解，并且有$d^＊=p^＊=L(w^＊,\alpha^＊,\beta^＊)$,同时这３个参数满足KKT条件，KKT条件描述如下：    <br />
<img src="../images/140508/3.png" alt="image" />
其中第三个约束称为对偶互补条件，并且当$a_i＞０$时，$g_i(w^*)=0$,满足这些条件的点所对应的几何间隔便是最小几何间隔．这些点称为支持向量.</p>

<p>现在回到优化边界分类器部分,我们的原始问题定义为：     <br />
<script type="math/tex">\min_{\gamma,w,b}: \frac{1}{2}w^2 \\
 s.t.    y^\left(i\right)\left(wx^\left(i\right)+b\right)\geq 1,i=1,...,m
</script>    <br />
约束条件可以表示为：      <br />
<script type="math/tex">g_i(w)=-y^\left(i\right)(w^Tx\left(i\right)+b)+1\leq0</script>  <br />
<img src="../images/140508/4.png" alt="image" />    </p>

<p>我们可以看到，有最小几何间隔的点离决策边界最近．我们知道这些点$\left(x^\left(i\right),y\left(i\right)\right)$满足$g_i(w)=0$.我们将这些点称之为支持向量．从上图知道，数据集中有３个支持向量，一般来说支持向量的个数会明显小于数据集的大小，这在后面会相当有用．      </p>

<p>原始问题的拉格朗日算子可以表示为：    <br />
<script type="math/tex">L(w,b,\alpha)=\frac{1}{2}w^2-\sum_{i=1}^m\alpha_i[y^\left(i\right)(w^Tx\left(i\right)+b)+1]</script>
这个时候我们通过求对偶问题$\theta_p(w)$来求原始问题的解．具体方法是对Ｌ函数关于参数ｗ和ｂ求偏导数：    <br />
<img src="../images/140508/5.png" alt="image" />
<img src="../images/140508/6.png" alt="image" />
<img src="../images/140508/7.png" alt="image" />
将得到的约束代回到上面的拉格朗日算子中．得到：    <br />
<img src="../images/140508/8.png" alt="image" />
<img src="../images/140508/9.png" alt="image" />
最后原始问题的对偶优化问题可以定义为：    <br />
<img src="../images/140508/10.png" alt="image" /></p>

<p>现在假定我们已经求得对偶问题最优解的$\alpha_i$，那么我们是如何做预测的呢？对于一个新的数据点ｘ，我们知道预测是通过判断$w^Tx+b$的值来判定的，如果大于０，类别为正例，如果小于０，类别为负例,我们将上面求解的ｗ值代入：         <br />
<img src="../images/140508/11.png" alt="image" />
我们知道$\alpha_i\geq0$,且根据KKT约束条件中的对偶互补条件$\alpha_ig_i(w)\geq0$,并且$\alpha&gt;0$时，$g_i(w)=0$,表示这些点有最小的几何间隔，也就是说这些点表示支持向量．我们知道，在判断ｘ类别的时候，我们只需要考虑$\alpha_i&gt;0$的情况，也对应的我们只需要考虑数据集中的支持向量．同时，支持向量想对于数据集来说是小很多的．这样很显然可以进行高效求解.</p>

<script type="math/tex; mode=display">\mbox{对任意的$x>0$}, \mbox{有 }f(x)>0. </script>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-05-07T00:00:00-04:00"><a href="http://jwchennlp.github.com/generative-model-and-discriminative-model/">May 07, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/generative-model-and-discriminative-model/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/generative-model-and-discriminative-model/" rel="bookmark" title="生成模型和判别模型" itemprop="url">生成模型和判别模型</a></h1>
    
  </header>
  <div class="entry-content">
    <h2 id="section">定义</h2>
<hr />
<blockquote>
  <p>生成方法由数据学习联合概率分布P(x,y)，然后求出条件概率分布P(y｜ｘ)作为预测的模
型，即成生模型:  <br />
<img src="../images/140507/1.png" alt="image" /> <br />
这样的方法成为生成方法，是因为模型表示了给定输入ｘ产生输出ｙ的生成关系．典型的生成模型有，朴素贝叶斯和隐马尔可夫模型．  <br />
判别方法是由数据直接学习决策函数ｆ(x)或者条件概率分布P(y｜x)作为预测的模型，即判别模型，判别方法关心的是对给定的输入x,应该预测什么样的输出ｙ，典型的方法包括感知机，决策树，逻辑回归．   </p>
</blockquote>

<h2 id="section-1">理解</h2>
<hr />
<p>在面对猫狗分类问题时，我们该如何实现呢？  <br />
方法一：当我们利用逻辑回归或是感知机模型时，我们需要数据集所投射的空间中，找到一个决策边界，在决策边界一边的属于一类动物，在决策边界另一边的属于另一种动物．当来一个我们不知道的动物时，我们将它放入空间中，通过判断它在决策边界的那一侧来判断是猫还是狗． <br />
方法二：将数据集中的猫都拿出来，建立一个关于猫的特征的模型．按同样的方法建立一个关于狗的模型．这样，当判断一个动物时，我们分别查看它在猫模型中属于猫的概率和在狗模型中属于狗的概率，哪个值大，便说明属于哪个模型．   </p>

<p>方法一通过对数据集训练出一个模型，并通过判断P(y|x)下的条件概率来判断ｙ的类别．这种方法成为判别方法，对应建立的模型属于判别模型．   <br />
方法二对数据集的每一个类别建立一个模型，并通过联合概率P(x,y)来判断ｘ特征所应对应的类别．这种方法成为生成方法．  </p>

<p>其实通过联合概率来判断类别进行了一个变形，一般我们是要判断P(y|x)下的概率，可以进行如下转换： <br />
<img src="../images/140507/1.png" alt="image" />  <br />
对于某个参数ｘ，其概率值P(x)值在所有类别下都是相同的，所以问题便等同于如下问题：            <br />
<img src="../images/140507/2.png" alt="image" />        </p>

<p>不妨通过一个朴素贝叶斯生成模型来了解生成模型的判定过程．
如图，训练集包含４篇文档，我们需要验证测试集中的文档类别： <br />
<img src="../images/140507/3.png" alt="iamge" />    </p>

<p>我们需要计算每一个类别下P(x｜y)P(y)的概率，并且概率最大的那一类便是文档所属类别．即计算P((Chinese,chinese,Chinese,Tokyo,Japan)｜y=c)P(y=c)和P((Chinese,chinese,Chinese,Tokyo,Japan)｜y=$\bar{c}$)P(y=$\bar{c}$)．</p>

<p>之后利用朴素贝叶斯的的条件独立定义进行求解便能获知测试及属于哪个类别．</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-05-06T00:00:00-04:00"><a href="http://jwchennlp.github.com/index-compress/">May 06, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/index-compress/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/index-compress/" rel="bookmark" title="索引压缩" itemprop="url">索引压缩</a></h1>
    
  </header>
  <div class="entry-content">
    <h3 id="section">为什么要进行索引压缩？</h3>
<hr />
<p>进行索引压缩有以下优点：  </p>

<ul>
  <li>节省磁盘空间．  </li>
  <li>增加高速缓存(cache)的利用率.<br />
倒排索引词典是放在内存中的，倒排记录表放在磁盘上．对与到拍记录上的某些词项ｔ，我们是需要经常访问的，如果将这次词项ｔ所对应的到拍记录表压缩后放在高速缓存中，只要采用得当的解压缩算法，那么当查询词项ｔ的倒排记录表时，只需要访问cache，而不用从磁盘读取数据，能充分减少IR系统的响应时间． </li>
  <li>压缩能够加快从磁盘读取数据的速度．</li>
</ul>

<p>压缩技术分为有损压缩和无损压缩，有损压缩指的是压缩后，原始数据的所有信息都保存下来了．词干还原，大小写转换都属于有损压缩．   </p>

<h3 id="heaps">Heaps定律：词项数目的估计</h3>

<p>heaps定律认为，文档集大小和词汇量之间存在对数上的线性关系.它将词项的数目估计为文档集大小的函数:<script type="math/tex">M=kT^b</script>,其中Ｔ代表文档集合中的词条的个数． </p>

<p>不同文档集下ｋ取值差异较大，因为词汇量大小取决于文档本身以及对他进行处理的方式．当进行词干还原，大小写转换时将降低词汇量增长的速度，允许加入数字和容忍拼写错误则会增加增长率．无论参数取值如何，heaps定律满足一下两条性质：    </p>

<ul>
  <li>词汇量会随着文档集的增加而增加，不会趋于一个定值．     </li>
  <li>大规模文档集的词汇量也会很大．       </li>
</ul>

<h3 id="zipf">Zipf定律：词项在文档中的分布</h3>
<p>Zipf定律用于估计词项在文档中分布，假设$t_1$用于表示文档集中出现最多的词，$t_2$用于表示文档集中出现第二多的词，文档集合中出现第i多的词的文档频率$cf_i$与$\frac{1}{i}$成正比:   <br />
    <script type="math/tex">cf_i=k\frac{1}{i}</script></p>

<h2 id="section-1">词典压缩</h2>
<hr />

<h3 id="section-2">为什么要进行词典压缩</h3>
<p>理想情况下在建立好索引后，我们希望将词典存放在内存中，但是这往往很难实现，尤其是对于实用的搜索引擎和嵌入式系统．限制IR系统的响应之间的一个因素包多对磁盘的访问次数．所以，如果通过压缩来讲所有的或大部分的词典存入内存，将大大加快IR系统的响应速度．    </p>

<h3 id="section-3">将词典看作单一字符串的压缩方法</h3>
<p>采用如下的数据结构进行存储：一个定长的数组用于存储词项（２０Ｂ），４Ｂ的空间用于存储文档频率，４Ｂ的空间用于存储指向倒排记录表的指针．对于一个包含Ｍ个词项的文档空间来说，词典的总空间为M*(20+4+4),当Ｍ＝400,000时，占用空间为11.2MB.
<img src="../images/1/dic_compress_1.png" alt="image" /></p>

<p>这种方法存在很大的不足，首先大部分的英文词平均长度为8B,这显然造成了大部分的空间浪费，其次也存在有些词的长度超过20B,导致的结果便是不能存储这些词．</p>

<p>我们可以采用如下的改进措施，我们建立一个字符串在存储字典中的所有词项,4B的空间存储文档频率,4B的空间存储倒排记录表的指针，这个指针指向前面所有词典构成的长字符串，在长字符串中我们需要每一个词加入一个定位指针，用于指定下一个词的开始位置和当前词的结束位置，由于有400,000个词，每个词为８B,所以寻址空间为400,000<em>8=3.2</em>$10^6$,所以可以用一个长为$\log{3.2<em>10^6}$$\approx$22b，即３Ｂ的指针来表示．词典的总空间为M</em>(4+4+3+8)=7.6MB．  <br />
<img src="../images/1/1.png" alt="image" /></p>

<h3 id="section-4">按块存储</h3>
<p>对上面的压缩方法进行一个变形，这里不再对每个词项都维护一个指向字符串(所有词的组合)的指针．我们首先将我们的词典按块进行划分，例如每５个词为一块，这样对没一个块只需要维护一个这个块指向字符串的指针，同时在长字符串中，我们需要加入一个空间用于指定当前词的长度．在这种机制下，假设一个块内有ｋ个词，我们减少了(k-1)个指针的空间，但是我们需要在字符串中对没个词增加空间以记录其词的长度．假设每个块内有４个词，减少的指针空间为9B,同时对４个词需要增加４Ｂ的空间用于记录词的长度，所以没４个词产生了5B的压缩，所以压缩的空间为400,000*$\frac{1}{4}$*5=0.5MB.    </p>

<p><strong>注意：</strong>我们在这里维护了两个指针，一个用于指向倒排记录表，一个指向字符串用词项的位置，我们压缩的部分是词项指针．   </p>

<p><img src="../images/1/2.png" alt="image" /></p>

<p>我们发现，每个块内的词越多时，则可以压缩的空间越大．但是并非块内词越多越好，在进行词项查找时，对于块间的词我们可以通过二分查找快速定位，但是在快内查找时则是简单的线性遍历，所以我们必须在查找速度和空间压缩见进行权衡．    </p>

<h2 id="section-5">倒排记录表的压缩</h2>
<hr />

<p>倒排记录表的压缩基于下面一个前提，当用文档ID来表示倒排记录表，对与高频词来说，倒排记录表中的记录多并且相邻的记录之间差距会很小，当某高频词出现在某篇文档中时，将其相近的文档中出现高频词的概率会很大．这就给我们提供了对倒排记录表进行压缩的灵感，正常情况下我们对到拍记录表中的每个文档id,都是用定长的空间来存储的，那么对那些高频词的话，我可以通过存储他们倒排记录表相邻的距离（明显小于存储文档id的长度）来达到压缩的目的．     </p>

<h3 id="section-6">可变字节码</h3>
<p>VB(Variable byte,可变字节)码的思想为，我们采用整数个字节来存储文档id,每个字节的后７位为有效编码，第一位为延续位，表示本次编码的结束与否,’1’表示结束．     </p>

<p>可变字节码的解码过程如下，根据延续位（一直获取字节直到字节的首位为１）来获取编码结果，对编码结果进行以下处理，去除所有的延续位，剩余有效编码表示间隔位，将此编码值与前一个编码的结果进行累加即表示文档的ID. 
<img src="../images/1/3.png" alt="image" /></p>

<h3 id="section-7">γ编码</h3>

<p>一元编码：将数值为ｎ的数用ｎ个１并在之后加上一个０来表示的编码方式．  </p>

<p>γ编码主要由两部分组成，偏移量(offset)和长度(length)．长度是数组的二进制编码，但是去除了首位１，长度则是偏移量的长度，但是是通过一元编码的方式实现．对于数值5,二进制编码是101,去掉首位的１，其偏移量是01,偏移量长度为２，则由一元编码表示为110,所以数值５的γ编码为11001.
<img src="../images/1/4.png" alt="image" /></p>

<p>我们发现对数值为Ｋ的数进行二进制编码，其偏移量的长度为$\lfloor\log{k}\rfloor$,其长度的长度为$\lfloor\log{k}\rfloor$+1,所以，数的γ编码长度为２$\lfloor\log{k}\rfloor$＋１．      </p>

<p>有一点不太明白的是采用γ编码是如何实现数据压缩的呢？书的原文是这么说的：      <br />
<img src="../images/1/5.png" alt="image" />
按上面理解，采用ｎ为进行进行表示，那么间距在1~$2^\left(n-1\right)－１$之间都将产生浪费，并在在间距为$2^n$时不能表示．</p>

<p>我的理解是这样的，我们有对倒排记录表的实现一般也是通过链表或是定长数组来实现的，当采用定长的编码格式来存储每个文档ＩＤ时，必然会产生很大的浪费．那么如何通过变长的编码格式并且不需要额外的数组或指针来表明文档的长度，这边是γ编码所做的事了，让我们看一下γ编码的的解码过程：11001,我们首先遍历该编码，知道遇到０时停止，发现长度为２，剩下的偏移量为01,我们知道实际的二进制数为101,也就是说我们通过编码本身可以确定文档id,不需要进行额外的存储．</p>

<h4 id="section-8">参考资料</h4>
<p><a href="https://www.google.com.hk/search?q=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;oq=%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AF%BC%E8%AE%BA&amp;aqs=chrome..69i57j69i65j69i61l3j0.3218j0j1&amp;sourceid=chrome&amp;ie=UTF-8">索引压缩</a>     <br />
<strong>说明：</strong>文章主要内容和图片来自信息检索导论一书。</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-04-26T00:00:00-04:00"><a href="http://jwchennlp.github.com/odps-sql/">April 26, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://jwchennlp.github.com/odps-sql/#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://jwchennlp.github.com/odps-sql/" rel="bookmark" title="odps_sql" itemprop="url">odps_sql</a></h1>
    
  </header>
  <div class="entry-content">
    <p>odps(open data processing service，开源数据处理服务)是阿里巴巴的分布式计算平台。</p>

<p>数据以sql表格的形式存放在odps中，我们可以是使用类似与sql命令的方式对数据进行操作。当让sql中嵌入了odps平台自己的函数和命令。</p>

<p>文档学习ing…</p>

<h2 id="section">表格建立</h2>

<p>创建表格语句如下：   </p>

<div class="highlight"><pre><code class="sql"><span class="lineno">1</span> <span class="k">create</span> <span class="k">table</span> <span class="n">if</span> <span class="k">not</span> <span class="n">exist</span> <span class="n">sales</span><span class="p">(</span>
<span class="lineno">2</span> <span class="n">shop_name</span>  <span class="n">string</span><span class="p">,</span>      
<span class="lineno">3</span> <span class="p">...</span>     
<span class="lineno">4</span> <span class="n">partitioned</span> <span class="k">by</span> <span class="p">(</span><span class="n">sale_date</span> <span class="n">string</span><span class="p">,</span><span class="n">region</span> <span class="n">string</span><span class="p">);</span>     
<span class="lineno">5</span>     <span class="c1">--创建一张分区表sales</span>
</code></pre></div>

<p>partitioned by指定了分区字段，采用分区字段主要是在跟新，新增和读取分区数据时不需要做全表扫描，可以提高效率。       </p>

<p>可以用命令create table…as select…来新建表格，如：        </p>

<div class="highlight"><pre><code class="sql"><span class="lineno">1</span> <span class="k">create</span> <span class="k">table</span> <span class="n">sales1</span> <span class="k">as</span>      
<span class="lineno">2</span>     <span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">sales</span><span class="p">;</span>    
</code></pre></div>

<p>这样在建立表格的同时，将sales的数据复制到新表中，但是原表格的分区字段没有复制到新表中。如果希望新表格和原表格有相同的数据和表结构（分区属性）.可以用create table… like …命令：    </p>

<div class="highlight"><pre><code class="sql"><span class="lineno">1</span> <span class="k">create</span> <span class="k">table</span> <span class="n">sales1</span> <span class="k">like</span> <span class="n">sales</span>  
</code></pre></div>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->


<div class="pagination">
  
    Previous
  
  <ul class="inline-list">
    <li>
      
        <span class="current-page">1</span>
      
    </li>
    
      <li>
        
          <a href="http://jwchennlp.github.com/page2">2</a>
        
      </li>
    
  </ul>
  
    <a href="http://jwchennlp.github.com/page2" class="btn">Next</a>
  
</div><!-- /.pagination -->
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2014 jwchen. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://jwchennlp.github.com/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://jwchennlp.github.com/assets/js/scripts.min.js"></script>

          

</body>
</html>