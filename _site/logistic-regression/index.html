<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>逻辑回归 &#8211; My blog</title>
<meta name="description" content="good good study,day day up">
<meta name="keywords" content="逻辑回归, 机器学习, 梯度下降, 正则化">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="逻辑回归">
<meta property="og:description" content="good good study,day day up">
<meta property="og:url" content="http://jwchennlp.github.com/logistic-regression/">
<meta property="og:site_name" content="My blog">





<link rel="canonical" href="http://jwchennlp.github.com/logistic-regression/">
<link href="http://jwchennlp.github.com/feed.xml" type="application/atom+xml" rel="alternate" title="My blog Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://jwchennlp.github.com/assets/css/main.min.css">
<!-- Webfonts -->
<link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://jwchennlp.github.com/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://jwchennlp.github.com/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://jwchennlp.github.com/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://jwchennlp.github.com/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://jwchennlp.github.com/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://jwchennlp.github.com/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://jwchennlp.github.com/images/apple-touch-icon-144x144-precomposed.png">





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
                  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                          });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
<!--
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
-->

</head>

<body id="post" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://jwchennlp.github.com">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://jwchennlp.github.com/images/avatar.jpg" alt="jwchen photo" class="author-photo">
					<h4>jwchen</h4>
					<p>不积跬步，无以至千里;不积小流，无以成江海</p>
				</li>
				<li><a href="http://jwchennlp.github.com/about/">Learn More</a></li>
				<li>
					<a href="mailto:hit1093710417@email.com"><i class="icon-envelope"></i> Email</a>
				</li>
				
				
				
				
				<li>
					<a href="http://github.com/jwchennlp"><i class="icon-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://jwchennlp.github.com/posts/">All Posts</a></li>
				<li><a href="http://jwchennlp.github.com/tags/">All Tags</a></li>
			</ul>
		</li>
		<li><a href="http://jwchennlp.github.com/theme-setup">Theme Setup</a></li><li><a href="http://mademistakes.com">External Link</a></li>
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->



<div class="entry-header">
  <div class="image-credit">Image source: <a href="http://www.dargadgetz.com/ios-7-abstract-wallpaper-pack-for-iphone-5-and-ipod-touch-retina/">dargadgetz</a></div><!-- /.image-credit -->
  <div class="entry-image">
    <img src="http://jwchennlp.github.com/images/abstract-8.jpg" alt="逻辑回归">
  </div><!-- /.entry-image -->
</div><!-- /.entry-header -->


<div id="main" role="main">
  <article class="hentry">
    <header class="header-title">
      <div class="header-title-wrap">
        
          <h1 class="entry-title"><a href="http://jwchennlp.github.com/logistic-regression/" rel="bookmark" title="逻辑回归">逻辑回归</a></h1>
        
        <h2>May 18, 2014</h2>
      </div><!-- /.header-title-wrap -->
    </header>
    <div class="entry-content">
      <p>对于逻辑回归函数，我们的假设方程为：		</p>

<script type="math/tex; mode=display">h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}</script>

<p>其中，$g(z)=\frac{1}{1+e^{-z}}$称之为sigmoid函数。		</p>

<p>那么为什么要在逻辑回归里使用sigmoid函数呢，观察逻辑回归的假设方程可以发现，如果没有使用sigmoid函数，假设方程与线性回归的假设方程是一样的。但是很显然，线性回归的假设方程的值域为$(-\infty,+\infty)$,而二分类问题一般去之都是固定的值{0,1}.这个时候我们可以使用sigmoid函数，它的作用相当与实现了一个映射，将$(-\infty,+\infty)$值域映射到（0,1）之间。		</p>

<p>我们假定：	
<script type="math/tex">P(Y=1｜Ｘ;\theta)=h_\theta(x) \\
	P(Y=0｜Ｘ;\theta)=1-h_\theta(x)</script>	</p>

<p>上面公式我们可以合并成：		</p>

<script type="math/tex; mode=display">P(Y｜Ｘ;\theta)=(h_\theta(x))^y(1-h_\theta(x))^{1-y} </script>

<p>假设有ｍ个训练集并且相互独立，这参数的似然方程可以表述为：		</p>

<script type="math/tex; mode=display">L(\theta)=p(y｜x;\theta) \\
		   =\prod_{i=1}^mP(y^{(i)}｜x^{(i)};\theta) \\
		   =\prod_{i=1}^m(h_\theta(x^{(i)})^{y^{(i)}}(1-h_\theta(x^{(i)}))^{\left(1-y^{(i)}\right)}</script>

<p>下面可以采用最大似然估计来求出参数值：			</p>

<script type="math/tex; mode=display">l(\theta)=logL(\theta) \\
		   =\sum_{i=1}^my^{(i)}logh(x^{(i)})+(1-y^{(i)})\log(1-h(x^{(i)}))</script>

<p>而后对参数$\theta_j$求偏导:		</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial\theta_j}l(\theta) = \sum_{i=1}^n(y^{(i)}\frac{1}{g(\theta^Tx^{(i)})}-(1-y^{(i)})\frac{1}{1-g(\theta^Tx^{(i)})})\frac{\partial}{\partial\theta_j}g(\theta^Tx) \\
	= \sum_{i=1}^n(y^{(i)}\frac{1}{g(\theta^Tx^{(i)})}-(1-y^{(i)})\frac{1}{1-g(\theta^Tx^{(i)})})g(\theta^Tx)(1-g(\theta^Tx))\frac{\partial}{\partial\theta_j}\theta^Tx \\
	= \sum_{i=1}^m(y^{(i)}(1-g(\theta^Tx))-(1-y^{(i)})g(\theta^Tx))x_j \\
	=\sum_{i=1}^m(y^{(i)}-h_\theta(x))x_j</script>

<p>可以看出，通过极大似然估计来求解参数在逻辑回归中很难实现．		</p>

<h2 id="section">梯度下降</h2>
<hr />

<p>在线性回归中，我们的代价函数是这么定义的：		</p>

<script type="math/tex; mode=display">J(\theta) = \frac{1}{m}\sum_{i=1}^m(y^{(i)}-h_\theta(x^{(i)}))^2</script>

<p>将我们的假设方程$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$代入上面的代价方程会发现代价函数是一个非凸函数，所以很难获得最优解．		
这里我们需要重新定义代价函数：		</p>

<script type="math/tex; mode=display">J(\theta) = \frac{1}{m}\sum_{i=1}^mCost(y^{(i)},h_\theta(x^{(i)}))</script>

<p>其中　		</p>

<p><img src="../images/0518/1.png" alt="image" />		</p>

<p>$h<em>\theta(x)与Cost(y^{(i)},h</em>\theta(x^{(i)}))$之间的关系如下图所示：		</p>

<p><img src="../images/0518/2.png" alt="image" />		</p>

<p>当ｙ＝１时，如果假设方程$h_\theta(x)=1$,代价函数为０，如果代假设方程越接近０，则代价函数越大．			</p>

<p>当ｙ＝０时，如果假设方程$h_\theta(x)=０$,代价函数为０，如果代假设方程越接近１，则代价函数越大．		</p>

<p>可以将代价方程组合为如下格式：		</p>

<script type="math/tex; mode=display">Cost(y,h_\theta(x))=-y\log{h_\theta(x)}-(1-y)\log{(1-h_\theta(x))}</script>

<p>那么含有ｍ个样例的数据集的代价代价函数可以表示为:		</p>

<script type="math/tex; mode=display">J(\theta)=\frac{1}{m}\sum_{i=1}^m-y^{(i)}\log{h_\theta(x^{(i)})}-(1-y^{(i)})\log{(1-h_\theta(x^{(i)}))}</script>

<p>而后，我们可以按如下方式不断的更新$\theta$值，指导代价函数达到最优值，这个时候的所的参数便为解．		</p>

<script type="math/tex; mode=display">\theta_j  =  \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) \\
		   =  \theta_j-\alpha\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}</script>

<p>在之前讨论过的文本分类问题中，我们对每一类别用互信息特征选择方法选择100个特征，文档集的特征总数为741个，用这些特征构建向量空间模型，用逻辑回归进行测试（用scikit-learn工具包），Ｆ_1值能达到86.6%,具体如下图所示：		</p>

<p><img src="../images/0518/3.png" alt="image" /></p>

<h2 id="section-1">正则化</h2>
<hr />

<p>为了避免过拟合现象,可以采用正则化方法.正则化方法是结构风险最小化策略的实现,是在经验风险上加上一个正则化项(regularizer)或罚项(penalty).正则化项一般是模型复杂度的单调递增函数,模型越复杂,正则化值就越大.		</p>

<p>$L_2$正则化代价函数定义为:		</p>

<p><img src="../images/0518/4.png" alt="image" />		</p>

<p>当我们采用梯度下降发求解参数时,$\theta$按如下方式进行迭代:		</p>

<p><img src="../images/0518/5.png" alt="image" />		</p>

<p>$L_1$正则化代价函数定义为:		</p>

<script type="math/tex; mode=display">J(\theta)=\frac{1}{m}[\sum_{i=1}^m-y^{(i)}\log{h_\theta(x^{(i)})}-(1-y^{(i)})\log{(1-h_\theta(x^{(i)}))}]+\frac{\lambda}{2m}\sum_{i=i}^n｜\theta_j｜</script>

<p>采用$L_1$正则化一般可以产生稀疏解,也就是说我们得到的参数解$\theta中会有很多位为0,这这可以理解为\theta_j=0$对应的特征的贡献为0,我们可以直接忽略这些特征.		</p>

<p>为了避免模型的过拟合,我们还可以通过减少特征数目,只选取对模型有很强分类能力或贡献的强特征,从而提出那些表示能力较差的弱特征.这在特征选择中有讲解.同时,我们也可以通过交叉验证的方式进行来查模型是否产生过拟合.		</p>

<h2 id="section-2">随机梯度下降法</h2>

<p>从上面的梯度下降可以知道,在每次对$\theta进行迭代的过程中,我们要通过整个训练集更新\theta值$.那么当训练集非常大的时候,显然更新参数将变得非常耗时.随机梯度下降法可以弥补这些缺点.</p>

<p>在随机梯度下降法中,我们定义代价函数为一个单一训练实例的代价:		</p>

<script type="math/tex; mode=display">cost(\theta,(x^{(i)},y^{(i)}))=\frac{1}{2}(h_{theta}(x^{(i)})-y^{(i)})^2</script>

<p>伪代码如下:		</p>

<p><img src="../images/0518/6.png" alt="image" /></p>

<p>随机梯度下降算法在每一次计算之后便更新参数$\theta$,而不需要在整个训练集上进行迭代.随机梯度法比梯度下降法运算速度要快,但是缺点是,随机梯度下降法不是每一步都朝着”正确”的方向迈出的,因此,算法虽然会逐渐走向全局最小值的位置,但是无法达到全局最优解,而是在最优解附近振荡.		</p>

<h2 id="section-3">在线算法和批处理算法</h2>
<hr />

<p>批处理和在线学习算法都是基于梯度下降原理实现的,批处理需要每次计算时都要考虑整个训练集的数据,并找到一个最快下降方向进行迭代.而在线学习算法只着眼于当前的某一观测值.前者的优点是收敛速度快,缺点是计算复杂.后者的优点是计算量小,收敛速度慢.</p>

      <footer class="entry-meta">
        <span class="entry-tags"><a href="http://jwchennlp.github.com/tags/#逻辑回归" title="Pages tagged 逻辑回归" class="tag">逻辑回归</a><a href="http://jwchennlp.github.com/tags/#机器学习" title="Pages tagged 机器学习" class="tag">机器学习</a><a href="http://jwchennlp.github.com/tags/#梯度下降" title="Pages tagged 梯度下降" class="tag">梯度下降</a><a href="http://jwchennlp.github.com/tags/#正则化" title="Pages tagged 正则化" class="tag">正则化</a></span>
        <span><a href="http://jwchennlp.github.com/logistic-regression/" rel="bookmark" title="逻辑回归">逻辑回归</a> was published on <span class="entry-date date published updated"><time datetime="2014-05-18T00:00:00-04:00">May 18, 2014</time></span></span>
        (revised: <span class="entry-date date modified"><time datetime="2014-05-18 11:25:41 UTC">05/18/2014</time></span>)
        <span class="author vcard"><span class="fn"><a href="http://jwchennlp.github.com/about/" title="About jwchen">jwchen</a></span></span>
        <div class="social-share">
          <ul class="socialcount socialcount-small inline-list">
            <li class="facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=http://jwchennlp.github.com/logistic-regression/" title="Share on Facebook"><span class="count"><i class="icon-facebook-sign"></i> Like</span></a></li>
            <li class="twitter"><a href="https://twitter.com/intent/tweet?text=http://jwchennlp.github.com/logistic-regression/" title="Share on Twitter"><span class="count"><i class="icon-twitter-sign"></i> Tweet</span></a></li>
            <li class="googleplus"><a href="https://plus.google.com/share?url=http://jwchennlp.github.com/logistic-regression/" title="Share on Google Plus"><span class="count"><i class="icon-google-plus-sign"></i> +1</span></a></li>
          </ul>
        </div><!-- /.social-share -->
      </footer>
    </div><!-- /.entry-content -->
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
    
    <div class="read-more">
      
        <div class="read-more-header">
          <a href="http://jwchennlp.github.com/test-classification/" class="read-more-btn">Read More</a>
        </div><!-- /.read-more-header -->
        <div class="read-more-content">
          <h3><a href="http://jwchennlp.github.com/plsa/" title="pLSA">pLSA</a></h3>
          <p>在对LSA的介绍中,我们知道LSA的核心思想是将建立的文档-词项矩阵运用SVD将高维空间映射到到隐语义空间,这样可以较好的解决同义词的问题.但语义的权重不好解释.##1.层面模型(aspect model)-----------pLSA是以层面模型进行建模,层面模型是一个统...&hellip; <a href="http://jwchennlp.github.com/plsa/">Continue reading</a></p>
        </div><!-- /.read-more-content -->
      
      <div class="read-more-list">
        
          <div class="list-item">
            <h4><a href="http://jwchennlp.github.com/lsa-topic-model/" title="Latent Semantic Analysis(LSA)">Latent Semantic Analysis(LSA)</a></h4>
            <span>Published on May 30, 2014</span>
          </div><!-- /.list-item -->
        
          <div class="list-item">
            <h4><a href="http://jwchennlp.github.com/em-algorithm/" title="EM算法">EM算法</a></h4>
            <span>Published on May 20, 2014</span>
          </div><!-- /.list-item -->
        
      </div><!-- /.read-more-list -->
      
    </div><!-- /.read-more -->
  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2014 jwchen. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://jwchennlp.github.com/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://jwchennlp.github.com/assets/js/scripts.min.js"></script>

<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'jwchennlp'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>	        

</body>
</html>
