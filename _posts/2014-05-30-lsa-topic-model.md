---
layout: post
title: "Latent Semantic Analysis(LSA的目标是在找到一个数据映射之后能很好的词汇层面信息的同时能够表示不同实体间的语义关系)"
modified: 2014-05-30 19:01:55 +0800
tags: [LSA,向量空间模型]
image:
  feature: abstract-10.jpg
  credit: dargadgetz
  creditlink: http://www.dargadgetz.com/ios-7-abstract-wallpaper-pack-for-iphone-5-and-ipod-touch-retina/
comments: true
share: true
---

##1.向量空间模型
-------------

在信息检索领域,需要对文本语料进行建模,使得能够实现高效进行语料检索的同时保留文本的主要信息.当前采用较多的是向量空间模型.向量空间模型可以看作是一个文本-词项矩阵(matrix_{mn}),假设整个语料库有m篇文档,语料库的词项总数为n个,则对每一篇文档,我们用一个n维的向量来描述这个文档,向量中的每一维表示文档中词的权重,权重的度量方法可以有多种(tf,tf-idf),当我们采用tf-idf方法时,对于得到的矩阵中的某一元素,m_{ij}表示的是此项w_j在文档d_i中的tf-idf值.向量空间模型基于词袋方法(bag of words)构建文档-词项矩阵,词袋方法也就是说我们之关注文档中出现的词,而不关注文档中词的顺序.

对于用响亮空间模型来描述的文档,每个文档都是一个关于词项的向量,当我们要衡量两个文档的相似度时,我们只要求两个向量之间的相似度.当两个文档向量的相似度为1时,说明两个文档完全相同,当两个文档向量的相似度为0时,说明两个向量完全不相关.		


假设我们有两篇文档$d_1,d_2$:		

$d_1为"The product of apple is excellent",d_2为"Iphone is popular in world"$,两篇文档的词袋为{the product of apple is excellent iphone,popular,in world}.如果我们按照词袋的词序用tf(词项在文档中出现的次数)描述词的权重时,两篇文档的文档向量可以表述为:		

$d_1=[1,1,1,1,1,1,0,0,0,0],d_2=[0,0,0,0,1,0,1,1,1,1]$,当我们要衡量两个文档的相似度时,两个文档向量的相似度为$\frac{d_1*d_2}{｜d_1｜｜d_2｜}=0.182$,可以认为两篇文档相似度很低.其实返回文档,我们发现文档1说的是苹果公司的产品很好,文档2说的是Iphone手机很受欢迎,可以理解为apple和iphone来说是比较相近的.		

向量空间模型用规范化的格式(每个文档都是一个定长的向量)来对文档进行建模,且每个词的权重可以通过tf-idf等方式进行很好的度量,所以向量空间模型在描述文档信息方面是比较有效的.但是向量空间模型很难识别文档中的同义词和一词多义情况,这从文档的向量空间模型表述中可以看出来,文档中的每一个词项都在文档向量的某一维中表述出来,所以当文档中出现相似词时,相似词的权重是在不同维度中描述的,并且当一个词在文档中有多个含义时,词的多个含义在文档向量中也只是在某一维中描述.

##2.Latent Semantic Analysis	
----------------

针对向量空间模型的这些缺点,我们可以采用LSA方法.LSA主要是对按向量空间模型构建的文档-词项高维矩阵映射到一个低维的隐语义空间.LSA的目标是在找到一个数据映射之后能很好的词汇层面信息的同时能够表示不同实体间的语义关系.		

LSA依靠奇异值分解(SVD)将文档的向量空间模型矩阵映射到低维空间.这里我们先介绍一下SVD的概念.		

###奇异值分解(SVD)

当矩阵是方阵的时候,我们可以通过球特征值,来描述矩阵中的重要特征.奇异值分解能描述任意矩阵中的重要特征,对于矩阵A:		

$$A_{mn}=U_{mm}\sum_{mn}V^T_{mm}$$

其中U和V都是正交矩阵,即$UU^T=VV^T=I,\sum中对角线外的其他元素都为0,对角线上元素为奇异值$.		

$$A^TA=(U\sum{V^T})^T(U\sum{V^T}) \\
  	 =V\sum{^T}U^TU\sum{V^T} \\
  	 =V\sum{^T}\sum{V^T}$$

因为U为正交矩阵,所以$A^TA的的特征值为\sum{^T}\sum中对脚线上的非0值$.		

$$A^TAv_i=\lambda_iv_i$$

$v_i表示上面的右奇异响亮V^T$,此外我们可以得到:		

$$\sigma_i=\sqrt{\lambda_i}  \\
  u_i = \frac{1}{\lambda_i}Av_i$$		

这里的$\sigma就是上面说的奇异值,u_i是有奇异向量U,奇异值矩阵\sum$中对角线上的奇异值是按从大到小顺序排列的,奇异值的大小可以理解为特征的重要程度,奇异值越大,描述的特征就越重要.

现在回到LSA的讨论中,我们知道LSA通过SVD来将向量空间映射到隐语义空间,主要实现过程是,对奇异值矩阵,我们只取对角线上的前K个奇异值,其余奇异值设为0,现在我们得到的向量空间为:		

$$\overline{A} =U_{mk}\sum_{kk}V^T_{kn}\approx 	U_{mm}\sum_{mn}V^T_{nn}=A$$		

映射后的向量空间会近视等于原始的向量空间是因为,对奇异值矩阵进行了处理,只保留了前k个最大的奇异值,奇异值中值越大的说明所描述的特征越重要,奇异值越小说明所描述的特征贡献越弱,且奇异值在对角线上的减少程度很大,所以所除去前k个最大的奇异值,剩下的非0的奇异值说描述的特征贡献很弱.所以映射后的向量空间与初始向量空间接近.		

说明一点:正常情况下,按照向量空间模型构建的向量空间是非常稀疏的,当采用LSA进行隐语义空间映射后,向量空间的稀疏性会减弱,这可能有助于计算文档之间的联系,尽管文档之间不存在很多相同的词项.	


###参考资料	

[机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 http://leftnoteasy.cnblogs.com](http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html)		

[Latent Semantic Analysis(LSA/ LSI)算法简介 http://www.cnblogs.com/kemaswill/](http://www.cnblogs.com/kemaswill/archive/2013/04/17/3022100.html)	






